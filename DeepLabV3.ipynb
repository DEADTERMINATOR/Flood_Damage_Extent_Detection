{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","mount_file_id":"1KMWS2Oyo4vVFS2GYmUhqId2DopiI21d6","authorship_tag":"ABX9TyMUD9kBpM7O5RdIjgQzkzRM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["pip install rasterio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kJpUbvTRbFN","executionInfo":{"status":"ok","timestamp":1702314138869,"user_tz":480,"elapsed":14875,"user":{"displayName":"DEADTERMINATOR","userId":"00537253248716777682"}},"outputId":"0ccaaf6b-8157-4422-c66e-d0100d69440a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rasterio in /usr/local/lib/python3.10/dist-packages (1.3.9)\n","Requirement already satisfied: affine in /usr/local/lib/python3.10/dist-packages (from rasterio) (2.4.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2023.11.17)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.23.5)\n","Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.4.7)\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n","Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","from torchvision import models\n","import torchvision.transforms as transforms\n","from torchvision.transforms import Compose, Resize, v2\n","from torchvision.transforms.functional import to_tensor, hflip, vflip, rotate, adjust_gamma\n","from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n","\n","import os\n","import time\n","import random\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","from PIL import Image\n","import rasterio"],"metadata":{"id":"-lriJGWRzuyX","executionInfo":{"status":"ok","timestamp":1702314151646,"user_tz":480,"elapsed":12780,"user":{"displayName":"DEADTERMINATOR","userId":"00537253248716777682"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class HarveyData(Dataset):\n","    #dataset_dir: Provide a path to either \"./dataset/training\" or \"./dataset/testing\"\n","    #transforms: Any transformations that should be performed on the image when retrieved.\n","    def __init__(self, dataset_dir, image_size = 224, augment_data=True):\n","        super(HarveyData, self).__init__()\n","        self.dataset_dir = dataset_dir\n","        self.image_size = image_size\n","        self.augment_data = augment_data\n","\n","        self.pre_image_paths = sorted(os.listdir(os.path.join(dataset_dir, 'pre_img')))\n","        self.post_image_paths = sorted(os.listdir(os.path.join(dataset_dir, 'post_img')))\n","        self.mask_paths = sorted(os.listdir(os.path.join(dataset_dir, 'PDE_labels')))\n","        self.elevation_paths = sorted(os.listdir(os.path.join(dataset_dir, 'elevation')))\n","        self.hand_paths = sorted(os.listdir(os.path.join(dataset_dir, 'hand')))\n","        self.imperviousness_paths = sorted(os.listdir(os.path.join(dataset_dir, 'imperviousness')))\n","\n","        self.pre_images = []\n","        self.post_images = []\n","        self.masks = []\n","\n","        self.elevation = []\n","        self.hand = []\n","        self.imperviousness = []\n","\n","        self.num_images = len(self.pre_image_paths)\n","\n","        for i in range(self.num_images):\n","            pre_image = Image.open(os.path.join(dataset_dir, 'pre_img', self.pre_image_paths[i]))\n","            post_image = Image.open(os.path.join(dataset_dir, 'post_img', self.post_image_paths[i]))\n","            mask = Image.open(os.path.join(dataset_dir, 'PDE_labels', self.mask_paths[i])).convert('L')\n","\n","            with rasterio.open(os.path.join(dataset_dir, 'elevation', self.elevation_paths[i])) as src:\n","                elevation = src.read(1)\n","                elevation = torch.tensor(elevation).unsqueeze(0)\n","            with rasterio.open(os.path.join(dataset_dir, 'hand', self.hand_paths[i])) as src:\n","                hand = src.read(1)\n","                hand = torch.tensor(hand).unsqueeze(0)\n","            with rasterio.open(os.path.join(dataset_dir, 'imperviousness', self.imperviousness_paths[i])) as src:\n","                imperviousness = src.read(1)\n","                imperviousness = torch.tensor(imperviousness).unsqueeze(0)\n","\n","            self.pre_images.append(pre_image)\n","            self.post_images.append(post_image)\n","            self.masks.append(mask)\n","\n","            self.elevation.append(elevation)\n","            self.hand.append(hand)\n","            self.imperviousness.append(imperviousness)\n","\n","    def __getitem__(self, idx):\n","        #Get pre and post image, and the mask, for the current index.\n","        pre_image = self.pre_images[idx]\n","        post_image = self.post_images[idx]\n","        mask = self.masks[idx]\n","\n","        elevation = self.elevation[idx]\n","        hand = self.hand[idx]\n","        imperviousness = self.imperviousness[idx]\n","\n","        image_transforms = v2.Compose([\n","                           v2.ToImage(),\n","                           v2.ToDtype(torch.float32, scale=True),\n","                           v2.Resize((self.image_size, self.image_size), antialias=True),\n","                           v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  #These are the normalization values used by the pretrained weights in DeepLabv3\n","        ])\n","        mask_transforms = v2.Compose([\n","                          v2.ToImage(),\n","                          v2.ToDtype(torch.int64, scale=False),\n","                          v2.Resize((self.image_size, self.image_size), antialias=True)\n","        ])\n","        meta_transforms = v2.Compose([\n","                          v2.ToImage(),\n","                          v2.ToDtype(torch.float32, scale=True),\n","                          v2.Resize((self.image_size, self.image_size), antialias=True),\n","                          v2.Grayscale()\n","        ])\n","\n","        pre_image = image_transforms(pre_image)\n","        post_image = image_transforms(post_image)\n","        mask = mask_transforms(mask)\n","\n","        elevation = meta_transforms(elevation)\n","        hand = meta_transforms(hand)\n","        imperviousness = meta_transforms(imperviousness)\n","\n","        if self.augment_data:\n","            augmentation_switches = {0, 1, 2, 3, 4, 5, 6}\n","            augment_mode_1 = np.random.choice(list(augmentation_switches))\n","            augmentation_switches.remove(augment_mode_1)\n","\n","            additional_augment_chance = np.random.random()\n","            augment_mode_2 = -1\n","            augment_mode_3 = -1\n","\n","            if (additional_augment_chance > 0.5):\n","                augment_mode_2 = np.random.choice(list(augmentation_switches))\n","                augmentation_switches.remove(augment_mode_2)\n","            #if (additional_augment_chance > 0.8):\n","                #augment_mode_3 = np.random.choice(list(augmentation_switches))\n","                #augmentation_switches.remove(augment_mode_3)\n","\n","            if augment_mode_1 or augment_mode_2 or augment_mode_3 == 0:\n","                # flip image vertically\n","                pre_image = vflip(pre_image)\n","                post_image = vflip(post_image)\n","\n","                elevation = vflip(elevation)\n","                hand = vflip(hand)\n","                imperviousness = vflip(imperviousness)\n","\n","                mask = vflip(mask)\n","            elif augment_mode_1 or augment_mode_2 or augment_mode_3 == 1:\n","                # flip image horizontally\n","                pre_image = hflip(pre_image)\n","                post_image = hflip(post_image)\n","\n","                elevation = hflip(elevation)\n","                hand = hflip(hand)\n","                imperviousness = hflip(imperviousness)\n","\n","                mask = hflip(mask)\n","            elif augment_mode_1 or augment_mode_2 or augment_mode_3 == 2:\n","                # zoom image\n","                zoom = v2.RandomResizedCrop(self.size, antialias=True)\n","\n","                pre_image = zoom(pre_image)\n","                post_image = zoom(post_image)\n","\n","                elevation = zoom(elevation)\n","                hand = zoom(hand)\n","                imperviousness = zoom(imperviousness)\n","\n","                mask = zoom(mask)\n","            elif augment_mode_1 or augment_mode_2 or augment_mode_3 == 3:\n","                # modify gamma\n","                min_gamma = 0.25\n","                gamma_range = 2.25\n","                gamma = gamma_range * np.random.random() + min_gamma\n","\n","                pre_image = adjust_gamma(pre_image, gamma)\n","                post_image = adjust_gamma(post_image, gamma)\n","\n","                elevation = adjust_gamma(elevation)\n","                hand = adjust_gamma(hand)\n","                imperviousness = adjust_gamma(imperviousness)\n","\n","                mask = adjust_gamma(mask, gamma)\n","            elif augment_mode_1 or augment_mode_2 or augment_mode_3 == 4:\n","                # perform elastic transformation\n","                elastic = v2.ElasticTransform(sigma=10)\n","\n","                pre_image = elastic(pre_image)\n","                post_image = elastic(post_image)\n","\n","                elevation = elastic(elevation)\n","                hand = elastic(hand)\n","                imperviousness = elastic(imperviousness)\n","\n","                mask = elastic(mask)\n","            elif augment_mode_1 or augment_mode_2 or augment_mode_3 == 5:\n","                # modify brightness/contrast/saturation/hue\n","                jitter = v2.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.25)\n","\n","                pre_image = jitter(pre_image)\n","                post_image = jitter(post_image)\n","\n","                elevation = jitter(elevation)\n","                hand = jitter(hand)\n","                imperviousness = jitter(imperviousness)\n","\n","                mask = jitter(mask)\n","            elif augment_mode_1 or augment_mode_2 or augment_mode_3 == 6:\n","                # rotate image\n","                random_degree = random.randint(1, 359)\n","\n","                pre_image = rotate(pre_image, random_degree)\n","                post_image = rotate(post_image, random_degree)\n","\n","                elevation = rotate(elevation, random_degree)\n","                hand = rotate(hand, random_degree)\n","                imperviousness = rotate(imperviousness, random_degree)\n","\n","                mask = rotate(mask, random_degree)\n","\n","        #Concatenate the pre and post disaster images, as well as the meta-attributes, together along the channel dimension.\n","        #combined_image = torch.cat([pre_image, post_image, elevation, imperviousness], dim=0)\n","        combined_image = torch.cat([pre_image, post_image], dim=0)\n","        return combined_image, mask\n","\n","    def get_item_resize_only(self, idx, image_size):\n","        #Get pre and post image, and the mask, for the current index.\n","        pre_image = self.pre_images[idx]\n","        post_image = self.post_images[idx]\n","        mask = self.masks[idx]\n","\n","        elevation = self.elevation[idx]\n","        hand = self.hand[idx]\n","        imperviousness = self.imperviousness[idx]\n","\n","        #Convert image to normalized tensor.\n","        pre_image = to_tensor(pre_image)\n","        post_image = to_tensor(post_image)\n","\n","        mask = to_tensor(mask)\n","        mask *= 255  # Manually adjust the label values back to the original values after the normalization of to_tensor()\n","\n","        elevation = to_tensor(elevation)\n","        hand = to_tensor(hand)\n","        imperviousness = to_tensor(imperviousness)\n","\n","        #Resize the images to the same size as was used during training.\n","        resize = v2.Compose([v2.Resize((image_size, image_size), antialias=True)])\n","        pre_image = resize(pre_image)\n","        post_image = resize(post_image)\n","        mask = resize(mask)\n","\n","        elevation = resize(elevation)\n","        hand = resize(hand)\n","        imperviousness = resize(imperviousness)\n","\n","        #Concatenate the pre and post disaster images, as well as the meta attributes, together along the channel dimension.\n","        combined_image = torch.cat([pre_image, post_image, elevation, imperviousness], dim=0)\n","        return combined_image, mask\n","\n","    def __len__(self):\n","        return self.num_images"],"metadata":{"id":"PnwOdQzvzr8n","executionInfo":{"status":"ok","timestamp":1702314151647,"user_tz":480,"elapsed":12,"user":{"displayName":"DEADTERMINATOR","userId":"00537253248716777682"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class DeepLabV3(nn.Module):\n","    def __init__(self, num_input_channels, num_classes):\n","        super(DeepLabV3, self).__init__()\n","        self.deeplabv3_weights = torchvision.models.segmentation.DeepLabV3_ResNet101_Weights.DEFAULT\n","        self.resnet101_weights = models.ResNet101_Weights.DEFAULT\n","        self.deeplabv3 = torchvision.models.segmentation.deeplabv3_resnet101(weights=self.deeplabv3_weights, weights_backbone=self.resnet101_weights)\n","\n","        #Replaces the first convolution of the backbone of the model to accept 6-channel input.\n","        self.deeplabv3.backbone.conv1 = nn.Conv2d(num_input_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","        #Replaces the final classifier to change the number of output classes to 4.\n","        self.deeplabv3.classifier[-1] = torch.nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1, stride=1)\n","\n","    def forward(self, x):\n","        x = self.deeplabv3.forward(x)\n","        return x"],"metadata":{"id":"QQ6jLPz9zy0S","executionInfo":{"status":"ok","timestamp":1702314151647,"user_tz":480,"elapsed":12,"user":{"displayName":"DEADTERMINATOR","userId":"00537253248716777682"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class FocalLoss(nn.Module):\n","    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.reduction = reduction\n","\n","    def forward(self, input, target):\n","        ce_loss = F.cross_entropy(input, target, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.alpha is not None:\n","            focal_loss = self.alpha * focal_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss"],"metadata":{"id":"4as-yETN0JRz","executionInfo":{"status":"ok","timestamp":1702314151647,"user_tz":480,"elapsed":11,"user":{"displayName":"DEADTERMINATOR","userId":"00537253248716777682"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1cohYEr2fNhY6VZxVuSpI5zZ0CLybG1v6"},"id":"BDnqqojUpKLE","executionInfo":{"status":"ok","timestamp":1702321512286,"user_tz":480,"elapsed":7360650,"user":{"displayName":"DEADTERMINATOR","userId":"00537253248716777682"}},"outputId":"ad9dcb9c-ec20-4875-82cb-910778865a2d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["def visualize_results(num_results, predictions, images=None, masks=None, randomize_images=False):\n","    fig, axes = plt.subplots(num_results, 3, figsize=(32, 32))\n","\n","    predictions_flat = [item for sublist in predictions for item in sublist]\n","    if (images != None):\n","        images_flat = [item for sublist in images for item in sublist]\n","    if (masks != None):\n","        masks_flat = [item for sublist in masks for item in sublist]\n","\n","    if (randomize_images):\n","        # Choose num_results number of images at random from the results.\n","        image_idxs = random.sample(range(0, len(predictions_flat) - 1), num_results)\n","    else:\n","        image_idxs = [i for i in range(1, num_results + 2)]\n","\n","    for i in range(num_results):\n","        # Plot the input image and ground truth mask\n","        if (images == None or masks == None):\n","            image, mask = test_dataset.get_item_resize_only(image_idxs[i], image_size)\n","\n","            #Reorder the channels for matplotlib.\n","            image = torch.permute(image, (1, 2, 0))\n","            mask = torch.permute(mask, (1, 2, 0))\n","\n","            axes[i, 0].imshow(image.numpy()[:, :, 0:3], aspect='equal')\n","            axes[i, 0].imshow(image.numpy()[:, :, 3:6], alpha=0.5, aspect='equal')\n","            #axes[i, 0].imshow(image.numpy()[:, :, 6:7], alpha=0.5, aspect='equal')\n","            #axes[i, 0].imshow(image.numpy()[:, :, 7:8], alpha=0.5, aspect='equal')\n","            axes[i, 2].imshow(mask.numpy(), cmap=\"viridis\", aspect='equal')\n","        else:\n","            image = images_flat[image_idxs[i]]\n","            mask = masks_flat[image_idxs[i]]\n","\n","            #Reorder the channels for matplotlib.\n","            image = np.transpose(image, (1, 2, 0))\n","            #mask = np.transpose(mask, (1, 2, 0))\n","\n","            axes[i, 0].imshow(image[:, :, 0:3], aspect='equal')\n","            axes[i, 0].imshow(image[:, :, 3:6], alpha=0.5, aspect='equal')\n","            #axes[i, 0].imshow(image[:, :, 6:7], alpha=0.5, aspect='equal')\n","            #axes[i, 0].imshow(image[:, :, 7:8], alpha=0.5, aspect='equal')\n","            axes[i, 2].imshow(mask, cmap=\"viridis\", aspect='equal')\n","\n","        axes[i, 0].set_title(\"Combined Image\")\n","        axes[i, 0].axis('off')\n","\n","        axes[i, 2].set_title(\"Ground Truth Mask\")\n","        axes[i, 2].axis('off')\n","\n","        # Plot the predicted image\n","        axes[i, 1].imshow(predictions_flat[image_idxs[i]], cmap=\"viridis\", aspect='equal')\n","        axes[i, 1].set_title(\"Predicted Image\")\n","        axes[i, 1].axis('off')\n","\n","    plt.show()\n","\n","batch_size = 2\n","num_input_channels = 6\n","num_classes = 5\n","lr = 1e-4\n","image_size = 800  #520x520 is the image size used by the pretrained weights in DeepLabv3\n","# Whether the models parameters should be saved following the completion of a run.\n","save = True\n","#Whether an existing models parameters should be loaded before the run.\n","load = False\n","\n","cwd = os.getcwd()\n","\n","print(\"Loading train and test images\")\n","train_dataset = HarveyData(os.path.join(cwd, 'drive/MyDrive/Flood Damage Extent Detection/dataset/training'), image_size=image_size)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataset = HarveyData(os.path.join(cwd, 'drive/MyDrive/Flood Damage Extent Detection/dataset/testing'), image_size=image_size)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","print(\"Finished loading images\")\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = DeepLabV3(num_input_channels, num_classes)\n","if (load):\n","    if (os.path.exists('drive/MyDrive/Flood Damage Extent Detection/DeepLabv3.pt')):\n","        print(\"Loading model.\")\n","        model.load_state_dict(torch.load('drive/MyDrive/Flood Damage Extent Detection/DeepLabv3.pt'))\n","    else:\n","        print('Could not load model. File does not exist.')\n","model.to(device)\n","#model_preprocess = model.deeplabv3_weights.transforms()\n","\n","criterion = FocalLoss(reduction='sum')#torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005)\n","\n","softmax = nn.Softmax(dim=1)\n","\n","num_epochs = 50\n","\n","images = []\n","masks = []\n","predicted_images = []\n","\n","#Training\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for i, data in enumerate(train_dataloader):\n","        image, mask = data\n","\n","        image = image.to(device)\n","        mask = mask.squeeze().to(device)\n","\n","        outputs = softmax(model(image)['out'])\n","\n","        loss = criterion(outputs, mask)\n","        loss.backward()\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        epoch_loss += loss.item()\n","\n","        print('Batch %d --- Loss: %.4f' % (i, loss.item() / batch_size))\n","    print('Epoch %d / %d --- Average Loss: %.4f' % (epoch + 1, num_epochs, epoch_loss / train_dataset.__len__()))\n","\n","    total_loss = 0.0\n","\n","    total_macro_precision = 0.0\n","    total_macro_recall = 0.0\n","    total_macro_f1 = 0.0\n","\n","    total_class_precision = [0.0, 0.0, 0.0, 0.0, 0.0]\n","    total_class_recall = [0.0, 0.0, 0.0, 0.0, 0.0]\n","    total_class_f1 = [0.0, 0.0, 0.0, 0.0, 0.0]\n","\n","#Testing\n","    model.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(test_dataloader):\n","            image, mask = data\n","\n","            image = image.to(device)\n","            mask = mask.squeeze().to(device)\n","\n","            outputs = softmax(model(image)['out'])\n","\n","            loss = criterion(outputs, mask)\n","            total_loss += loss.item()\n","\n","            predicted = torch.argmax(outputs, dim=1, keepdim=False)\n","\n","            image = image.cpu().numpy()\n","            mask = mask.cpu().numpy()\n","            predicted = predicted.cpu().numpy()\n","\n","            for i in range(len(mask)):\n","                # Calculate scores globally.\n","                precision, recall, f1, _ = precision_recall_fscore_support(mask[i].flatten(), predicted[i].flatten(), average='macro', zero_division=0.0)\n","                total_macro_precision += precision\n","                total_macro_recall += recall\n","                total_macro_f1 += f1\n","\n","                # Calculate scores by class.\n","                precision, recall, f1, _ = precision_recall_fscore_support(mask[i].flatten(), predicted[i].flatten(), labels=[0, 1, 2, 3, 4], average=None, zero_division=0.0)\n","                total_class_precision += precision\n","                total_class_recall += recall\n","                total_class_f1 += f1\n","\n","            if (epoch + 1 == num_epochs):\n","                images.append(image)\n","                masks.append(mask)\n","                predicted_images.append(predicted)\n","\n","    average_loss = total_loss / len(test_dataset)\n","\n","    average_macro_precision = total_macro_precision / len(test_dataset)\n","    average_macro_recall = total_macro_recall / len(test_dataset)\n","    average_macro_f1 = total_macro_f1 / len(test_dataset)\n","\n","    average_class_precision = total_class_precision / len(test_dataset)\n","    average_class_recall = total_class_recall / len(test_dataset)\n","    average_class_f1 = total_class_f1 / len(test_dataset)\n","\n","    print('Average Macro Precision: %.4f ---- Average Macro Recall: %.4f ---- Average F1 Score: %.4f ---- Average Loss: %.4f' % (average_macro_precision, average_macro_recall, average_macro_f1, average_loss))\n","    print('Average No Damage Precision: %.4f ---- Average No Damage Recall: %.4f ---- Average No Damage F1: %.4f' % (average_class_precision[0], average_class_recall[0], average_class_f1[0]))\n","    print('Average Minor Precision: %.4f ---- Average Minor Recall: %.4f ---- Average Minor F1: %.4f' % (average_class_precision[1], average_class_recall[1], average_class_f1[1]))\n","    print('Average Moderate Precision: %.4f ---- Average Moderate Recall: %.4f ---- Average Moderate F1: %.4f' % (average_class_precision[2], average_class_recall[2], average_class_f1[2]))\n","    print('Average Major Precision: %.4f ---- Average Major Recall: %.4f ---- Average Major F1: %.4f' % (average_class_precision[3], average_class_recall[3], average_class_f1[3]))\n","    print('Average Background Precision: %.4f ---- Average Background Recall: %.4f ---- Average Background F1: %.4f' % (average_class_precision[4], average_class_recall[4], average_class_f1[4]))\n","\n","    if (epoch + 1 == num_epochs):\n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","        print(f\"Elapsed Time at Epoch {epoch + 1} : {elapsed_time} seconds\")\n","\n","        if save:\n","            torch.save(model.state_dict(), 'drive/MyDrive/Flood Damage Extent Detection/DeepLabv3.pt')\n","\n","        visualize_results(6, predicted_images, images, masks)\n","\n","        images.clear()\n","        masks.clear()\n","        predicted_images.clear()\n","\n","        start_time = time.time()"]}]}