{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"116airy7vw8bo75PG6P8_Fl8oNggQpfzh"},"executionInfo":{"elapsed":2458886,"status":"ok","timestamp":1700442290003,"user":{"displayName":"DEADTERMINATOR","userId":"00537253248716777682"},"user_tz":480},"id":"BDnqqojUpKLE","outputId":"6090b0f9-8ac7-470d-9795-4c840cf0b953"},"outputs":[{"name":"stderr","output_type":"stream","text":["'unzip' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["from re import A\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","from torchvision import models\n","import torchvision.transforms as transforms\n","from torchvision.transforms import Compose, Resize, v2\n","from torchvision.transforms.functional import to_tensor\n","\n","import os\n","import time\n","import random\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","import dataset\n","import dataloader\n","\n","from PIL import Image\n","\n","#import dataset\n","!unzip dataset.zip\n","\n","class HarveyData(Dataset):\n","    #dataset_dir: Provide a path to either \"./dataset/training\" or \"./dataset/testing\"\n","    #transforms: Any transformations that should be performed on the image when retrieved.\n","    def __init__(self, dataset_dir, image_transforms=None, mask_transforms=None):\n","        super(HarveyData, self).__init__()\n","        self.dataset_dir = dataset_dir\n","        self.image_transforms = image_transforms\n","        self.mask_transforms = mask_transforms\n","        \n","        self.pre_image_paths = sorted(os.listdir(os.path.join(dataset_dir, 'pre_img')))\n","        self.post_image_paths = sorted(os.listdir(os.path.join(dataset_dir, 'post_img')))\n","        self.mask_paths = sorted(os.listdir(os.path.join(dataset_dir, 'post_msk')))\n","        \n","        self.pre_images = []\n","        self.post_images = []\n","        self.masks = []\n","        \n","        self.num_images = len(self.pre_image_paths)\n","        \n","        for i in range(self.num_images):\n","            pre_image = Image.open(os.path.join(dataset_dir, 'pre_img', self.pre_image_paths[i]))\n","            post_image = Image.open(os.path.join(dataset_dir, 'post_img', self.post_image_paths[i]))\n","            mask = Image.open(os.path.join(dataset_dir, 'post_msk', self.mask_paths[i])).convert('L')\n","            \n","            self.pre_images.append(pre_image)\n","            self.post_images.append(post_image)\n","            self.masks.append(mask)\n","            \n","    def __getitem__(self, idx):\n","        #Get pre and post image, and the mask, for the current index.\n","        pre_image = self.pre_images[idx]\n","        post_image = self.post_images[idx]\n","        mask = self.masks[idx]\n","            \n","        #r, g, b, _ = mask.split()\n","        #mask = Image.merge(\"RGBA\", (r, g, b, r))\n","        #mask = np.array(mask)\n","            \n","        #Apply transformations to images\n","        if (self.image_transforms is not None):\n","            pre_image = self.image_transforms(pre_image)\n","            post_image = self.image_transforms(post_image)\n","        if (self.mask_transforms is not None):\n","            mask = self.mask_transforms(mask)\n","            \n","        #Concatenate the pre and post disaster images together along the channel dimension.\n","        combined_image = torch.cat([pre_image, post_image], dim=0)\n","        return combined_image, mask\n","        \n","    def get_item_no_transforms(self, idx):\n","        #Get pre and post image, and the mask, for the current index.\n","        pre_image = self.pre_images[idx]\n","        post_image = self.post_images[idx]\n","        mask = self.masks[idx]\n","            \n","        #Convert image to normalized tensor.\n","        pre_image = to_tensor(pre_image)\n","        post_image = to_tensor(post_image)\n","        mask = to_tensor(mask)\n","        mask *= 255  # Manually adjust the label values back to the original values after the normalization of to_tensor()\n","        \n","        #Concatenate the pre and post disaster images together along the channel dimension.\n","        combined_image = torch.cat([pre_image, post_image], dim=0)\n","        return combined_image, mask\n","    \n","    def __len__(self):\n","        return self.num_images\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to C:\\Users\\weiba/.cache\\torch\\hub\\checkpoints\\deeplabv3_resnet101_coco-586e9e4e.pth\n","100%|██████████| 233M/233M [00:06<00:00, 39.3MB/s] \n"]},{"name":"stdout","output_type":"stream","text":["Batch 0 --- Loss: 0.1726\n","Batch 1 --- Loss: 0.1725\n","Batch 2 --- Loss: 0.1726\n","Batch 3 --- Loss: 0.1703\n","Batch 4 --- Loss: 0.1711\n","Batch 5 --- Loss: 0.1701\n","Batch 6 --- Loss: 0.1702\n","Batch 7 --- Loss: 0.1674\n","Batch 8 --- Loss: 0.1671\n","Batch 9 --- Loss: 0.1711\n","Batch 10 --- Loss: 0.1679\n","Batch 11 --- Loss: 0.1680\n","Batch 12 --- Loss: 0.1682\n","Batch 13 --- Loss: 0.1671\n","Batch 14 --- Loss: 0.1654\n","Batch 15 --- Loss: 0.1659\n","Batch 16 --- Loss: 0.1675\n","Batch 17 --- Loss: 0.1649\n","Batch 18 --- Loss: 0.1612\n","Batch 19 --- Loss: 0.1638\n","Batch 20 --- Loss: 0.1527\n","Batch 21 --- Loss: 0.1584\n","Batch 22 --- Loss: 0.1603\n","Batch 23 --- Loss: 0.1480\n","Batch 24 --- Loss: 0.1567\n","Batch 25 --- Loss: 0.1499\n","Epoch 1 / 50 --- Average Loss: 0.1650\n","Average Precision: 0.2417 ---- Average Recall: 0.2998 ---- Average F1: 0.2361 ---- Average Loss: 0.1675\n","Batch 0 --- Loss: 0.1512\n","Batch 1 --- Loss: 0.1525\n","Batch 2 --- Loss: 0.1566\n","Batch 3 --- Loss: 0.1436\n","Batch 4 --- Loss: 0.1560\n","Batch 5 --- Loss: 0.1482\n","Batch 6 --- Loss: 0.1428\n","Batch 7 --- Loss: 0.1531\n","Batch 8 --- Loss: 0.1430\n","Batch 9 --- Loss: 0.1477\n","Batch 10 --- Loss: 0.1378\n","Batch 11 --- Loss: 0.1434\n","Batch 12 --- Loss: 0.1474\n","Batch 13 --- Loss: 0.1533\n","Batch 14 --- Loss: 0.1470\n","Batch 15 --- Loss: 0.1451\n","Batch 16 --- Loss: 0.1553\n","Batch 17 --- Loss: 0.1510\n","Batch 18 --- Loss: 0.1490\n","Batch 19 --- Loss: 0.1468\n","Batch 20 --- Loss: 0.1487\n","Batch 21 --- Loss: 0.1444\n","Batch 22 --- Loss: 0.1429\n","Batch 23 --- Loss: 0.1437\n","Batch 24 --- Loss: 0.1453\n","Batch 25 --- Loss: 0.1462\n","Epoch 2 / 50 --- Average Loss: 0.1478\n","Average Precision: 0.3350 ---- Average Recall: 0.3591 ---- Average F1: 0.3107 ---- Average Loss: 0.1599\n","Batch 0 --- Loss: 0.1408\n","Batch 1 --- Loss: 0.1430\n","Batch 2 --- Loss: 0.1422\n","Batch 3 --- Loss: 0.1438\n","Batch 4 --- Loss: 0.1417\n","Batch 5 --- Loss: 0.1402\n","Batch 6 --- Loss: 0.1448\n","Batch 7 --- Loss: 0.1410\n","Batch 8 --- Loss: 0.1430\n","Batch 9 --- Loss: 0.1450\n","Batch 10 --- Loss: 0.1376\n","Batch 11 --- Loss: 0.1383\n","Batch 12 --- Loss: 0.1378\n","Batch 13 --- Loss: 0.1322\n","Batch 14 --- Loss: 0.1424\n","Batch 15 --- Loss: 0.1417\n","Batch 16 --- Loss: 0.1383\n","Batch 17 --- Loss: 0.1415\n","Batch 18 --- Loss: 0.1451\n","Batch 19 --- Loss: 0.1373\n","Batch 20 --- Loss: 0.1390\n","Batch 21 --- Loss: 0.1348\n","Batch 22 --- Loss: 0.1413\n","Batch 23 --- Loss: 0.1387\n","Batch 24 --- Loss: 0.1300\n","Batch 25 --- Loss: 0.1390\n","Epoch 3 / 50 --- Average Loss: 0.1400\n","Average Precision: 0.4197 ---- Average Recall: 0.4141 ---- Average F1: 0.3683 ---- Average Loss: 0.1538\n","Batch 0 --- Loss: 0.1377\n","Batch 1 --- Loss: 0.1380\n","Batch 2 --- Loss: 0.1337\n","Batch 3 --- Loss: 0.1370\n","Batch 4 --- Loss: 0.1368\n","Batch 5 --- Loss: 0.1306\n","Batch 6 --- Loss: 0.1364\n","Batch 7 --- Loss: 0.1383\n","Batch 8 --- Loss: 0.1445\n","Batch 9 --- Loss: 0.1278\n","Batch 10 --- Loss: 0.1339\n","Batch 11 --- Loss: 0.1362\n","Batch 12 --- Loss: 0.1322\n","Batch 13 --- Loss: 0.1338\n","Batch 14 --- Loss: 0.1330\n","Batch 15 --- Loss: 0.1311\n","Batch 16 --- Loss: 0.1296\n","Batch 17 --- Loss: 0.1325\n","Batch 18 --- Loss: 0.1309\n","Batch 19 --- Loss: 0.1302\n","Batch 20 --- Loss: 0.1324\n","Batch 21 --- Loss: 0.1327\n","Batch 22 --- Loss: 0.1358\n","Batch 23 --- Loss: 0.1378\n","Batch 24 --- Loss: 0.1321\n","Batch 25 --- Loss: 0.1300\n","Epoch 4 / 50 --- Average Loss: 0.1340\n","Average Precision: 0.4232 ---- Average Recall: 0.4191 ---- Average F1: 0.3896 ---- Average Loss: 0.1465\n","Batch 0 --- Loss: 0.1305\n","Batch 1 --- Loss: 0.1283\n","Batch 2 --- Loss: 0.1386\n","Batch 3 --- Loss: 0.1290\n","Batch 4 --- Loss: 0.1335\n","Batch 5 --- Loss: 0.1296\n","Batch 6 --- Loss: 0.1309\n","Batch 7 --- Loss: 0.1335\n","Batch 8 --- Loss: 0.1308\n","Batch 9 --- Loss: 0.1361\n","Batch 10 --- Loss: 0.1309\n","Batch 11 --- Loss: 0.1310\n","Batch 12 --- Loss: 0.1262\n","Batch 13 --- Loss: 0.1386\n","Batch 14 --- Loss: 0.1300\n","Batch 15 --- Loss: 0.1293\n","Batch 16 --- Loss: 0.1353\n","Batch 17 --- Loss: 0.1290\n","Batch 18 --- Loss: 0.1300\n","Batch 19 --- Loss: 0.1235\n","Batch 20 --- Loss: 0.1228\n","Batch 21 --- Loss: 0.1341\n","Batch 22 --- Loss: 0.1310\n","Batch 23 --- Loss: 0.1255\n","Batch 24 --- Loss: 0.1307\n","Batch 25 --- Loss: 0.1322\n","Epoch 5 / 50 --- Average Loss: 0.1308\n","Average Precision: 0.4556 ---- Average Recall: 0.4360 ---- Average F1: 0.4061 ---- Average Loss: 0.1432\n","Batch 0 --- Loss: 0.1335\n","Batch 1 --- Loss: 0.1325\n","Batch 2 --- Loss: 0.1278\n","Batch 3 --- Loss: 0.1307\n","Batch 4 --- Loss: 0.1277\n","Batch 5 --- Loss: 0.1307\n","Batch 6 --- Loss: 0.1249\n","Batch 7 --- Loss: 0.1319\n","Batch 8 --- Loss: 0.1230\n","Batch 9 --- Loss: 0.1256\n","Batch 10 --- Loss: 0.1297\n","Batch 11 --- Loss: 0.1280\n","Batch 12 --- Loss: 0.1293\n","Batch 13 --- Loss: 0.1316\n","Batch 14 --- Loss: 0.1254\n","Batch 15 --- Loss: 0.1289\n","Batch 16 --- Loss: 0.1281\n","Batch 17 --- Loss: 0.1196\n","Batch 18 --- Loss: 0.1240\n","Batch 19 --- Loss: 0.1218\n","Batch 20 --- Loss: 0.1314\n","Batch 21 --- Loss: 0.1193\n","Batch 22 --- Loss: 0.1181\n","Batch 23 --- Loss: 0.1295\n","Batch 24 --- Loss: 0.1255\n","Batch 25 --- Loss: 0.1315\n","Epoch 6 / 50 --- Average Loss: 0.1273\n","Average Precision: 0.4687 ---- Average Recall: 0.4373 ---- Average F1: 0.4163 ---- Average Loss: 0.1428\n","Batch 0 --- Loss: 0.1223\n","Batch 1 --- Loss: 0.1238\n","Batch 2 --- Loss: 0.1308\n","Batch 3 --- Loss: 0.1239\n","Batch 4 --- Loss: 0.1216\n","Batch 5 --- Loss: 0.1300\n","Batch 6 --- Loss: 0.1251\n","Batch 7 --- Loss: 0.1220\n","Batch 8 --- Loss: 0.1298\n","Batch 9 --- Loss: 0.1184\n","Batch 10 --- Loss: 0.1236\n","Batch 11 --- Loss: 0.1270\n","Batch 12 --- Loss: 0.1249\n","Batch 13 --- Loss: 0.1304\n","Batch 14 --- Loss: 0.1244\n","Batch 15 --- Loss: 0.1266\n","Batch 16 --- Loss: 0.1268\n","Batch 17 --- Loss: 0.1207\n","Batch 18 --- Loss: 0.1228\n","Batch 19 --- Loss: 0.1247\n","Batch 20 --- Loss: 0.1196\n","Batch 21 --- Loss: 0.1276\n","Batch 22 --- Loss: 0.1250\n","Batch 23 --- Loss: 0.1270\n","Batch 24 --- Loss: 0.1220\n","Batch 25 --- Loss: 0.1262\n","Epoch 7 / 50 --- Average Loss: 0.1249\n","Average Precision: 0.4680 ---- Average Recall: 0.4372 ---- Average F1: 0.4149 ---- Average Loss: 0.1427\n","Batch 0 --- Loss: 0.1291\n","Batch 1 --- Loss: 0.1254\n","Batch 2 --- Loss: 0.1230\n","Batch 3 --- Loss: 0.1221\n","Batch 4 --- Loss: 0.1237\n","Batch 5 --- Loss: 0.1279\n","Batch 6 --- Loss: 0.1201\n","Batch 7 --- Loss: 0.1219\n","Batch 8 --- Loss: 0.1238\n","Batch 9 --- Loss: 0.1161\n","Batch 10 --- Loss: 0.1215\n","Batch 11 --- Loss: 0.1142\n","Batch 12 --- Loss: 0.1274\n","Batch 13 --- Loss: 0.1268\n","Batch 14 --- Loss: 0.1238\n","Batch 15 --- Loss: 0.1204\n","Batch 16 --- Loss: 0.1256\n","Batch 17 --- Loss: 0.1173\n","Batch 18 --- Loss: 0.1273\n","Batch 19 --- Loss: 0.1164\n","Batch 20 --- Loss: 0.1169\n","Batch 21 --- Loss: 0.1204\n","Batch 22 --- Loss: 0.1216\n","Batch 23 --- Loss: 0.1219\n","Batch 24 --- Loss: 0.1210\n","Batch 25 --- Loss: 0.1236\n","Epoch 8 / 50 --- Average Loss: 0.1223\n","Average Precision: 0.4982 ---- Average Recall: 0.4612 ---- Average F1: 0.4497 ---- Average Loss: 0.1432\n","Batch 0 --- Loss: 0.1202\n","Batch 1 --- Loss: 0.1236\n","Batch 2 --- Loss: 0.1215\n","Batch 3 --- Loss: 0.1224\n","Batch 4 --- Loss: 0.1276\n","Batch 5 --- Loss: 0.1246\n","Batch 6 --- Loss: 0.1240\n","Batch 7 --- Loss: 0.1187\n","Batch 8 --- Loss: 0.1218\n","Batch 9 --- Loss: 0.1156\n","Batch 10 --- Loss: 0.1239\n","Batch 11 --- Loss: 0.1265\n","Batch 12 --- Loss: 0.1175\n","Batch 13 --- Loss: 0.1127\n","Batch 14 --- Loss: 0.1210\n","Batch 15 --- Loss: 0.1229\n","Batch 16 --- Loss: 0.1217\n","Batch 17 --- Loss: 0.1148\n","Batch 18 --- Loss: 0.1172\n","Batch 19 --- Loss: 0.1142\n","Batch 20 --- Loss: 0.1279\n","Batch 21 --- Loss: 0.1217\n","Batch 22 --- Loss: 0.1194\n","Batch 23 --- Loss: 0.1175\n","Batch 24 --- Loss: 0.1170\n","Batch 25 --- Loss: 0.1205\n","Epoch 9 / 50 --- Average Loss: 0.1206\n","Average Precision: 0.5081 ---- Average Recall: 0.4429 ---- Average F1: 0.4219 ---- Average Loss: 0.1364\n","Batch 0 --- Loss: 0.1182\n","Batch 1 --- Loss: 0.1256\n","Batch 2 --- Loss: 0.1215\n","Batch 3 --- Loss: 0.1223\n","Batch 4 --- Loss: 0.1146\n","Batch 5 --- Loss: 0.1149\n","Batch 6 --- Loss: 0.1177\n","Batch 7 --- Loss: 0.1225\n","Batch 8 --- Loss: 0.1155\n","Batch 9 --- Loss: 0.1176\n","Batch 10 --- Loss: 0.1224\n","Batch 11 --- Loss: 0.1135\n","Batch 12 --- Loss: 0.1169\n","Batch 13 --- Loss: 0.1171\n","Batch 14 --- Loss: 0.1209\n","Batch 15 --- Loss: 0.1190\n","Batch 16 --- Loss: 0.1194\n","Batch 17 --- Loss: 0.1119\n","Batch 18 --- Loss: 0.1175\n","Batch 19 --- Loss: 0.1171\n","Batch 20 --- Loss: 0.1254\n","Batch 21 --- Loss: 0.1146\n","Batch 22 --- Loss: 0.1159\n","Batch 23 --- Loss: 0.1157\n","Batch 24 --- Loss: 0.1129\n","Batch 25 --- Loss: 0.1216\n","Epoch 10 / 50 --- Average Loss: 0.1182\n","Average Precision: 0.5336 ---- Average Recall: 0.4736 ---- Average F1: 0.4678 ---- Average Loss: 0.1369\n","Batch 0 --- Loss: 0.1162\n","Batch 1 --- Loss: 0.1169\n","Batch 2 --- Loss: 0.1085\n","Batch 3 --- Loss: 0.1171\n","Batch 4 --- Loss: 0.1158\n","Batch 5 --- Loss: 0.1138\n","Batch 6 --- Loss: 0.1157\n","Batch 7 --- Loss: 0.1169\n","Batch 8 --- Loss: 0.1174\n","Batch 9 --- Loss: 0.1123\n","Batch 10 --- Loss: 0.1217\n","Batch 11 --- Loss: 0.1176\n","Batch 12 --- Loss: 0.1159\n","Batch 13 --- Loss: 0.1173\n","Batch 14 --- Loss: 0.1180\n","Batch 15 --- Loss: 0.1145\n","Batch 16 --- Loss: 0.1147\n","Batch 17 --- Loss: 0.1183\n","Batch 18 --- Loss: 0.1216\n","Batch 19 --- Loss: 0.1199\n","Batch 20 --- Loss: 0.1155\n","Batch 21 --- Loss: 0.1210\n","Batch 22 --- Loss: 0.1152\n","Batch 23 --- Loss: 0.1163\n","Batch 24 --- Loss: 0.1134\n","Batch 25 --- Loss: 0.1108\n","Epoch 11 / 50 --- Average Loss: 0.1162\n","Average Precision: 0.5504 ---- Average Recall: 0.4828 ---- Average F1: 0.4802 ---- Average Loss: 0.1333\n","Batch 0 --- Loss: 0.1114\n","Batch 1 --- Loss: 0.1172\n","Batch 2 --- Loss: 0.1178\n","Batch 3 --- Loss: 0.1131\n","Batch 4 --- Loss: 0.1177\n","Batch 5 --- Loss: 0.1180\n","Batch 6 --- Loss: 0.1162\n","Batch 7 --- Loss: 0.1122\n","Batch 8 --- Loss: 0.1203\n","Batch 9 --- Loss: 0.1155\n","Batch 10 --- Loss: 0.1163\n","Batch 11 --- Loss: 0.1161\n","Batch 12 --- Loss: 0.1134\n","Batch 13 --- Loss: 0.1179\n","Batch 14 --- Loss: 0.1129\n","Batch 15 --- Loss: 0.1185\n","Batch 16 --- Loss: 0.1146\n","Batch 17 --- Loss: 0.1170\n","Batch 18 --- Loss: 0.1197\n","Batch 19 --- Loss: 0.1107\n","Batch 20 --- Loss: 0.1095\n","Batch 21 --- Loss: 0.1120\n","Batch 22 --- Loss: 0.1087\n","Batch 23 --- Loss: 0.1129\n","Batch 24 --- Loss: 0.1103\n","Batch 25 --- Loss: 0.1210\n","Epoch 12 / 50 --- Average Loss: 0.1150\n","Average Precision: 0.5522 ---- Average Recall: 0.4779 ---- Average F1: 0.4750 ---- Average Loss: 0.1346\n","Batch 0 --- Loss: 0.1201\n","Batch 1 --- Loss: 0.1090\n","Batch 2 --- Loss: 0.1125\n","Batch 3 --- Loss: 0.1136\n","Batch 4 --- Loss: 0.1164\n","Batch 5 --- Loss: 0.1157\n","Batch 6 --- Loss: 0.1131\n","Batch 7 --- Loss: 0.1147\n","Batch 8 --- Loss: 0.1073\n","Batch 9 --- Loss: 0.1143\n","Batch 10 --- Loss: 0.1117\n","Batch 11 --- Loss: 0.1087\n","Batch 12 --- Loss: 0.1126\n","Batch 13 --- Loss: 0.1165\n","Batch 14 --- Loss: 0.1151\n","Batch 15 --- Loss: 0.1116\n","Batch 16 --- Loss: 0.1102\n","Batch 17 --- Loss: 0.1107\n","Batch 18 --- Loss: 0.1170\n","Batch 19 --- Loss: 0.1141\n","Batch 20 --- Loss: 0.1116\n","Batch 21 --- Loss: 0.1129\n","Batch 22 --- Loss: 0.1066\n","Batch 23 --- Loss: 0.1128\n","Batch 24 --- Loss: 0.1121\n","Batch 25 --- Loss: 0.1170\n","Epoch 13 / 50 --- Average Loss: 0.1130\n","Average Precision: 0.5750 ---- Average Recall: 0.4983 ---- Average F1: 0.4974 ---- Average Loss: 0.1329\n","Batch 0 --- Loss: 0.1110\n","Batch 1 --- Loss: 0.1106\n","Batch 2 --- Loss: 0.1130\n","Batch 3 --- Loss: 0.1108\n","Batch 4 --- Loss: 0.1131\n","Batch 5 --- Loss: 0.1070\n","Batch 6 --- Loss: 0.1114\n","Batch 7 --- Loss: 0.1128\n","Batch 8 --- Loss: 0.1142\n","Batch 9 --- Loss: 0.1154\n","Batch 10 --- Loss: 0.1097\n","Batch 11 --- Loss: 0.1163\n","Batch 12 --- Loss: 0.1143\n","Batch 13 --- Loss: 0.1154\n","Batch 14 --- Loss: 0.1152\n","Batch 15 --- Loss: 0.1100\n","Batch 16 --- Loss: 0.1073\n","Batch 17 --- Loss: 0.1091\n","Batch 18 --- Loss: 0.1098\n","Batch 19 --- Loss: 0.1143\n","Batch 20 --- Loss: 0.1127\n","Batch 21 --- Loss: 0.1129\n","Batch 22 --- Loss: 0.1129\n","Batch 23 --- Loss: 0.1158\n","Batch 24 --- Loss: 0.1167\n","Batch 25 --- Loss: 0.1113\n","Epoch 14 / 50 --- Average Loss: 0.1124\n","Average Precision: 0.5645 ---- Average Recall: 0.5271 ---- Average F1: 0.5206 ---- Average Loss: 0.1327\n","Batch 0 --- Loss: 0.1161\n","Batch 1 --- Loss: 0.1121\n","Batch 2 --- Loss: 0.1074\n","Batch 3 --- Loss: 0.1130\n","Batch 4 --- Loss: 0.1086\n","Batch 5 --- Loss: 0.1115\n","Batch 6 --- Loss: 0.1184\n","Batch 7 --- Loss: 0.1094\n","Batch 8 --- Loss: 0.1061\n","Batch 9 --- Loss: 0.1141\n","Batch 10 --- Loss: 0.1104\n","Batch 11 --- Loss: 0.1129\n","Batch 12 --- Loss: 0.1114\n","Batch 13 --- Loss: 0.1110\n","Batch 14 --- Loss: 0.1079\n","Batch 15 --- Loss: 0.1129\n","Batch 16 --- Loss: 0.1102\n","Batch 17 --- Loss: 0.1131\n","Batch 18 --- Loss: 0.1176\n","Batch 19 --- Loss: 0.1119\n","Batch 20 --- Loss: 0.1100\n","Batch 21 --- Loss: 0.1150\n","Batch 22 --- Loss: 0.1072\n","Batch 23 --- Loss: 0.1103\n","Batch 24 --- Loss: 0.1105\n","Batch 25 --- Loss: 0.1084\n","Epoch 15 / 50 --- Average Loss: 0.1114\n","Average Precision: 0.5809 ---- Average Recall: 0.5149 ---- Average F1: 0.5160 ---- Average Loss: 0.1314\n","Batch 0 --- Loss: 0.1084\n","Batch 1 --- Loss: 0.1114\n","Batch 2 --- Loss: 0.1113\n","Batch 3 --- Loss: 0.1103\n","Batch 4 --- Loss: 0.1101\n","Batch 5 --- Loss: 0.1048\n","Batch 6 --- Loss: 0.1086\n","Batch 7 --- Loss: 0.1109\n","Batch 8 --- Loss: 0.1123\n","Batch 9 --- Loss: 0.1136\n","Batch 10 --- Loss: 0.1150\n","Batch 11 --- Loss: 0.1097\n","Batch 12 --- Loss: 0.1154\n","Batch 13 --- Loss: 0.1072\n","Batch 14 --- Loss: 0.1051\n","Batch 15 --- Loss: 0.1112\n","Batch 16 --- Loss: 0.1107\n","Batch 17 --- Loss: 0.1100\n","Batch 18 --- Loss: 0.1133\n","Batch 19 --- Loss: 0.1153\n","Batch 20 --- Loss: 0.1126\n","Batch 21 --- Loss: 0.1083\n","Batch 22 --- Loss: 0.1071\n","Batch 23 --- Loss: 0.1083\n","Batch 24 --- Loss: 0.1059\n","Batch 25 --- Loss: 0.1096\n","Epoch 16 / 50 --- Average Loss: 0.1102\n","Average Precision: 0.5782 ---- Average Recall: 0.5025 ---- Average F1: 0.5018 ---- Average Loss: 0.1316\n","Batch 0 --- Loss: 0.1086\n","Batch 1 --- Loss: 0.1086\n","Batch 2 --- Loss: 0.1100\n","Batch 3 --- Loss: 0.1056\n","Batch 4 --- Loss: 0.1115\n","Batch 5 --- Loss: 0.1088\n","Batch 6 --- Loss: 0.1134\n","Batch 7 --- Loss: 0.1100\n","Batch 8 --- Loss: 0.1111\n","Batch 9 --- Loss: 0.1095\n","Batch 10 --- Loss: 0.1065\n","Batch 11 --- Loss: 0.1075\n","Batch 12 --- Loss: 0.1115\n","Batch 13 --- Loss: 0.1115\n","Batch 14 --- Loss: 0.1048\n","Batch 15 --- Loss: 0.1050\n","Batch 16 --- Loss: 0.1143\n","Batch 17 --- Loss: 0.1132\n","Batch 18 --- Loss: 0.1101\n","Batch 19 --- Loss: 0.1112\n","Batch 20 --- Loss: 0.1094\n","Batch 21 --- Loss: 0.1109\n","Batch 22 --- Loss: 0.1045\n","Batch 23 --- Loss: 0.1083\n","Batch 24 --- Loss: 0.1141\n","Batch 25 --- Loss: 0.1050\n","Epoch 17 / 50 --- Average Loss: 0.1094\n","Average Precision: 0.5782 ---- Average Recall: 0.5248 ---- Average F1: 0.5206 ---- Average Loss: 0.1305\n","Batch 0 --- Loss: 0.1110\n","Batch 1 --- Loss: 0.1089\n","Batch 2 --- Loss: 0.1094\n","Batch 3 --- Loss: 0.1129\n","Batch 4 --- Loss: 0.1115\n","Batch 5 --- Loss: 0.1099\n","Batch 6 --- Loss: 0.1091\n","Batch 7 --- Loss: 0.1099\n","Batch 8 --- Loss: 0.1073\n","Batch 9 --- Loss: 0.1057\n","Batch 10 --- Loss: 0.1069\n","Batch 11 --- Loss: 0.1071\n","Batch 12 --- Loss: 0.1119\n","Batch 13 --- Loss: 0.1102\n","Batch 14 --- Loss: 0.1089\n","Batch 15 --- Loss: 0.1114\n","Batch 16 --- Loss: 0.1083\n","Batch 17 --- Loss: 0.1075\n","Batch 18 --- Loss: 0.1120\n","Batch 19 --- Loss: 0.1148\n","Batch 20 --- Loss: 0.1087\n","Batch 21 --- Loss: 0.1055\n","Batch 22 --- Loss: 0.1095\n","Batch 23 --- Loss: 0.1082\n","Batch 24 --- Loss: 0.1057\n","Batch 25 --- Loss: 0.1114\n","Epoch 18 / 50 --- Average Loss: 0.1094\n","Average Precision: 0.5791 ---- Average Recall: 0.5203 ---- Average F1: 0.5191 ---- Average Loss: 0.1309\n","Batch 0 --- Loss: 0.1091\n","Batch 1 --- Loss: 0.1157\n","Batch 2 --- Loss: 0.1075\n","Batch 3 --- Loss: 0.1070\n","Batch 4 --- Loss: 0.1090\n","Batch 5 --- Loss: 0.1119\n","Batch 6 --- Loss: 0.1084\n","Batch 7 --- Loss: 0.1103\n","Batch 8 --- Loss: 0.1087\n","Batch 9 --- Loss: 0.1097\n","Batch 10 --- Loss: 0.1070\n","Batch 11 --- Loss: 0.1084\n","Batch 12 --- Loss: 0.1115\n","Batch 13 --- Loss: 0.1107\n","Batch 14 --- Loss: 0.1054\n","Batch 15 --- Loss: 0.1081\n","Batch 16 --- Loss: 0.1085\n","Batch 17 --- Loss: 0.1121\n","Batch 18 --- Loss: 0.1099\n","Batch 19 --- Loss: 0.1048\n","Batch 20 --- Loss: 0.1096\n","Batch 21 --- Loss: 0.1070\n","Batch 22 --- Loss: 0.1038\n","Batch 23 --- Loss: 0.1065\n","Batch 24 --- Loss: 0.1120\n","Batch 25 --- Loss: 0.1039\n","Epoch 19 / 50 --- Average Loss: 0.1087\n","Average Precision: 0.5693 ---- Average Recall: 0.5530 ---- Average F1: 0.5392 ---- Average Loss: 0.1299\n","Batch 0 --- Loss: 0.1038\n","Batch 1 --- Loss: 0.1081\n","Batch 2 --- Loss: 0.1078\n","Batch 3 --- Loss: 0.1130\n","Batch 4 --- Loss: 0.1096\n","Batch 5 --- Loss: 0.1105\n","Batch 6 --- Loss: 0.1047\n","Batch 7 --- Loss: 0.1105\n","Batch 8 --- Loss: 0.1131\n","Batch 9 --- Loss: 0.1039\n","Batch 10 --- Loss: 0.1071\n","Batch 11 --- Loss: 0.1097\n","Batch 12 --- Loss: 0.1075\n","Batch 13 --- Loss: 0.1051\n","Batch 14 --- Loss: 0.1044\n","Batch 15 --- Loss: 0.1137\n","Batch 16 --- Loss: 0.1075\n","Batch 17 --- Loss: 0.1089\n","Batch 18 --- Loss: 0.1082\n","Batch 19 --- Loss: 0.1082\n","Batch 20 --- Loss: 0.1118\n","Batch 21 --- Loss: 0.1062\n","Batch 22 --- Loss: 0.1105\n","Batch 23 --- Loss: 0.1073\n","Batch 24 --- Loss: 0.1066\n","Batch 25 --- Loss: 0.1061\n","Epoch 20 / 50 --- Average Loss: 0.1082\n","Average Precision: 0.5849 ---- Average Recall: 0.5164 ---- Average F1: 0.5152 ---- Average Loss: 0.1301\n","Batch 0 --- Loss: 0.1093\n","Batch 1 --- Loss: 0.1056\n","Batch 2 --- Loss: 0.1069\n","Batch 3 --- Loss: 0.1086\n","Batch 4 --- Loss: 0.1081\n","Batch 5 --- Loss: 0.1064\n","Batch 6 --- Loss: 0.1091\n","Batch 7 --- Loss: 0.1061\n","Batch 8 --- Loss: 0.1085\n","Batch 9 --- Loss: 0.1048\n","Batch 10 --- Loss: 0.1072\n","Batch 11 --- Loss: 0.1072\n","Batch 12 --- Loss: 0.1146\n","Batch 13 --- Loss: 0.1116\n","Batch 14 --- Loss: 0.1101\n","Batch 15 --- Loss: 0.1076\n","Batch 16 --- Loss: 0.1078\n","Batch 17 --- Loss: 0.1043\n","Batch 18 --- Loss: 0.1077\n","Batch 19 --- Loss: 0.1076\n","Batch 20 --- Loss: 0.1077\n","Batch 21 --- Loss: 0.1040\n","Batch 22 --- Loss: 0.1057\n","Batch 23 --- Loss: 0.1072\n","Batch 24 --- Loss: 0.1032\n","Batch 25 --- Loss: 0.1105\n","Epoch 21 / 50 --- Average Loss: 0.1076\n","Average Precision: 0.5950 ---- Average Recall: 0.5390 ---- Average F1: 0.5370 ---- Average Loss: 0.1289\n","Batch 0 --- Loss: 0.1097\n","Batch 1 --- Loss: 0.1104\n","Batch 2 --- Loss: 0.1117\n","Batch 3 --- Loss: 0.1081\n","Batch 4 --- Loss: 0.1041\n","Batch 5 --- Loss: 0.1074\n","Batch 6 --- Loss: 0.1039\n","Batch 7 --- Loss: 0.1068\n","Batch 8 --- Loss: 0.1113\n","Batch 9 --- Loss: 0.1021\n","Batch 10 --- Loss: 0.1106\n","Batch 11 --- Loss: 0.1029\n","Batch 12 --- Loss: 0.1068\n","Batch 13 --- Loss: 0.1090\n","Batch 14 --- Loss: 0.1062\n","Batch 15 --- Loss: 0.1097\n","Batch 16 --- Loss: 0.1097\n","Batch 17 --- Loss: 0.1068\n","Batch 18 --- Loss: 0.1055\n","Batch 19 --- Loss: 0.1086\n","Batch 20 --- Loss: 0.1153\n","Batch 21 --- Loss: 0.1095\n","Batch 22 --- Loss: 0.1064\n","Batch 23 --- Loss: 0.1090\n","Batch 24 --- Loss: 0.1093\n","Batch 25 --- Loss: 0.1090\n","Epoch 22 / 50 --- Average Loss: 0.1081\n","Average Precision: 0.5659 ---- Average Recall: 0.5521 ---- Average F1: 0.5338 ---- Average Loss: 0.1353\n","Batch 0 --- Loss: 0.1066\n","Batch 1 --- Loss: 0.1076\n","Batch 2 --- Loss: 0.1112\n","Batch 3 --- Loss: 0.1029\n","Batch 4 --- Loss: 0.1095\n","Batch 5 --- Loss: 0.1051\n","Batch 6 --- Loss: 0.1044\n","Batch 7 --- Loss: 0.1142\n","Batch 8 --- Loss: 0.1065\n","Batch 9 --- Loss: 0.1110\n","Batch 10 --- Loss: 0.1094\n","Batch 11 --- Loss: 0.1077\n","Batch 12 --- Loss: 0.1226\n","Batch 13 --- Loss: 0.1054\n","Batch 14 --- Loss: 0.1070\n","Batch 15 --- Loss: 0.1098\n","Batch 16 --- Loss: 0.1085\n","Batch 17 --- Loss: 0.1060\n","Batch 18 --- Loss: 0.1146\n","Batch 19 --- Loss: 0.1079\n","Batch 20 --- Loss: 0.1033\n","Batch 21 --- Loss: 0.1048\n","Batch 22 --- Loss: 0.1115\n","Batch 23 --- Loss: 0.1108\n","Batch 24 --- Loss: 0.1083\n","Batch 25 --- Loss: 0.1055\n","Epoch 23 / 50 --- Average Loss: 0.1085\n","Average Precision: 0.5702 ---- Average Recall: 0.5366 ---- Average F1: 0.5241 ---- Average Loss: 0.1303\n","Batch 0 --- Loss: 0.1097\n","Batch 1 --- Loss: 0.1030\n","Batch 2 --- Loss: 0.1152\n","Batch 3 --- Loss: 0.1099\n","Batch 4 --- Loss: 0.1126\n","Batch 5 --- Loss: 0.1083\n","Batch 6 --- Loss: 0.1043\n","Batch 7 --- Loss: 0.1103\n","Batch 8 --- Loss: 0.1081\n","Batch 9 --- Loss: 0.1070\n","Batch 10 --- Loss: 0.1046\n","Batch 11 --- Loss: 0.1082\n","Batch 12 --- Loss: 0.1118\n","Batch 13 --- Loss: 0.1089\n","Batch 14 --- Loss: 0.1100\n","Batch 15 --- Loss: 0.1106\n","Batch 16 --- Loss: 0.1096\n","Batch 17 --- Loss: 0.1067\n","Batch 18 --- Loss: 0.1173\n","Batch 19 --- Loss: 0.1066\n","Batch 20 --- Loss: 0.1027\n","Batch 21 --- Loss: 0.1137\n","Batch 22 --- Loss: 0.1074\n","Batch 23 --- Loss: 0.1183\n","Batch 24 --- Loss: 0.1081\n","Batch 25 --- Loss: 0.1081\n","Epoch 24 / 50 --- Average Loss: 0.1093\n","Average Precision: 0.5779 ---- Average Recall: 0.5282 ---- Average F1: 0.5283 ---- Average Loss: 0.1276\n","Batch 0 --- Loss: 0.1076\n","Batch 1 --- Loss: 0.1079\n","Batch 2 --- Loss: 0.1079\n","Batch 3 --- Loss: 0.1056\n","Batch 4 --- Loss: 0.1090\n","Batch 5 --- Loss: 0.1108\n","Batch 6 --- Loss: 0.1083\n","Batch 7 --- Loss: 0.1078\n","Batch 8 --- Loss: 0.1049\n","Batch 9 --- Loss: 0.1135\n","Batch 10 --- Loss: 0.1086\n","Batch 11 --- Loss: 0.1093\n","Batch 12 --- Loss: 0.1102\n","Batch 13 --- Loss: 0.1120\n","Batch 14 --- Loss: 0.1084\n","Batch 15 --- Loss: 0.1098\n","Batch 16 --- Loss: 0.1091\n","Batch 17 --- Loss: 0.1027\n","Batch 18 --- Loss: 0.1063\n","Batch 19 --- Loss: 0.1079\n","Batch 20 --- Loss: 0.1136\n","Batch 21 --- Loss: 0.1065\n","Batch 22 --- Loss: 0.1092\n","Batch 23 --- Loss: 0.1080\n","Batch 24 --- Loss: 0.1127\n","Batch 25 --- Loss: 0.1070\n","Epoch 25 / 50 --- Average Loss: 0.1086\n","Average Precision: 0.5784 ---- Average Recall: 0.5241 ---- Average F1: 0.5181 ---- Average Loss: 0.1299\n","Batch 0 --- Loss: 0.1091\n","Batch 1 --- Loss: 0.1061\n","Batch 2 --- Loss: 0.1101\n","Batch 3 --- Loss: 0.1046\n","Batch 4 --- Loss: 0.1098\n","Batch 5 --- Loss: 0.1079\n","Batch 6 --- Loss: 0.1111\n","Batch 7 --- Loss: 0.1085\n","Batch 8 --- Loss: 0.1055\n","Batch 9 --- Loss: 0.1050\n","Batch 10 --- Loss: 0.1082\n","Batch 11 --- Loss: 0.1114\n","Batch 12 --- Loss: 0.1081\n","Batch 13 --- Loss: 0.1055\n","Batch 14 --- Loss: 0.1045\n","Batch 15 --- Loss: 0.1044\n","Batch 16 --- Loss: 0.1097\n","Batch 17 --- Loss: 0.1056\n","Batch 18 --- Loss: 0.1046\n","Batch 19 --- Loss: 0.1105\n","Batch 20 --- Loss: 0.1072\n","Batch 21 --- Loss: 0.1061\n","Batch 22 --- Loss: 0.1069\n","Batch 23 --- Loss: 0.1080\n","Batch 24 --- Loss: 0.1076\n","Batch 25 --- Loss: 0.1138\n","Epoch 26 / 50 --- Average Loss: 0.1077\n","Average Precision: 0.5807 ---- Average Recall: 0.5563 ---- Average F1: 0.5465 ---- Average Loss: 0.1298\n","Batch 0 --- Loss: 0.1076\n","Batch 1 --- Loss: 0.1091\n","Batch 2 --- Loss: 0.1071\n","Batch 3 --- Loss: 0.1089\n","Batch 4 --- Loss: 0.1104\n","Batch 5 --- Loss: 0.1126\n","Batch 6 --- Loss: 0.1076\n","Batch 7 --- Loss: 0.1048\n","Batch 8 --- Loss: 0.1070\n","Batch 9 --- Loss: 0.1079\n","Batch 10 --- Loss: 0.1046\n","Batch 11 --- Loss: 0.1088\n","Batch 12 --- Loss: 0.1080\n","Batch 13 --- Loss: 0.1073\n","Batch 14 --- Loss: 0.1112\n","Batch 15 --- Loss: 0.1060\n","Batch 16 --- Loss: 0.1034\n","Batch 17 --- Loss: 0.1059\n","Batch 18 --- Loss: 0.1060\n","Batch 19 --- Loss: 0.1112\n","Batch 20 --- Loss: 0.1048\n","Batch 21 --- Loss: 0.1056\n","Batch 22 --- Loss: 0.1082\n","Batch 23 --- Loss: 0.1023\n","Batch 24 --- Loss: 0.1017\n","Batch 25 --- Loss: 0.1079\n","Epoch 27 / 50 --- Average Loss: 0.1072\n","Average Precision: 0.5953 ---- Average Recall: 0.5467 ---- Average F1: 0.5429 ---- Average Loss: 0.1289\n","Batch 0 --- Loss: 0.1050\n","Batch 1 --- Loss: 0.1086\n","Batch 2 --- Loss: 0.1041\n","Batch 3 --- Loss: 0.1070\n","Batch 4 --- Loss: 0.1082\n","Batch 5 --- Loss: 0.1035\n","Batch 6 --- Loss: 0.1056\n","Batch 7 --- Loss: 0.1072\n","Batch 8 --- Loss: 0.1051\n","Batch 9 --- Loss: 0.1062\n","Batch 10 --- Loss: 0.1083\n","Batch 11 --- Loss: 0.1066\n","Batch 12 --- Loss: 0.1061\n","Batch 13 --- Loss: 0.1049\n","Batch 14 --- Loss: 0.1028\n","Batch 15 --- Loss: 0.1053\n","Batch 16 --- Loss: 0.1094\n","Batch 17 --- Loss: 0.1073\n","Batch 18 --- Loss: 0.1084\n","Batch 19 --- Loss: 0.1059\n","Batch 20 --- Loss: 0.1086\n","Batch 21 --- Loss: 0.1107\n","Batch 22 --- Loss: 0.1134\n","Batch 23 --- Loss: 0.1066\n","Batch 24 --- Loss: 0.1072\n","Batch 25 --- Loss: 0.1076\n","Epoch 28 / 50 --- Average Loss: 0.1069\n","Average Precision: 0.5932 ---- Average Recall: 0.5066 ---- Average F1: 0.5089 ---- Average Loss: 0.1301\n","Batch 0 --- Loss: 0.1081\n","Batch 1 --- Loss: 0.1080\n","Batch 2 --- Loss: 0.1033\n","Batch 3 --- Loss: 0.1073\n","Batch 4 --- Loss: 0.1100\n","Batch 5 --- Loss: 0.1101\n","Batch 6 --- Loss: 0.1072\n","Batch 7 --- Loss: 0.1045\n","Batch 8 --- Loss: 0.1128\n","Batch 9 --- Loss: 0.1077\n","Batch 10 --- Loss: 0.1099\n","Batch 11 --- Loss: 0.1073\n","Batch 12 --- Loss: 0.1070\n","Batch 13 --- Loss: 0.1078\n","Batch 14 --- Loss: 0.1152\n","Batch 15 --- Loss: 0.1134\n","Batch 16 --- Loss: 0.1035\n","Batch 17 --- Loss: 0.1106\n","Batch 18 --- Loss: 0.1100\n","Batch 19 --- Loss: 0.1028\n","Batch 20 --- Loss: 0.1081\n","Batch 21 --- Loss: 0.1050\n","Batch 22 --- Loss: 0.1028\n","Batch 23 --- Loss: 0.1070\n","Batch 24 --- Loss: 0.1086\n","Batch 25 --- Loss: 0.1069\n","Epoch 29 / 50 --- Average Loss: 0.1079\n","Average Precision: 0.5729 ---- Average Recall: 0.5539 ---- Average F1: 0.5453 ---- Average Loss: 0.1279\n","Batch 0 --- Loss: 0.1036\n","Batch 1 --- Loss: 0.1017\n","Batch 2 --- Loss: 0.1124\n","Batch 3 --- Loss: 0.1103\n","Batch 4 --- Loss: 0.1065\n","Batch 5 --- Loss: 0.1045\n","Batch 6 --- Loss: 0.1050\n","Batch 7 --- Loss: 0.1067\n","Batch 8 --- Loss: 0.1074\n","Batch 9 --- Loss: 0.1052\n","Batch 10 --- Loss: 0.1029\n","Batch 11 --- Loss: 0.1110\n","Batch 12 --- Loss: 0.1056\n","Batch 13 --- Loss: 0.1069\n","Batch 14 --- Loss: 0.1076\n","Batch 15 --- Loss: 0.1090\n","Batch 16 --- Loss: 0.1109\n","Batch 17 --- Loss: 0.1023\n","Batch 18 --- Loss: 0.1069\n","Batch 19 --- Loss: 0.1032\n","Batch 20 --- Loss: 0.1047\n","Batch 21 --- Loss: 0.1095\n","Batch 22 --- Loss: 0.1055\n","Batch 23 --- Loss: 0.1120\n","Batch 24 --- Loss: 0.1064\n","Batch 25 --- Loss: 0.1038\n","Epoch 30 / 50 --- Average Loss: 0.1066\n","Average Precision: 0.5936 ---- Average Recall: 0.5348 ---- Average F1: 0.5327 ---- Average Loss: 0.1287\n","Batch 0 --- Loss: 0.1011\n","Batch 1 --- Loss: 0.1059\n","Batch 2 --- Loss: 0.1048\n","Batch 3 --- Loss: 0.1045\n","Batch 4 --- Loss: 0.1028\n","Batch 5 --- Loss: 0.1030\n","Batch 6 --- Loss: 0.1020\n","Batch 7 --- Loss: 0.1065\n","Batch 8 --- Loss: 0.1025\n","Batch 9 --- Loss: 0.1110\n","Batch 10 --- Loss: 0.1103\n","Batch 11 --- Loss: 0.1115\n","Batch 12 --- Loss: 0.1032\n","Batch 13 --- Loss: 0.1072\n","Batch 14 --- Loss: 0.1055\n","Batch 15 --- Loss: 0.1038\n","Batch 16 --- Loss: 0.1054\n","Batch 17 --- Loss: 0.1041\n","Batch 18 --- Loss: 0.1051\n","Batch 19 --- Loss: 0.1076\n","Batch 20 --- Loss: 0.1085\n","Batch 21 --- Loss: 0.1103\n","Batch 22 --- Loss: 0.1068\n","Batch 23 --- Loss: 0.1068\n","Batch 24 --- Loss: 0.1039\n","Batch 25 --- Loss: 0.1077\n","Epoch 31 / 50 --- Average Loss: 0.1058\n","Average Precision: 0.5792 ---- Average Recall: 0.5396 ---- Average F1: 0.5350 ---- Average Loss: 0.1279\n","Batch 0 --- Loss: 0.1086\n","Batch 1 --- Loss: 0.1029\n","Batch 2 --- Loss: 0.1082\n","Batch 3 --- Loss: 0.1079\n"]}],"source":["class DeepLabV3(nn.Module):\n","    def __init__(self, num_input_channels, num_classes):\n","        super(DeepLabV3, self).__init__()\n","        self.deeplabv3_weights = torchvision.models.segmentation.DeepLabV3_ResNet101_Weights.DEFAULT\n","        self.resnet101_weights = models.ResNet101_Weights.DEFAULT\n","        self.deeplabv3 = torchvision.models.segmentation.deeplabv3_resnet101(weights=self.deeplabv3_weights, weights_backbone=self.resnet101_weights)\n","        \n","        #Replaces the first convolution of the backbone of the model to accept 6-channel input.\n","        self.deeplabv3.backbone.conv1 = nn.Conv2d(num_input_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n","        \n","        #Replaces the final classifier to change the number of output classes to 4.\n","        self.deeplabv3.classifier[-1] = torch.nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1, stride=1)\n","        \n","    def forward(self, x):\n","        x = self.deeplabv3.forward(x)\n","        return x\n","    \n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.reduction = reduction\n","\n","    def forward(self, input, target):\n","        ce_loss = F.cross_entropy(input, target, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = (1 - pt) ** self.gamma * ce_loss\n","\n","        if self.alpha is not None:\n","            focal_loss = self.alpha * focal_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","    \n","def visualize_results(num_results, predictions, images=None, masks=None, randomize_images=False):\n","    fig, axs = plt.subplots(num_results, 3, figsize=(32, 32))\n","\n","    predictions_flat = [item for sublist in predictions for item in sublist]\n","    if (images != None):\n","        images_flat = [item for sublist in images for item in sublist]\n","    if (masks != None):\n","        masks_flat = [item for sublist in masks for item in sublist]\n","        \n","    if (randomize_images):\n","        # Choose num_results number of images at random from the results.\n","        image_idxs = random.sample(range(0, len(predictions_flat) - 1), num_results)\n","    else:\n","        image_idxs = [i for i in range(num_results + 1)]\n","        \n","    for i in range(num_results):\n","        # Plot the input image and ground truth mask\n","        if (images == None or masks == None):    \n","            image, mask = test_dataset.get_item_no_transforms(image_idxs[i])\n","            \n","            axs[i, 0].imshow(image.numpy()[0:3, :, :].T, aspect='equal')\n","            axs[i, 0].imshow(image.numpy()[3:6, :, :].T, alpha=0.5, aspect='equal')\n","            axs[i, 2].imshow(mask.numpy().T, cmap=\"viridis\", aspect='equal')\n","        else:\n","            image = images_flat[image_idxs[i]]\n","            mask = masks_flat[image_idxs[i]]\n","            \n","            axs[i, 0].imshow(image[0:3, :, :].T, aspect='equal')\n","            axs[i, 0].imshow(image[3:6, :, :].T, alpha=0.5, aspect='equal')\n","            axs[i, 2].imshow(mask.T, cmap=\"viridis\", aspect='equal')\n","\n","        axs[i, 0].set_title(\"Combined Image\")\n","        axs[i, 0].axis('off')\n","        \n","        axs[i, 2].set_title(\"Ground Truth Mask\")\n","        axs[i, 2].axis('off')\n","        \n","        # Plot the predicted image\n","        axs[i, 1].imshow(predictions_flat[image_idxs[i]].T, cmap=\"viridis\", aspect='equal')\n","        axs[i, 1].set_title(\"Predicted Image\")\n","        axs[i, 1].axis('off')\n","\n","    plt.show()\n","\n","batch_size = 8\n","num_input_channels = 6\n","num_classes = 4\n","lr = 1e-4\n","image_size = 224\n","# Whether the models parameters should be saved following the completion of a run.\n","save = False\n","#Whether an existing models parameters should be loaded before the run.\n","load = False\n","\n","horizontal_flip = v2.RandomHorizontalFlip(p=0.5)\n","vertical_flip = v2.RandomVerticalFlip(p=0.5)\n","rotation = v2.RandomRotation(random.randint(1, 359))\n","random_crop = v2.RandomResizedCrop(size=image_size, antialias=True)\n","\n","image_transforms = v2.Compose([\n","    v2.ToImage(),\n","    v2.ToDtype(torch.float32, scale=True),\n","    v2.Resize(image_size, antialias=True),\n","    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  #These are the normalization values used by the pretrained weights in DeepLabv3\n","    #horizontal_flip,\n","    #vertical_flip\n","    #rotation,\n","    #random_crop\n","    ])\n","mask_transforms = v2.Compose([\n","    v2.ToImage(),\n","    v2.ToDtype(torch.int64, scale=False),\n","    v2.Resize(image_size, antialias=True)\n","    #horizontal_flip,\n","    #vertical_flip\n","    #rotation,\n","    #random_crop\n","    ])\n","\n","cwd = os.getcwd()\n","\n","train_dataset = dataloader.HarveyData(os.path.join(cwd, 'dataset/training'), image_transforms=image_transforms, mask_transforms=mask_transforms)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataset = dataloader.HarveyData(os.path.join(cwd, 'dataset/testing'), image_transforms=image_transforms, mask_transforms=mask_transforms)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = DeepLabV3(num_input_channels, num_classes)\n","if (load):\n","    if (os.path.exists('DeepLabv3.pt')):\n","        print(\"Loading model.\")\n","        model.load_state_dict(torch.load('DeepLabv3.pt'))\n","    else:\n","        print('Could not load model. File does not exist.')\n","model.to(device)\n","#model_preprocess = model.deeplabv3_weights.transforms()\n","\n","criterion = torch.nn.CrossEntropyLoss()#FocalLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005)\n","\n","softmax = nn.Softmax(dim=1)\n","\n","num_epochs = 50\n","\n","images = []\n","masks = []\n","predicted_images = []\n","\n","#Training\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for i, data in enumerate(train_dataloader):\n","        image, mask = data\n","        \n","        image = image.to(device)\n","        mask = mask.squeeze().to(device)\n","        \n","        outputs = softmax(model(image)['out'])\n","        \n","        loss = criterion(outputs, mask)\n","        loss.backward()\n","        \n","        optimizer.step()\n","        optimizer.zero_grad()\n","        \n","        epoch_loss += loss.item()\n","\n","        print('Batch %d --- Loss: %.4f' % (i, loss.item() / batch_size))\n","    print('Epoch %d / %d --- Average Loss: %.4f' % (epoch + 1, num_epochs, epoch_loss / train_dataset.__len__()))\n","    \n","    total_loss = 0.0\n","    total_weighted_precision = 0.0\n","    total_weighted_recall = 0.0\n","    total_weighted_f1 = 0.0\n"," \n","#Testing\n","    model.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(test_dataloader):\n","            image, mask = data\n","            \n","            image = image.to(device)\n","            mask = mask.squeeze().to(device)\n","            \n","            outputs = softmax(model(image)['out'])\n","        \n","            loss = criterion(outputs, mask)\n","            total_loss += loss.item()\n","            \n","            predicted = torch.argmax(outputs, dim=1, keepdim=False)\n","            \n","            image = image.cpu().numpy()\n","            mask = mask.cpu().numpy()\n","            predicted = predicted.cpu().numpy()\n","\n","            for i in range(len(mask)):\n","                precision, recall, f1, _ = precision_recall_fscore_support(mask[i].flatten(), predicted[i].flatten(), average='macro', zero_division=0.0)\n","                total_weighted_precision += precision\n","                total_weighted_recall += recall\n","                total_weighted_f1 += f1\n","                      \n","            if (epoch + 1 == num_epochs):\n","                images.append(image)\n","                masks.append(mask)\n","                predicted_images.append(predicted)\n","    \n","    average_weighted_precision = total_weighted_precision / len(test_dataset)\n","    average_weighted_recall = total_weighted_recall / len(test_dataset)\n","    average_weighted_f1 = total_weighted_f1 / len(test_dataset)\n","    average_loss = total_loss / len(test_dataset)\n","\n","    print('Average Precision: %.4f ---- Average Recall: %.4f ---- Average F1: %.4f ---- Average Loss: %.4f' % (average_weighted_precision, average_weighted_recall, average_weighted_f1, average_loss))\n","    \n","    if (epoch + 1 == num_epochs):\n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","        print(f\"Elapsed Time at Epoch {epoch + 1} : {elapsed_time} seconds\")\n","        \n","        if save:\n","            torch.save(model.state_dict(), 'DeepLabv3.pt')\n","            \n","        visualize_results(6, predicted_images, images, masks)\n","        \n","        images.clear()\n","        masks.clear()\n","        predicted_images.clear()\n","        \n","        start_time = time.time()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNO6v/H6tc3abrfxRCEsT6Y","gpuType":"T4","mount_file_id":"1KMWS2Oyo4vVFS2GYmUhqId2DopiI21d6","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
