{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rasterio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kJpUbvTRbFN",
        "outputId": "a193e867-f36b-4232-ec8b-a56c1a55f189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.9-cp310-cp310-manylinux2014_x86_64.whl (20.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2024.2.2)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.25.2)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n",
            "Installing collected packages: snuggs, affine, rasterio\n",
            "Successfully installed affine-2.4.0 rasterio-1.3.9 snuggs-1.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Compose, Resize, v2\n",
        "from torchvision.transforms.functional import to_tensor, hflip, vflip, rotate, adjust_gamma\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "from PIL import Image\n",
        "import rasterio"
      ],
      "metadata": {
        "id": "-lriJGWRzuyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HarveyData(Dataset):\n",
        "    #dataset_dir: Provide a path to either \"./dataset/training\" or \"./dataset/testing\"\n",
        "    #transforms: Any transformations that should be performed on the image when retrieved.\n",
        "    def __init__(self, dataset_dir, image_size = 224, augment_data=True, verbose_logging=False):\n",
        "        super(HarveyData, self).__init__()\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.image_size = image_size\n",
        "        self.augment_data = augment_data\n",
        "\n",
        "        self.pre_image_paths = sorted(os.listdir(os.path.join(dataset_dir, 'pre_img')))\n",
        "        self.post_image_paths = sorted(os.listdir(os.path.join(dataset_dir, 'post_img')))\n",
        "        self.mask_paths = sorted(os.listdir(os.path.join(dataset_dir, 'PDE_labels')))\n",
        "        self.elevation_paths = sorted(os.listdir(os.path.join(dataset_dir, 'elevation')))\n",
        "        self.hand_paths = sorted(os.listdir(os.path.join(dataset_dir, 'hand')))\n",
        "        self.imperviousness_paths = sorted(os.listdir(os.path.join(dataset_dir, 'imperviousness')))\n",
        "        self.distance_coast_paths = sorted(os.listdir(os.path.join(dataset_dir, 'distance_to_coast')))\n",
        "        self.distance_stream_paths = sorted(os.listdir(os.path.join(dataset_dir, 'distance_to_stream')))\n",
        "\n",
        "        self.rain_824_paths = sorted(os.listdir(os.path.join(dataset_dir, 'rain/824')))\n",
        "        self.rain_825_paths = sorted(os.listdir(os.path.join(dataset_dir, 'rain/825')))\n",
        "        self.rain_826_paths = sorted(os.listdir(os.path.join(dataset_dir, 'rain/826')))\n",
        "        self.rain_827_paths = sorted(os.listdir(os.path.join(dataset_dir, 'rain/827')))\n",
        "        self.rain_828_paths = sorted(os.listdir(os.path.join(dataset_dir, 'rain/828')))\n",
        "        self.rain_829_paths = sorted(os.listdir(os.path.join(dataset_dir, 'rain/829')))\n",
        "        self.rain_830_paths = sorted(os.listdir(os.path.join(dataset_dir, 'rain/830')))\n",
        "\n",
        "        self.stream_elev_824_paths = sorted(os.listdir(os.path.join(dataset_dir, 'stream_elev/824')))\n",
        "        self.stream_elev_825_paths = sorted(os.listdir(os.path.join(dataset_dir, 'stream_elev/825')))\n",
        "        self.stream_elev_826_paths = sorted(os.listdir(os.path.join(dataset_dir, 'stream_elev/826')))\n",
        "        self.stream_elev_827_paths = sorted(os.listdir(os.path.join(dataset_dir, 'stream_elev/827')))\n",
        "        self.stream_elev_828_paths = sorted(os.listdir(os.path.join(dataset_dir, 'stream_elev/828')))\n",
        "        self.stream_elev_829_paths = sorted(os.listdir(os.path.join(dataset_dir, 'stream_elev/829')))\n",
        "        self.stream_elev_830_paths = sorted(os.listdir(os.path.join(dataset_dir, 'stream_elev/830')))\n",
        "\n",
        "        self.pre_images = []\n",
        "        self.post_images = []\n",
        "        self.masks = []\n",
        "\n",
        "        self.elevation = []\n",
        "        self.hand = []\n",
        "        self.imperviousness = []\n",
        "        self.distance_coast = []\n",
        "        self.distance_stream = []\n",
        "\n",
        "        self.rain_824 = []\n",
        "        self.rain_825 = []\n",
        "        self.rain_826 = []\n",
        "        self.rain_827 = []\n",
        "        self.rain_828 = []\n",
        "        self.rain_829 = []\n",
        "        self.rain_830 = []\n",
        "\n",
        "        self.stream_elev_824 = []\n",
        "        self.stream_elev_825 = []\n",
        "        self.stream_elev_826 = []\n",
        "        self.stream_elev_827 = []\n",
        "        self.stream_elev_828 = []\n",
        "        self.stream_elev_829 = []\n",
        "        self.stream_elev_830 = []\n",
        "\n",
        "        self.num_images = len(self.pre_image_paths)\n",
        "\n",
        "        for i in range(self.num_images):\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'pre_img', self.pre_image_paths[i])) as src:\n",
        "                pre_image = src.read()\n",
        "                pre_image = torch.tensor(pre_image, dtype=torch.float32)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'pre_img', self.pre_image_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'post_img', self.post_image_paths[i])) as src:\n",
        "                post_image = src.read()\n",
        "                post_image = torch.tensor(post_image, dtype=torch.float32)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'post_img', self.post_image_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'PDE_labels', self.mask_paths[i])) as src:\n",
        "                mask = src.read(1)\n",
        "                mask = torch.tensor(mask, dtype=torch.int64).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'PDE_labels', self.mask_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'elevation', self.elevation_paths[i])) as src:\n",
        "                elevation = src.read(1)\n",
        "                elevation = torch.tensor(elevation, dtype=torch.float32).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'elevation', self.elevation_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'hand', self.hand_paths[i])) as src:\n",
        "                hand = src.read(1)\n",
        "                hand = torch.tensor(hand, dtype=torch.int16).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'hand', self.hand_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'imperviousness', self.imperviousness_paths[i])) as src:\n",
        "                imperviousness = src.read(1)\n",
        "                imperviousness = torch.tensor(imperviousness, dtype=torch.float32).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'imperviousness', self.imperviousness_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'distance_to_coast', self.distance_coast_paths[i])) as src:\n",
        "                distance_coast = src.read(1)\n",
        "                distance_coast = torch.tensor(distance_coast, dtype=torch.float32).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'distance_to_coast', self.distance_coast_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'distance_to_stream', self.distance_stream_paths[i])) as src:\n",
        "                distance_stream = src.read(1)\n",
        "                distance_stream = torch.tensor(distance_stream, dtype=torch.float32).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'distance_to_stream', self.distance_stream_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'rain/824', self.rain_824_paths[i])) as src:\n",
        "                rain_824 = src.read(1)\n",
        "                rain_824 = torch.tensor(rain_824).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'rain/824', self.rain_824_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'rain/825', self.rain_825_paths[i])) as src:\n",
        "                rain_825 = src.read(1)\n",
        "                rain_825 = torch.tensor(rain_825).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'rain/825', self.rain_825_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'rain/826', self.rain_826_paths[i])) as src:\n",
        "                rain_826 = src.read(1)\n",
        "                rain_826 = torch.tensor(rain_826).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'rain/826', self.rain_826_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'rain/827', self.rain_827_paths[i])) as src:\n",
        "                rain_827 = src.read(1)\n",
        "                rain_827 = torch.tensor(rain_827).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'rain/827', self.rain_827_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'rain/828', self.rain_828_paths[i])) as src:\n",
        "                rain_828 = src.read(1)\n",
        "                rain_828 = torch.tensor(rain_828).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'rain/828', self.rain_828_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'rain/829', self.rain_829_paths[i])) as src:\n",
        "                rain_829 = src.read(1)\n",
        "                rain_829 = torch.tensor(rain_829).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'rain/829', self.rain_829_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'rain/830', self.rain_830_paths[i])) as src:\n",
        "                rain_830 = src.read(1)\n",
        "                rain_830 = torch.tensor(rain_830).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'rain/830', self.rain_830_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'stream_elev/824', self.stream_elev_824_paths[i])) as src:\n",
        "                stream_elev_824 = src.read(1)\n",
        "                stream_elev_824 = torch.tensor(stream_elev_824).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'stream_elev/824', self.stream_elev_824_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'stream_elev/825', self.stream_elev_825_paths[i])) as src:\n",
        "                stream_elev_825 = src.read(1)\n",
        "                stream_elev_825 = torch.tensor(stream_elev_825).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'stream_elev/824', self.stream_elev_825_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'stream_elev/826', self.stream_elev_826_paths[i])) as src:\n",
        "                stream_elev_826 = src.read(1)\n",
        "                stream_elev_826 = torch.tensor(stream_elev_826).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'stream_elev/824', self.stream_elev_826_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'stream_elev/827', self.stream_elev_827_paths[i])) as src:\n",
        "                stream_elev_827 = src.read(1)\n",
        "                stream_elev_827 = torch.tensor(stream_elev_827).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'stream_elev/824', self.stream_elev_827_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'stream_elev/828', self.stream_elev_828_paths[i])) as src:\n",
        "                stream_elev_828 = src.read(1)\n",
        "                stream_elev_828 = torch.tensor(stream_elev_828).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'stream_elev/824', self.stream_elev_828_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'stream_elev/829', self.stream_elev_829_paths[i])) as src:\n",
        "                stream_elev_829 = src.read(1)\n",
        "                stream_elev_829 = torch.tensor(stream_elev_829).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'stream_elev/824', self.stream_elev_829_paths[i]))\n",
        "\n",
        "            with rasterio.open(os.path.join(dataset_dir, 'stream_elev/830', self.stream_elev_830_paths[i])) as src:\n",
        "                stream_elev_830 = src.read(1)\n",
        "                stream_elev_830 = torch.tensor(stream_elev_830).unsqueeze(0)\n",
        "                if verbose_logging: print(\"Loading \" + os.path.join(dataset_dir, 'stream_elev/824', self.stream_elev_830_paths[i]))\n",
        "\n",
        "            self.pre_images.append(pre_image)\n",
        "            self.post_images.append(post_image)\n",
        "            self.masks.append(mask)\n",
        "\n",
        "            self.elevation.append(elevation)\n",
        "            self.hand.append(hand)\n",
        "            self.imperviousness.append(imperviousness)\n",
        "            self.distance_coast.append(distance_coast)\n",
        "            self.distance_stream.append(distance_stream)\n",
        "\n",
        "            self.rain_824.append(rain_824)\n",
        "            self.rain_825.append(rain_825)\n",
        "            self.rain_826.append(rain_826)\n",
        "            self.rain_827.append(rain_827)\n",
        "            self.rain_828.append(rain_828)\n",
        "            self.rain_829.append(rain_829)\n",
        "            self.rain_830.append(rain_830)\n",
        "\n",
        "            self.stream_elev_824.append(stream_elev_824)\n",
        "            self.stream_elev_825.append(stream_elev_825)\n",
        "            self.stream_elev_826.append(stream_elev_826)\n",
        "            self.stream_elev_827.append(stream_elev_827)\n",
        "            self.stream_elev_828.append(stream_elev_828)\n",
        "            self.stream_elev_829.append(stream_elev_829)\n",
        "            self.stream_elev_830.append(stream_elev_830)\n",
        "\n",
        "    def normalize_image(self, image):\n",
        "        min = torch.min(image)\n",
        "        max = torch.max(image)\n",
        "        return (image - min) / (max - min)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #Get pre and post image, and the mask, for the current index.\n",
        "        pre_image = self.pre_images[idx]\n",
        "        post_image = self.post_images[idx]\n",
        "        mask = self.masks[idx]\n",
        "\n",
        "        elevation = self.elevation[idx]\n",
        "        hand = self.hand[idx]\n",
        "        imperviousness = self.imperviousness[idx]\n",
        "        distance_coast = self.distance_coast[idx]\n",
        "        distance_stream = self.distance_stream[idx]\n",
        "\n",
        "        rain_824 = self.rain_824[idx]\n",
        "        rain_825 = self.rain_825[idx]\n",
        "        rain_826 = self.rain_826[idx]\n",
        "        rain_827 = self.rain_827[idx]\n",
        "        rain_828 = self.rain_828[idx]\n",
        "        rain_829 = self.rain_829[idx]\n",
        "        rain_830 = self.rain_830[idx]\n",
        "\n",
        "        stream_elev_824 = self.stream_elev_824[idx]\n",
        "        stream_elev_825 = self.stream_elev_825[idx]\n",
        "        stream_elev_826 = self.stream_elev_826[idx]\n",
        "        stream_elev_827 = self.stream_elev_827[idx]\n",
        "        stream_elev_828 = self.stream_elev_828[idx]\n",
        "        stream_elev_829 = self.stream_elev_829[idx]\n",
        "        stream_elev_830 = self.stream_elev_830[idx]\n",
        "\n",
        "        #elevation = elevation.repeat(3, 1, 1)\n",
        "        #hand = hand.repeat(3, 1, 1)\n",
        "        #imperviousness = imperviousness.repeat(3, 1, 1)\n",
        "        #distance_coast = distance_coast.repeat(3, 1, 1)\n",
        "        #distance_stream = distance_stream.repeat(3, 1, 1)\n",
        "\n",
        "        #These are the normalization values used by the pretrained weights in DeepLabv3\n",
        "        mean_normalize_rgb_channels = [0.485, 0.456, 0.406]\n",
        "        std_normalize_rgb_channels = [0.229, 0.224, 0.225]\n",
        "\n",
        "        #Average values for single channel inputs\n",
        "        #mean_normalize_grayscale_channels = [sum(mean_normalize_rgb_channels) / len(mean_normalize_rgb_channels)]\n",
        "        #std_normalize_grayscale_channels = [sum(std_normalize_rgb_channels) / len(std_normalize_rgb_channels)]\n",
        "\n",
        "        image_transforms = v2.Compose([\n",
        "                           v2.ToImage(),\n",
        "                           v2.ToDtype(torch.float32, scale=True),\n",
        "                           v2.Resize((self.image_size, self.image_size), antialias=True),\n",
        "                           v2.Normalize(mean=mean_normalize_rgb_channels, std=std_normalize_rgb_channels)\n",
        "        ])\n",
        "        mask_transforms = v2.Compose([\n",
        "                          #v2.ToImage(),\n",
        "                          #v2.ToDtype(torch.int64, scale=False),\n",
        "                          v2.Resize((self.image_size, self.image_size), antialias=True)\n",
        "        ])\n",
        "        meta_transforms = v2.Compose([\n",
        "                          v2.ToImage(),\n",
        "                          v2.ToDtype(torch.float32, scale=True),\n",
        "                          v2.Resize((self.image_size, self.image_size), antialias=True)\n",
        "        ])\n",
        "        int_meta_transforms = v2.Compose([\n",
        "                              v2.ToImage(),\n",
        "                              v2.ToDtype(torch.int16, scale=True),\n",
        "                              v2.Resize((self.image_size, self.image_size), antialias=True)\n",
        "        ])\n",
        "\n",
        "        pre_image = image_transforms(pre_image)\n",
        "        post_image = image_transforms(post_image)\n",
        "        mask = mask_transforms(mask)\n",
        "\n",
        "        elevation = meta_transforms(elevation)\n",
        "        hand = int_meta_transforms(hand)\n",
        "        imperviousness = meta_transforms(imperviousness)\n",
        "        distance_coast = meta_transforms(distance_coast)\n",
        "        distance_stream = meta_transforms(distance_stream)\n",
        "\n",
        "        rain_824 = meta_transforms(rain_824)\n",
        "        rain_825 = meta_transforms(rain_825)\n",
        "        rain_826 = meta_transforms(rain_826)\n",
        "        rain_827 = meta_transforms(rain_827)\n",
        "        rain_828 = meta_transforms(rain_828)\n",
        "        rain_829 = meta_transforms(rain_829)\n",
        "        rain_830 = meta_transforms(rain_830)\n",
        "\n",
        "        stream_elev_824 = meta_transforms(stream_elev_824)\n",
        "        stream_elev_825 = meta_transforms(stream_elev_825)\n",
        "        stream_elev_826 = meta_transforms(stream_elev_826)\n",
        "        stream_elev_827 = meta_transforms(stream_elev_827)\n",
        "        stream_elev_828 = meta_transforms(stream_elev_828)\n",
        "        stream_elev_829 = meta_transforms(stream_elev_829)\n",
        "        stream_elev_830 = meta_transforms(stream_elev_830)\n",
        "\n",
        "        #distance_coast = self.normalize_image(distance_coast)\n",
        "        #distance_stream = self.normalize_image(distance_stream)\n",
        "\n",
        "        if self.augment_data:\n",
        "            augmentation_switches = {0, 1, 2, 3}\n",
        "            augment_mode_1 = np.random.choice(list(augmentation_switches))\n",
        "            augmentation_switches.remove(augment_mode_1)\n",
        "\n",
        "            additional_augment_chance = np.random.random()\n",
        "            augment_mode_2 = -1\n",
        "            augment_mode_3 = -1\n",
        "\n",
        "            if (additional_augment_chance > 0.5):\n",
        "                augment_mode_2 = np.random.choice(list(augmentation_switches))\n",
        "                augmentation_switches.remove(augment_mode_2)\n",
        "            #if (additional_augment_chance > 0.8):\n",
        "                #augment_mode_3 = np.random.choice(list(augmentation_switches))\n",
        "                #augmentation_switches.remove(augment_mode_3)\n",
        "\n",
        "            if 0 in [augment_mode_1 or augment_mode_2 or augment_mode_3]:\n",
        "                # flip image vertically\n",
        "                pre_image = vflip(pre_image)\n",
        "                post_image = vflip(post_image)\n",
        "\n",
        "                elevation = vflip(elevation)\n",
        "                hand = vflip(hand)\n",
        "                imperviousness = vflip(imperviousness)\n",
        "                distance_coast = vflip(distance_coast)\n",
        "                distance_stream = vflip(distance_stream)\n",
        "\n",
        "                rain_824 = vflip(rain_824)\n",
        "                rain_825 = vflip(rain_825)\n",
        "                rain_826 = vflip(rain_826)\n",
        "                rain_827 = vflip(rain_827)\n",
        "                rain_828 = vflip(rain_828)\n",
        "                rain_829 = vflip(rain_829)\n",
        "                rain_830 = vflip(rain_830)\n",
        "\n",
        "                stream_elev_824 = vflip(stream_elev_824)\n",
        "                stream_elev_825 = vflip(stream_elev_825)\n",
        "                stream_elev_826 = vflip(stream_elev_826)\n",
        "                stream_elev_827 = vflip(stream_elev_827)\n",
        "                stream_elev_828 = vflip(stream_elev_828)\n",
        "                stream_elev_829 = vflip(stream_elev_829)\n",
        "                stream_elev_830 = vflip(stream_elev_830)\n",
        "\n",
        "                mask = vflip(mask)\n",
        "            elif 1 in [augment_mode_1 or augment_mode_2 or augment_mode_3]:\n",
        "                # flip image horizontally\n",
        "                pre_image = hflip(pre_image)\n",
        "                post_image = hflip(post_image)\n",
        "\n",
        "                elevation = hflip(elevation)\n",
        "                hand = hflip(hand)\n",
        "                imperviousness = hflip(imperviousness)\n",
        "                distance_coast = hflip(distance_coast)\n",
        "                distance_stream = hflip(distance_stream)\n",
        "\n",
        "                rain_824 = hflip(rain_824)\n",
        "                rain_825 = hflip(rain_825)\n",
        "                rain_826 = hflip(rain_826)\n",
        "                rain_827 = hflip(rain_827)\n",
        "                rain_828 = hflip(rain_828)\n",
        "                rain_829 = hflip(rain_829)\n",
        "                rain_830 = hflip(rain_830)\n",
        "\n",
        "                stream_elev_824 = hflip(stream_elev_824)\n",
        "                stream_elev_825 = hflip(stream_elev_825)\n",
        "                stream_elev_826 = hflip(stream_elev_826)\n",
        "                stream_elev_827 = hflip(stream_elev_827)\n",
        "                stream_elev_828 = hflip(stream_elev_828)\n",
        "                stream_elev_829 = hflip(stream_elev_829)\n",
        "                stream_elev_830 = hflip(stream_elev_830)\n",
        "\n",
        "                mask = hflip(mask)\n",
        "            elif 2 in [augment_mode_1 or augment_mode_2 or augment_mode_3]:\n",
        "                # crop image\n",
        "                crop = v2.RandomResizedCrop(self.image_size, antialias=True)\n",
        "\n",
        "                pre_image = crop(pre_image)\n",
        "                post_image = crop(post_image)\n",
        "\n",
        "                elevation = crop(elevation)\n",
        "                hand = crop(hand)\n",
        "                imperviousness = crop(imperviousness)\n",
        "                distance_coast = crop(distance_coast)\n",
        "                distance_stream = crop(distance_stream)\n",
        "\n",
        "                rain_824 = crop(rain_824)\n",
        "                rain_825 = crop(rain_825)\n",
        "                rain_826 = crop(rain_826)\n",
        "                rain_827 = crop(rain_827)\n",
        "                rain_828 = crop(rain_828)\n",
        "                rain_829 = crop(rain_829)\n",
        "                rain_830 = crop(rain_830)\n",
        "\n",
        "                stream_elev_824 = crop(stream_elev_824)\n",
        "                stream_elev_825 = crop(stream_elev_825)\n",
        "                stream_elev_826 = crop(stream_elev_826)\n",
        "                stream_elev_827 = crop(stream_elev_827)\n",
        "                stream_elev_828 = crop(stream_elev_828)\n",
        "                stream_elev_829 = crop(stream_elev_829)\n",
        "                stream_elev_830 = crop(stream_elev_830)\n",
        "\n",
        "                mask = crop(mask)\n",
        "            elif 3 in [augment_mode_1 or augment_mode_2 or augment_mode_3]:\n",
        "                # rotate image\n",
        "                random_degree = random.randint(1, 359)\n",
        "\n",
        "                pre_image = rotate(pre_image, random_degree)\n",
        "                post_image = rotate(post_image, random_degree)\n",
        "\n",
        "                elevation = rotate(elevation, random_degree)\n",
        "                hand = rotate(hand, random_degree)\n",
        "                imperviousness = rotate(imperviousness, random_degree)\n",
        "                distance_coast = rotate(distance_coast, random_degree)\n",
        "                distance_stream = rotate(distance_stream, random_degree)\n",
        "\n",
        "                rain_824 = rotate(rain_824, random_degree)\n",
        "                rain_825 = rotate(rain_825, random_degree)\n",
        "                rain_826 = rotate(rain_826, random_degree)\n",
        "                rain_827 = rotate(rain_827, random_degree)\n",
        "                rain_828 = rotate(rain_828, random_degree)\n",
        "                rain_829 = rotate(rain_829, random_degree)\n",
        "\n",
        "                stream_elev_824 = rotate(stream_elev_824, random_degree)\n",
        "                stream_elev_825 = rotate(stream_elev_825, random_degree)\n",
        "                stream_elev_826 = rotate(stream_elev_826, random_degree)\n",
        "                stream_elev_827 = rotate(stream_elev_827, random_degree)\n",
        "                stream_elev_828 = rotate(stream_elev_828, random_degree)\n",
        "                stream_elev_829 = rotate(stream_elev_829, random_degree)\n",
        "                stream_elev_830 = rotate(stream_elev_830, random_degree)\n",
        "\n",
        "                mask = rotate(mask, random_degree)\n",
        "\n",
        "        #Concatenate the pre and post disaster images, as well as the meta-attributes, together along the channel dimension.\n",
        "        combined_image = torch.cat([pre_image, post_image, elevation, hand, imperviousness, distance_coast, distance_stream,\n",
        "                                    rain_824, rain_825, rain_826, rain_827, rain_828, rain_829, rain_830,\n",
        "                                    stream_elev_824, stream_elev_825, stream_elev_826, stream_elev_827, stream_elev_828, stream_elev_829, stream_elev_830], dim=0)\n",
        "        #combined_image = torch.cat([pre_image, post_image], dim=0)\n",
        "        return combined_image, mask\n",
        "\n",
        "    def get_item_resize_only(self, idx, image_size):\n",
        "        #Get pre and post image, and the mask, for the current index.\n",
        "        pre_image = self.pre_images[idx]\n",
        "        post_image = self.post_images[idx]\n",
        "        mask = self.masks[idx]\n",
        "\n",
        "        elevation = self.elevation[idx]\n",
        "        hand = self.hand[idx]\n",
        "        imperviousness = self.imperviousness[idx]\n",
        "\n",
        "        #Convert image to normalized tensor.\n",
        "        pre_image = to_tensor(pre_image)\n",
        "        post_image = to_tensor(post_image)\n",
        "\n",
        "        mask = to_tensor(mask)\n",
        "        mask *= 255  # Manually adjust the label values back to the original values after the normalization of to_tensor()\n",
        "\n",
        "        elevation = to_tensor(elevation)\n",
        "        hand = to_tensor(hand)\n",
        "        imperviousness = to_tensor(imperviousness)\n",
        "\n",
        "        #Resize the images to the same size as was used during training.\n",
        "        resize = v2.Compose([v2.Resize((image_size, image_size), antialias=True)])\n",
        "        pre_image = resize(pre_image)\n",
        "        post_image = resize(post_image)\n",
        "        mask = resize(mask)\n",
        "\n",
        "        elevation = resize(elevation)\n",
        "        hand = resize(hand)\n",
        "        imperviousness = resize(imperviousness)\n",
        "\n",
        "        #Concatenate the pre and post disaster images, as well as the meta attributes, together along the channel dimension.\n",
        "        combined_image = torch.cat([pre_image, post_image, elevation, imperviousness], dim=0)\n",
        "        return combined_image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_images"
      ],
      "metadata": {
        "id": "PnwOdQzvzr8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepLabV3(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_classes):\n",
        "        super(DeepLabV3, self).__init__()\n",
        "        self.deeplabv3_weights = torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT\n",
        "        self.resnet50_weights = models.ResNet50_Weights.DEFAULT\n",
        "        self.deeplabv3 = torchvision.models.segmentation.deeplabv3_resnet50(weights=self.deeplabv3_weights, weights_backbone=self.resnet50_weights)\n",
        "\n",
        "        #Replaces the first convolution of the backbone of the model to accept 6-channel input.\n",
        "        self.deeplabv3.backbone.conv1 = nn.Conv2d(num_input_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        #Replaces the final classifier to change the number of output classes to 4.\n",
        "        self.deeplabv3.classifier[-1] = torch.nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.deeplabv3.forward(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QQ6jLPz9zy0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            focal_loss = self.alpha * focal_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ],
      "metadata": {
        "id": "4as-yETN0JRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDnqqojUpKLE",
        "outputId": "2d881aab-36e8-4fe2-bec9-df4de673ceea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train images\n",
            "Loading test images\n",
            "Finished loading images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n",
            "100%|██████████| 161M/161M [00:10<00:00, 16.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 --- Loss: 0.4661\n",
            "Batch 1 --- Loss: 0.4612\n",
            "Batch 2 --- Loss: 0.4696\n",
            "Batch 3 --- Loss: 0.4633\n",
            "Batch 4 --- Loss: 0.4602\n",
            "Batch 5 --- Loss: 0.4466\n",
            "Batch 6 --- Loss: 0.4604\n",
            "Batch 7 --- Loss: 0.4559\n",
            "Batch 8 --- Loss: 0.4503\n",
            "Batch 9 --- Loss: 0.4552\n",
            "Batch 10 --- Loss: 0.4224\n",
            "Batch 11 --- Loss: 0.4316\n",
            "Batch 12 --- Loss: 0.4193\n",
            "Batch 13 --- Loss: 0.4167\n",
            "Batch 14 --- Loss: 0.4045\n",
            "Batch 15 --- Loss: 0.4425\n",
            "Batch 16 --- Loss: 0.4129\n",
            "Batch 17 --- Loss: 0.4040\n",
            "Batch 18 --- Loss: 0.4190\n",
            "Batch 19 --- Loss: 0.3882\n",
            "Batch 20 --- Loss: 0.4112\n",
            "Batch 21 --- Loss: 0.3968\n",
            "Batch 22 --- Loss: 0.3903\n",
            "Batch 23 --- Loss: 0.3845\n",
            "Batch 24 --- Loss: 0.3852\n",
            "Batch 25 --- Loss: 0.3893\n",
            "Batch 26 --- Loss: 0.3700\n",
            "Batch 27 --- Loss: 0.3843\n",
            "Batch 28 --- Loss: 0.3811\n",
            "Batch 29 --- Loss: 0.3697\n",
            "Batch 30 --- Loss: 0.3654\n",
            "Batch 31 --- Loss: 0.3341\n",
            "Batch 32 --- Loss: 0.3606\n",
            "Batch 33 --- Loss: 0.3487\n",
            "Batch 34 --- Loss: 0.3490\n",
            "Batch 35 --- Loss: 0.3131\n",
            "Batch 36 --- Loss: 0.3380\n",
            "Batch 37 --- Loss: 0.3342\n",
            "Batch 38 --- Loss: 0.2958\n",
            "Batch 39 --- Loss: 0.3259\n",
            "Batch 40 --- Loss: 0.3265\n",
            "Batch 41 --- Loss: 0.2725\n",
            "Batch 42 --- Loss: 0.3315\n",
            "Batch 43 --- Loss: 0.3133\n",
            "Batch 44 --- Loss: 0.2958\n",
            "Batch 45 --- Loss: 0.2729\n",
            "Batch 46 --- Loss: 0.3310\n",
            "Batch 47 --- Loss: 0.2959\n",
            "Batch 48 --- Loss: 0.2925\n",
            "Batch 49 --- Loss: 0.3131\n",
            "Batch 50 --- Loss: 0.2781\n",
            "Batch 51 --- Loss: 0.2476\n",
            "Batch 52 --- Loss: 0.3029\n",
            "Batch 53 --- Loss: 0.2610\n",
            "Batch 54 --- Loss: 0.2476\n",
            "Batch 55 --- Loss: 0.2627\n",
            "Batch 56 --- Loss: 0.2706\n",
            "Batch 57 --- Loss: 0.3503\n",
            "Batch 58 --- Loss: 0.2080\n",
            "Batch 59 --- Loss: 0.2602\n",
            "Batch 60 --- Loss: 0.2725\n",
            "Batch 61 --- Loss: 0.3342\n",
            "Batch 62 --- Loss: 0.2782\n",
            "Batch 63 --- Loss: 0.2761\n",
            "Batch 64 --- Loss: 0.3072\n",
            "Batch 65 --- Loss: 0.2885\n",
            "Batch 66 --- Loss: 0.2279\n",
            "Batch 67 --- Loss: 0.3122\n",
            "Batch 68 --- Loss: 0.2735\n",
            "Batch 69 --- Loss: 0.2514\n",
            "Batch 70 --- Loss: 0.2713\n",
            "Batch 71 --- Loss: 0.2049\n",
            "Batch 72 --- Loss: 0.2741\n",
            "Batch 73 --- Loss: 0.3330\n",
            "Batch 74 --- Loss: 0.2669\n",
            "Batch 75 --- Loss: 0.2565\n",
            "Batch 76 --- Loss: 0.2331\n",
            "Batch 77 --- Loss: 0.2506\n",
            "Batch 78 --- Loss: 0.2757\n",
            "Batch 79 --- Loss: 0.2496\n",
            "Batch 80 --- Loss: 0.2482\n",
            "Batch 81 --- Loss: 0.3008\n",
            "Batch 82 --- Loss: 0.2552\n",
            "Batch 83 --- Loss: 0.3289\n",
            "Batch 84 --- Loss: 0.2416\n",
            "Epoch 1 / 50 --- Average Loss: 0.3356\n",
            "Average Macro Precision: 0.2173 ---- Average Macro Recall: 0.2228 ---- Average F1 Score: 0.2109 ---- Average Loss: 0.2213\n",
            "Average No Damage Precision: 0.0981 ---- Average No Damage Recall: 0.0093 ---- Average No Damage F1: 0.0157\n",
            "Average Minor Precision: 0.0070 ---- Average Minor Recall: 0.0044 ---- Average Minor F1: 0.0045\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0033 ---- Average Major Recall: 0.0025 ---- Average Major F1: 0.0025\n",
            "Average Background Precision: 0.8644 ---- Average Background Recall: 0.9848 ---- Average Background F1: 0.9186\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_1_0.21092799844789295.pt\n",
            "Batch 0 --- Loss: 0.2319\n",
            "Batch 1 --- Loss: 0.2405\n",
            "Batch 2 --- Loss: 0.2378\n",
            "Batch 3 --- Loss: 0.2778\n",
            "Batch 4 --- Loss: 0.2639\n",
            "Batch 5 --- Loss: 0.2410\n",
            "Batch 6 --- Loss: 0.2742\n",
            "Batch 7 --- Loss: 0.3105\n",
            "Batch 8 --- Loss: 0.2296\n",
            "Batch 9 --- Loss: 0.3113\n",
            "Batch 10 --- Loss: 0.2139\n",
            "Batch 11 --- Loss: 0.1882\n",
            "Batch 12 --- Loss: 0.2154\n",
            "Batch 13 --- Loss: 0.2167\n",
            "Batch 14 --- Loss: 0.1603\n",
            "Batch 15 --- Loss: 0.2729\n",
            "Batch 16 --- Loss: 0.2280\n",
            "Batch 17 --- Loss: 0.1781\n",
            "Batch 18 --- Loss: 0.2359\n",
            "Batch 19 --- Loss: 0.2569\n",
            "Batch 20 --- Loss: 0.2488\n",
            "Batch 21 --- Loss: 0.2093\n",
            "Batch 22 --- Loss: 0.1984\n",
            "Batch 23 --- Loss: 0.2136\n",
            "Batch 24 --- Loss: 0.2348\n",
            "Batch 25 --- Loss: 0.2234\n",
            "Batch 26 --- Loss: 0.2066\n",
            "Batch 27 --- Loss: 0.2486\n",
            "Batch 28 --- Loss: 0.2023\n",
            "Batch 29 --- Loss: 0.1948\n",
            "Batch 30 --- Loss: 0.2268\n",
            "Batch 31 --- Loss: 0.1618\n",
            "Batch 32 --- Loss: 0.2154\n",
            "Batch 33 --- Loss: 0.1785\n",
            "Batch 34 --- Loss: 0.2009\n",
            "Batch 35 --- Loss: 0.1529\n",
            "Batch 36 --- Loss: 0.1685\n",
            "Batch 37 --- Loss: 0.2042\n",
            "Batch 38 --- Loss: 0.1307\n",
            "Batch 39 --- Loss: 0.1500\n",
            "Batch 40 --- Loss: 0.2350\n",
            "Batch 41 --- Loss: 0.1345\n",
            "Batch 42 --- Loss: 0.1830\n",
            "Batch 43 --- Loss: 0.2175\n",
            "Batch 44 --- Loss: 0.2137\n",
            "Batch 45 --- Loss: 0.1796\n",
            "Batch 46 --- Loss: 0.2175\n",
            "Batch 47 --- Loss: 0.1918\n",
            "Batch 48 --- Loss: 0.2266\n",
            "Batch 49 --- Loss: 0.2091\n",
            "Batch 50 --- Loss: 0.1712\n",
            "Batch 51 --- Loss: 0.1466\n",
            "Batch 52 --- Loss: 0.1820\n",
            "Batch 53 --- Loss: 0.1894\n",
            "Batch 54 --- Loss: 0.1731\n",
            "Batch 55 --- Loss: 0.2734\n",
            "Batch 56 --- Loss: 0.1694\n",
            "Batch 57 --- Loss: 0.2425\n",
            "Batch 58 --- Loss: 0.1322\n",
            "Batch 59 --- Loss: 0.1981\n",
            "Batch 60 --- Loss: 0.1307\n",
            "Batch 61 --- Loss: 0.2422\n",
            "Batch 62 --- Loss: 0.1914\n",
            "Batch 63 --- Loss: 0.1963\n",
            "Batch 64 --- Loss: 0.2772\n",
            "Batch 65 --- Loss: 0.2532\n",
            "Batch 66 --- Loss: 0.1593\n",
            "Batch 67 --- Loss: 0.2287\n",
            "Batch 68 --- Loss: 0.1265\n",
            "Batch 69 --- Loss: 0.1950\n",
            "Batch 70 --- Loss: 0.2091\n",
            "Batch 71 --- Loss: 0.1429\n",
            "Batch 72 --- Loss: 0.2534\n",
            "Batch 73 --- Loss: 0.2408\n",
            "Batch 74 --- Loss: 0.2652\n",
            "Batch 75 --- Loss: 0.2268\n",
            "Batch 76 --- Loss: 0.1802\n",
            "Batch 77 --- Loss: 0.1770\n",
            "Batch 78 --- Loss: 0.2429\n",
            "Batch 79 --- Loss: 0.1629\n",
            "Batch 80 --- Loss: 0.1144\n",
            "Batch 81 --- Loss: 0.1854\n",
            "Batch 82 --- Loss: 0.1313\n",
            "Batch 83 --- Loss: 0.2585\n",
            "Batch 84 --- Loss: 0.2074\n",
            "Epoch 2 / 50 --- Average Loss: 0.2075\n",
            "Average Macro Precision: 0.2613 ---- Average Macro Recall: 0.2609 ---- Average F1 Score: 0.2465 ---- Average Loss: 0.1795\n",
            "Average No Damage Precision: 0.1347 ---- Average No Damage Recall: 0.0020 ---- Average No Damage F1: 0.0039\n",
            "Average Minor Precision: 0.0019 ---- Average Minor Recall: 0.0007 ---- Average Minor F1: 0.0007\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0023 ---- Average Major Recall: 0.0005 ---- Average Major F1: 0.0006\n",
            "Average Background Precision: 0.8646 ---- Average Background Recall: 0.9984 ---- Average Background F1: 0.9244\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_2_0.24653138350027443.pt\n",
            "Batch 0 --- Loss: 0.1827\n",
            "Batch 1 --- Loss: 0.1434\n",
            "Batch 2 --- Loss: 0.1814\n",
            "Batch 3 --- Loss: 0.2074\n",
            "Batch 4 --- Loss: 0.1832\n",
            "Batch 5 --- Loss: 0.1746\n",
            "Batch 6 --- Loss: 0.2104\n",
            "Batch 7 --- Loss: 0.1959\n",
            "Batch 8 --- Loss: 0.1816\n",
            "Batch 9 --- Loss: 0.1822\n",
            "Batch 10 --- Loss: 0.1509\n",
            "Batch 11 --- Loss: 0.1368\n",
            "Batch 12 --- Loss: 0.2063\n",
            "Batch 13 --- Loss: 0.1503\n",
            "Batch 14 --- Loss: 0.1314\n",
            "Batch 15 --- Loss: 0.2484\n",
            "Batch 16 --- Loss: 0.2063\n",
            "Batch 17 --- Loss: 0.1239\n",
            "Batch 18 --- Loss: 0.1722\n",
            "Batch 19 --- Loss: 0.1399\n",
            "Batch 20 --- Loss: 0.1621\n",
            "Batch 21 --- Loss: 0.1576\n",
            "Batch 22 --- Loss: 0.1428\n",
            "Batch 23 --- Loss: 0.1365\n",
            "Batch 24 --- Loss: 0.1396\n",
            "Batch 25 --- Loss: 0.1634\n",
            "Batch 26 --- Loss: 0.2159\n",
            "Batch 27 --- Loss: 0.1700\n",
            "Batch 28 --- Loss: 0.1196\n",
            "Batch 29 --- Loss: 0.1419\n",
            "Batch 30 --- Loss: 0.1497\n",
            "Batch 31 --- Loss: 0.1564\n",
            "Batch 32 --- Loss: 0.1665\n",
            "Batch 33 --- Loss: 0.1358\n",
            "Batch 34 --- Loss: 0.1441\n",
            "Batch 35 --- Loss: 0.1613\n",
            "Batch 36 --- Loss: 0.1551\n",
            "Batch 37 --- Loss: 0.1047\n",
            "Batch 38 --- Loss: 0.0968\n",
            "Batch 39 --- Loss: 0.1765\n",
            "Batch 40 --- Loss: 0.1691\n",
            "Batch 41 --- Loss: 0.1288\n",
            "Batch 42 --- Loss: 0.1286\n",
            "Batch 43 --- Loss: 0.1411\n",
            "Batch 44 --- Loss: 0.1451\n",
            "Batch 45 --- Loss: 0.1434\n",
            "Batch 46 --- Loss: 0.1825\n",
            "Batch 47 --- Loss: 0.1604\n",
            "Batch 48 --- Loss: 0.1654\n",
            "Batch 49 --- Loss: 0.1716\n",
            "Batch 50 --- Loss: 0.1204\n",
            "Batch 51 --- Loss: 0.1349\n",
            "Batch 52 --- Loss: 0.1637\n",
            "Batch 53 --- Loss: 0.1427\n",
            "Batch 54 --- Loss: 0.1350\n",
            "Batch 55 --- Loss: 0.1536\n",
            "Batch 56 --- Loss: 0.1389\n",
            "Batch 57 --- Loss: 0.2173\n",
            "Batch 58 --- Loss: 0.1552\n",
            "Batch 59 --- Loss: 0.1899\n",
            "Batch 60 --- Loss: 0.0857\n",
            "Batch 61 --- Loss: 0.2250\n",
            "Batch 62 --- Loss: 0.1351\n",
            "Batch 63 --- Loss: 0.1767\n",
            "Batch 64 --- Loss: 0.2053\n",
            "Batch 65 --- Loss: 0.1506\n",
            "Batch 66 --- Loss: 0.1463\n",
            "Batch 67 --- Loss: 0.2485\n",
            "Batch 68 --- Loss: 0.1411\n",
            "Batch 69 --- Loss: 0.3204\n",
            "Batch 70 --- Loss: 0.1863\n",
            "Batch 71 --- Loss: 0.1393\n",
            "Batch 72 --- Loss: 0.2128\n",
            "Batch 73 --- Loss: 0.2290\n",
            "Batch 74 --- Loss: 0.2084\n",
            "Batch 75 --- Loss: 0.1751\n",
            "Batch 76 --- Loss: 0.1914\n",
            "Batch 77 --- Loss: 0.1403\n",
            "Batch 78 --- Loss: 0.2038\n",
            "Batch 79 --- Loss: 0.1092\n",
            "Batch 80 --- Loss: 0.1112\n",
            "Batch 81 --- Loss: 0.1710\n",
            "Batch 82 --- Loss: 0.1164\n",
            "Batch 83 --- Loss: 0.2738\n",
            "Batch 84 --- Loss: 0.1663\n",
            "Epoch 3 / 50 --- Average Loss: 0.1654\n",
            "Average Macro Precision: 0.3048 ---- Average Macro Recall: 0.2475 ---- Average F1 Score: 0.2353 ---- Average Loss: 0.1657\n",
            "Average No Damage Precision: 0.4309 ---- Average No Damage Recall: 0.0134 ---- Average No Damage F1: 0.0248\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8657 ---- Average Background Recall: 0.9968 ---- Average Background F1: 0.9244\n",
            "Batch 0 --- Loss: 0.1763\n",
            "Batch 1 --- Loss: 0.1314\n",
            "Batch 2 --- Loss: 0.1605\n",
            "Batch 3 --- Loss: 0.1909\n",
            "Batch 4 --- Loss: 0.1587\n",
            "Batch 5 --- Loss: 0.1501\n",
            "Batch 6 --- Loss: 0.1820\n",
            "Batch 7 --- Loss: 0.1895\n",
            "Batch 8 --- Loss: 0.1643\n",
            "Batch 9 --- Loss: 0.1839\n",
            "Batch 10 --- Loss: 0.1440\n",
            "Batch 11 --- Loss: 0.1293\n",
            "Batch 12 --- Loss: 0.1687\n",
            "Batch 13 --- Loss: 0.1338\n",
            "Batch 14 --- Loss: 0.0867\n",
            "Batch 15 --- Loss: 0.1710\n",
            "Batch 16 --- Loss: 0.1491\n",
            "Batch 17 --- Loss: 0.0940\n",
            "Batch 18 --- Loss: 0.1747\n",
            "Batch 19 --- Loss: 0.1480\n",
            "Batch 20 --- Loss: 0.2213\n",
            "Batch 21 --- Loss: 0.1444\n",
            "Batch 22 --- Loss: 0.1604\n",
            "Batch 23 --- Loss: 0.1338\n",
            "Batch 24 --- Loss: 0.1149\n",
            "Batch 25 --- Loss: 0.1604\n",
            "Batch 26 --- Loss: 0.1489\n",
            "Batch 27 --- Loss: 0.1511\n",
            "Batch 28 --- Loss: 0.1167\n",
            "Batch 29 --- Loss: 0.1154\n",
            "Batch 30 --- Loss: 0.1395\n",
            "Batch 31 --- Loss: 0.1075\n",
            "Batch 32 --- Loss: 0.1505\n",
            "Batch 33 --- Loss: 0.1425\n",
            "Batch 34 --- Loss: 0.1259\n",
            "Batch 35 --- Loss: 0.0920\n",
            "Batch 36 --- Loss: 0.0884\n",
            "Batch 37 --- Loss: 0.0840\n",
            "Batch 38 --- Loss: 0.1048\n",
            "Batch 39 --- Loss: 0.1111\n",
            "Batch 40 --- Loss: 0.1628\n",
            "Batch 41 --- Loss: 0.0742\n",
            "Batch 42 --- Loss: 0.1120\n",
            "Batch 43 --- Loss: 0.1626\n",
            "Batch 44 --- Loss: 0.1324\n",
            "Batch 45 --- Loss: 0.0940\n",
            "Batch 46 --- Loss: 0.1684\n",
            "Batch 47 --- Loss: 0.1329\n",
            "Batch 48 --- Loss: 0.1362\n",
            "Batch 49 --- Loss: 0.1248\n",
            "Batch 50 --- Loss: 0.1136\n",
            "Batch 51 --- Loss: 0.0850\n",
            "Batch 52 --- Loss: 0.1582\n",
            "Batch 53 --- Loss: 0.1325\n",
            "Batch 54 --- Loss: 0.1470\n",
            "Batch 55 --- Loss: 0.1563\n",
            "Batch 56 --- Loss: 0.1266\n",
            "Batch 57 --- Loss: 0.1941\n",
            "Batch 58 --- Loss: 0.0830\n",
            "Batch 59 --- Loss: 0.1691\n",
            "Batch 60 --- Loss: 0.0785\n",
            "Batch 61 --- Loss: 0.2056\n",
            "Batch 62 --- Loss: 0.1437\n",
            "Batch 63 --- Loss: 0.1193\n",
            "Batch 64 --- Loss: 0.2094\n",
            "Batch 65 --- Loss: 0.1261\n",
            "Batch 66 --- Loss: 0.1368\n",
            "Batch 67 --- Loss: 0.2164\n",
            "Batch 68 --- Loss: 0.0963\n",
            "Batch 69 --- Loss: 0.1352\n",
            "Batch 70 --- Loss: 0.1594\n",
            "Batch 71 --- Loss: 0.1083\n",
            "Batch 72 --- Loss: 0.2142\n",
            "Batch 73 --- Loss: 0.2198\n",
            "Batch 74 --- Loss: 0.1877\n",
            "Batch 75 --- Loss: 0.1747\n",
            "Batch 76 --- Loss: 0.1373\n",
            "Batch 77 --- Loss: 0.1256\n",
            "Batch 78 --- Loss: 0.1812\n",
            "Batch 79 --- Loss: 0.0904\n",
            "Batch 80 --- Loss: 0.1042\n",
            "Batch 81 --- Loss: 0.2367\n",
            "Batch 82 --- Loss: 0.1329\n",
            "Batch 83 --- Loss: 0.2323\n",
            "Batch 84 --- Loss: 0.1080\n",
            "Epoch 4 / 50 --- Average Loss: 0.1441\n",
            "Average Macro Precision: 0.3049 ---- Average Macro Recall: 0.2621 ---- Average F1 Score: 0.2486 ---- Average Loss: 0.1548\n",
            "Average No Damage Precision: 0.3559 ---- Average No Damage Recall: 0.0079 ---- Average No Damage F1: 0.0146\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8654 ---- Average Background Recall: 0.9995 ---- Average Background F1: 0.9253\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_4_0.2485912079052639.pt\n",
            "Batch 0 --- Loss: 0.1438\n",
            "Batch 1 --- Loss: 0.1269\n",
            "Batch 2 --- Loss: 0.1590\n",
            "Batch 3 --- Loss: 0.1514\n",
            "Batch 4 --- Loss: 0.1360\n",
            "Batch 5 --- Loss: 0.1277\n",
            "Batch 6 --- Loss: 0.1726\n",
            "Batch 7 --- Loss: 0.1702\n",
            "Batch 8 --- Loss: 0.1271\n",
            "Batch 9 --- Loss: 0.1719\n",
            "Batch 10 --- Loss: 0.1051\n",
            "Batch 11 --- Loss: 0.0807\n",
            "Batch 12 --- Loss: 0.1159\n",
            "Batch 13 --- Loss: 0.1090\n",
            "Batch 14 --- Loss: 0.0713\n",
            "Batch 15 --- Loss: 0.1547\n",
            "Batch 16 --- Loss: 0.1751\n",
            "Batch 17 --- Loss: 0.1044\n",
            "Batch 18 --- Loss: 0.1405\n",
            "Batch 19 --- Loss: 0.1031\n",
            "Batch 20 --- Loss: 0.1572\n",
            "Batch 21 --- Loss: 0.1093\n",
            "Batch 22 --- Loss: 0.1105\n",
            "Batch 23 --- Loss: 0.0997\n",
            "Batch 24 --- Loss: 0.1025\n",
            "Batch 25 --- Loss: 0.1670\n",
            "Batch 26 --- Loss: 0.1491\n",
            "Batch 27 --- Loss: 0.1287\n",
            "Batch 28 --- Loss: 0.0988\n",
            "Batch 29 --- Loss: 0.0943\n",
            "Batch 30 --- Loss: 0.1226\n",
            "Batch 31 --- Loss: 0.0875\n",
            "Batch 32 --- Loss: 0.1366\n",
            "Batch 33 --- Loss: 0.1375\n",
            "Batch 34 --- Loss: 0.1179\n",
            "Batch 35 --- Loss: 0.1023\n",
            "Batch 36 --- Loss: 0.1239\n",
            "Batch 37 --- Loss: 0.0662\n",
            "Batch 38 --- Loss: 0.0938\n",
            "Batch 39 --- Loss: 0.0944\n",
            "Batch 40 --- Loss: 0.1338\n",
            "Batch 41 --- Loss: 0.0783\n",
            "Batch 42 --- Loss: 0.0982\n",
            "Batch 43 --- Loss: 0.0988\n",
            "Batch 44 --- Loss: 0.1275\n",
            "Batch 45 --- Loss: 0.0961\n",
            "Batch 46 --- Loss: 0.1683\n",
            "Batch 47 --- Loss: 0.1486\n",
            "Batch 48 --- Loss: 0.1413\n",
            "Batch 49 --- Loss: 0.1173\n",
            "Batch 50 --- Loss: 0.1093\n",
            "Batch 51 --- Loss: 0.0778\n",
            "Batch 52 --- Loss: 0.1324\n",
            "Batch 53 --- Loss: 0.1073\n",
            "Batch 54 --- Loss: 0.1217\n",
            "Batch 55 --- Loss: 0.1158\n",
            "Batch 56 --- Loss: 0.1009\n",
            "Batch 57 --- Loss: 0.1712\n",
            "Batch 58 --- Loss: 0.0610\n",
            "Batch 59 --- Loss: 0.1440\n",
            "Batch 60 --- Loss: 0.0610\n",
            "Batch 61 --- Loss: 0.2021\n",
            "Batch 62 --- Loss: 0.1287\n",
            "Batch 63 --- Loss: 0.1354\n",
            "Batch 64 --- Loss: 0.1892\n",
            "Batch 65 --- Loss: 0.1288\n",
            "Batch 66 --- Loss: 0.1261\n",
            "Batch 67 --- Loss: 0.1469\n",
            "Batch 68 --- Loss: 0.0922\n",
            "Batch 69 --- Loss: 0.1304\n",
            "Batch 70 --- Loss: 0.1472\n",
            "Batch 71 --- Loss: 0.1243\n",
            "Batch 72 --- Loss: 0.1831\n",
            "Batch 73 --- Loss: 0.2091\n",
            "Batch 74 --- Loss: 0.1484\n",
            "Batch 75 --- Loss: 0.1551\n",
            "Batch 76 --- Loss: 0.1195\n",
            "Batch 77 --- Loss: 0.1085\n",
            "Batch 78 --- Loss: 0.2518\n",
            "Batch 79 --- Loss: 0.0956\n",
            "Batch 80 --- Loss: 0.0953\n",
            "Batch 81 --- Loss: 0.1775\n",
            "Batch 82 --- Loss: 0.0958\n",
            "Batch 83 --- Loss: 0.1992\n",
            "Batch 84 --- Loss: 0.0967\n",
            "Epoch 5 / 50 --- Average Loss: 0.1276\n",
            "Average Macro Precision: 0.3213 ---- Average Macro Recall: 0.2644 ---- Average F1 Score: 0.2528 ---- Average Loss: 0.1586\n",
            "Average No Damage Precision: 0.4366 ---- Average No Damage Recall: 0.0196 ---- Average No Damage F1: 0.0343\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8672 ---- Average Background Recall: 0.9995 ---- Average Background F1: 0.9264\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_5_0.25275031104056805.pt\n",
            "Batch 0 --- Loss: 0.1566\n",
            "Batch 1 --- Loss: 0.1189\n",
            "Batch 2 --- Loss: 0.1678\n",
            "Batch 3 --- Loss: 0.1444\n",
            "Batch 4 --- Loss: 0.1440\n",
            "Batch 5 --- Loss: 0.1117\n",
            "Batch 6 --- Loss: 0.1830\n",
            "Batch 7 --- Loss: 0.1874\n",
            "Batch 8 --- Loss: 0.1713\n",
            "Batch 9 --- Loss: 0.1615\n",
            "Batch 10 --- Loss: 0.1056\n",
            "Batch 11 --- Loss: 0.0933\n",
            "Batch 12 --- Loss: 0.1823\n",
            "Batch 13 --- Loss: 0.1166\n",
            "Batch 14 --- Loss: 0.0771\n",
            "Batch 15 --- Loss: 0.1922\n",
            "Batch 16 --- Loss: 0.1430\n",
            "Batch 17 --- Loss: 0.1057\n",
            "Batch 18 --- Loss: 0.1614\n",
            "Batch 19 --- Loss: 0.0863\n",
            "Batch 20 --- Loss: 0.1214\n",
            "Batch 21 --- Loss: 0.1290\n",
            "Batch 22 --- Loss: 0.1064\n",
            "Batch 23 --- Loss: 0.1000\n",
            "Batch 24 --- Loss: 0.1319\n",
            "Batch 25 --- Loss: 0.1261\n",
            "Batch 26 --- Loss: 0.1337\n",
            "Batch 27 --- Loss: 0.1716\n",
            "Batch 28 --- Loss: 0.1033\n",
            "Batch 29 --- Loss: 0.1141\n",
            "Batch 30 --- Loss: 0.1154\n",
            "Batch 31 --- Loss: 0.0931\n",
            "Batch 32 --- Loss: 0.1773\n",
            "Batch 33 --- Loss: 0.1119\n",
            "Batch 34 --- Loss: 0.1327\n",
            "Batch 35 --- Loss: 0.0797\n",
            "Batch 36 --- Loss: 0.1024\n",
            "Batch 37 --- Loss: 0.0716\n",
            "Batch 38 --- Loss: 0.0606\n",
            "Batch 39 --- Loss: 0.0920\n",
            "Batch 40 --- Loss: 0.1406\n",
            "Batch 41 --- Loss: 0.0549\n",
            "Batch 42 --- Loss: 0.0882\n",
            "Batch 43 --- Loss: 0.1210\n",
            "Batch 44 --- Loss: 0.1493\n",
            "Batch 45 --- Loss: 0.1099\n",
            "Batch 46 --- Loss: 0.1309\n",
            "Batch 47 --- Loss: 0.1297\n",
            "Batch 48 --- Loss: 0.1695\n",
            "Batch 49 --- Loss: 0.1361\n",
            "Batch 50 --- Loss: 0.0927\n",
            "Batch 51 --- Loss: 0.0678\n",
            "Batch 52 --- Loss: 0.0976\n",
            "Batch 53 --- Loss: 0.1107\n",
            "Batch 54 --- Loss: 0.0824\n",
            "Batch 55 --- Loss: 0.1193\n",
            "Batch 56 --- Loss: 0.1304\n",
            "Batch 57 --- Loss: 0.1822\n",
            "Batch 58 --- Loss: 0.0584\n",
            "Batch 59 --- Loss: 0.1264\n",
            "Batch 60 --- Loss: 0.0708\n",
            "Batch 61 --- Loss: 0.1879\n",
            "Batch 62 --- Loss: 0.0938\n",
            "Batch 63 --- Loss: 0.1075\n",
            "Batch 64 --- Loss: 0.1903\n",
            "Batch 65 --- Loss: 0.1178\n",
            "Batch 66 --- Loss: 0.1529\n",
            "Batch 67 --- Loss: 0.2233\n",
            "Batch 68 --- Loss: 0.0811\n",
            "Batch 69 --- Loss: 0.1332\n",
            "Batch 70 --- Loss: 0.1321\n",
            "Batch 71 --- Loss: 0.1009\n",
            "Batch 72 --- Loss: 0.2072\n",
            "Batch 73 --- Loss: 0.1695\n",
            "Batch 74 --- Loss: 0.2614\n",
            "Batch 75 --- Loss: 0.1572\n",
            "Batch 76 --- Loss: 0.1104\n",
            "Batch 77 --- Loss: 0.1945\n",
            "Batch 78 --- Loss: 0.1690\n",
            "Batch 79 --- Loss: 0.0837\n",
            "Batch 80 --- Loss: 0.0690\n",
            "Batch 81 --- Loss: 0.1422\n",
            "Batch 82 --- Loss: 0.0839\n",
            "Batch 83 --- Loss: 0.1988\n",
            "Batch 84 --- Loss: 0.0869\n",
            "Epoch 6 / 50 --- Average Loss: 0.1283\n",
            "Average Macro Precision: 0.2692 ---- Average Macro Recall: 0.2608 ---- Average F1 Score: 0.2459 ---- Average Loss: 0.1641\n",
            "Average No Damage Precision: 0.1783 ---- Average No Damage Recall: 0.0009 ---- Average No Damage F1: 0.0017\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8644 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9250\n",
            "Batch 0 --- Loss: 0.1079\n",
            "Batch 1 --- Loss: 0.1038\n",
            "Batch 2 --- Loss: 0.1714\n",
            "Batch 3 --- Loss: 0.1390\n",
            "Batch 4 --- Loss: 0.1613\n",
            "Batch 5 --- Loss: 0.1734\n",
            "Batch 6 --- Loss: 0.1537\n",
            "Batch 7 --- Loss: 0.1456\n",
            "Batch 8 --- Loss: 0.1091\n",
            "Batch 9 --- Loss: 0.1434\n",
            "Batch 10 --- Loss: 0.0945\n",
            "Batch 11 --- Loss: 0.0661\n",
            "Batch 12 --- Loss: 0.0971\n",
            "Batch 13 --- Loss: 0.1041\n",
            "Batch 14 --- Loss: 0.1302\n",
            "Batch 15 --- Loss: 0.2013\n",
            "Batch 16 --- Loss: 0.1178\n",
            "Batch 17 --- Loss: 0.0720\n",
            "Batch 18 --- Loss: 0.1296\n",
            "Batch 19 --- Loss: 0.0907\n",
            "Batch 20 --- Loss: 0.1495\n",
            "Batch 21 --- Loss: 0.1155\n",
            "Batch 22 --- Loss: 0.1081\n",
            "Batch 23 --- Loss: 0.1140\n",
            "Batch 24 --- Loss: 0.0892\n",
            "Batch 25 --- Loss: 0.1521\n",
            "Batch 26 --- Loss: 0.0969\n",
            "Batch 27 --- Loss: 0.1649\n",
            "Batch 28 --- Loss: 0.1311\n",
            "Batch 29 --- Loss: 0.1444\n",
            "Batch 30 --- Loss: 0.1058\n",
            "Batch 31 --- Loss: 0.0716\n",
            "Batch 32 --- Loss: 0.1221\n",
            "Batch 33 --- Loss: 0.1068\n",
            "Batch 34 --- Loss: 0.1039\n",
            "Batch 35 --- Loss: 0.0535\n",
            "Batch 36 --- Loss: 0.0893\n",
            "Batch 37 --- Loss: 0.1205\n",
            "Batch 38 --- Loss: 0.0658\n",
            "Batch 39 --- Loss: 0.0975\n",
            "Batch 40 --- Loss: 0.1884\n",
            "Batch 41 --- Loss: 0.0781\n",
            "Batch 42 --- Loss: 0.1082\n",
            "Batch 43 --- Loss: 0.0958\n",
            "Batch 44 --- Loss: 0.1162\n",
            "Batch 45 --- Loss: 0.0782\n",
            "Batch 46 --- Loss: 0.1598\n",
            "Batch 47 --- Loss: 0.1500\n",
            "Batch 48 --- Loss: 0.1205\n",
            "Batch 49 --- Loss: 0.0981\n",
            "Batch 50 --- Loss: 0.0931\n",
            "Batch 51 --- Loss: 0.0703\n",
            "Batch 52 --- Loss: 0.0958\n",
            "Batch 53 --- Loss: 0.1253\n",
            "Batch 54 --- Loss: 0.0783\n",
            "Batch 55 --- Loss: 0.0894\n",
            "Batch 56 --- Loss: 0.0867\n",
            "Batch 57 --- Loss: 0.1673\n",
            "Batch 58 --- Loss: 0.1115\n",
            "Batch 59 --- Loss: 0.1127\n",
            "Batch 60 --- Loss: 0.0472\n",
            "Batch 61 --- Loss: 0.1891\n",
            "Batch 62 --- Loss: 0.1213\n",
            "Batch 63 --- Loss: 0.0957\n",
            "Batch 64 --- Loss: 0.1553\n",
            "Batch 65 --- Loss: 0.1049\n",
            "Batch 66 --- Loss: 0.0925\n",
            "Batch 67 --- Loss: 0.1931\n",
            "Batch 68 --- Loss: 0.0567\n",
            "Batch 69 --- Loss: 0.1143\n",
            "Batch 70 --- Loss: 0.1735\n",
            "Batch 71 --- Loss: 0.0717\n",
            "Batch 72 --- Loss: 0.1638\n",
            "Batch 73 --- Loss: 0.2058\n",
            "Batch 74 --- Loss: 0.1397\n",
            "Batch 75 --- Loss: 0.1157\n",
            "Batch 76 --- Loss: 0.1142\n",
            "Batch 77 --- Loss: 0.1242\n",
            "Batch 78 --- Loss: 0.1510\n",
            "Batch 79 --- Loss: 0.0584\n",
            "Batch 80 --- Loss: 0.0515\n",
            "Batch 81 --- Loss: 0.1581\n",
            "Batch 82 --- Loss: 0.0885\n",
            "Batch 83 --- Loss: 0.2607\n",
            "Batch 84 --- Loss: 0.0834\n",
            "Epoch 7 / 50 --- Average Loss: 0.1185\n",
            "Average Macro Precision: 0.3321 ---- Average Macro Recall: 0.2614 ---- Average F1 Score: 0.2472 ---- Average Loss: 0.1675\n",
            "Average No Damage Precision: 0.4925 ---- Average No Damage Recall: 0.0040 ---- Average No Damage F1: 0.0077\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8649 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9252\n",
            "Batch 0 --- Loss: 0.1579\n",
            "Batch 1 --- Loss: 0.0999\n",
            "Batch 2 --- Loss: 0.1204\n",
            "Batch 3 --- Loss: 0.1826\n",
            "Batch 4 --- Loss: 0.1374\n",
            "Batch 5 --- Loss: 0.1540\n",
            "Batch 6 --- Loss: 0.1351\n",
            "Batch 7 --- Loss: 0.1532\n",
            "Batch 8 --- Loss: 0.1095\n",
            "Batch 9 --- Loss: 0.1599\n",
            "Batch 10 --- Loss: 0.1052\n",
            "Batch 11 --- Loss: 0.0780\n",
            "Batch 12 --- Loss: 0.0913\n",
            "Batch 13 --- Loss: 0.0869\n",
            "Batch 14 --- Loss: 0.1001\n",
            "Batch 15 --- Loss: 0.1992\n",
            "Batch 16 --- Loss: 0.1328\n",
            "Batch 17 --- Loss: 0.1555\n",
            "Batch 18 --- Loss: 0.1536\n",
            "Batch 19 --- Loss: 0.0744\n",
            "Batch 20 --- Loss: 0.1721\n",
            "Batch 21 --- Loss: 0.1559\n",
            "Batch 22 --- Loss: 0.1008\n",
            "Batch 23 --- Loss: 0.1324\n",
            "Batch 24 --- Loss: 0.0976\n",
            "Batch 25 --- Loss: 0.1881\n",
            "Batch 26 --- Loss: 0.1288\n",
            "Batch 27 --- Loss: 0.1221\n",
            "Batch 28 --- Loss: 0.0898\n",
            "Batch 29 --- Loss: 0.0966\n",
            "Batch 30 --- Loss: 0.1236\n",
            "Batch 31 --- Loss: 0.0972\n",
            "Batch 32 --- Loss: 0.1698\n",
            "Batch 33 --- Loss: 0.1133\n",
            "Batch 34 --- Loss: 0.1019\n",
            "Batch 35 --- Loss: 0.0709\n",
            "Batch 36 --- Loss: 0.0878\n",
            "Batch 37 --- Loss: 0.0613\n",
            "Batch 38 --- Loss: 0.0731\n",
            "Batch 39 --- Loss: 0.1119\n",
            "Batch 40 --- Loss: 0.1608\n",
            "Batch 41 --- Loss: 0.0439\n",
            "Batch 42 --- Loss: 0.0848\n",
            "Batch 43 --- Loss: 0.1174\n",
            "Batch 44 --- Loss: 0.1191\n",
            "Batch 45 --- Loss: 0.0629\n",
            "Batch 46 --- Loss: 0.1867\n",
            "Batch 47 --- Loss: 0.1074\n",
            "Batch 48 --- Loss: 0.1489\n",
            "Batch 49 --- Loss: 0.0901\n",
            "Batch 50 --- Loss: 0.0871\n",
            "Batch 51 --- Loss: 0.0561\n",
            "Batch 52 --- Loss: 0.0936\n",
            "Batch 53 --- Loss: 0.0885\n",
            "Batch 54 --- Loss: 0.0634\n",
            "Batch 55 --- Loss: 0.1133\n",
            "Batch 56 --- Loss: 0.1117\n",
            "Batch 57 --- Loss: 0.2034\n",
            "Batch 58 --- Loss: 0.0571\n",
            "Batch 59 --- Loss: 0.1376\n",
            "Batch 60 --- Loss: 0.0841\n",
            "Batch 61 --- Loss: 0.1306\n",
            "Batch 62 --- Loss: 0.0832\n",
            "Batch 63 --- Loss: 0.0953\n",
            "Batch 64 --- Loss: 0.2503\n",
            "Batch 65 --- Loss: 0.0935\n",
            "Batch 66 --- Loss: 0.1187\n",
            "Batch 67 --- Loss: 0.1755\n",
            "Batch 68 --- Loss: 0.0604\n",
            "Batch 69 --- Loss: 0.1057\n",
            "Batch 70 --- Loss: 0.1408\n",
            "Batch 71 --- Loss: 0.0808\n",
            "Batch 72 --- Loss: 0.1987\n",
            "Batch 73 --- Loss: 0.2527\n",
            "Batch 74 --- Loss: 0.1598\n",
            "Batch 75 --- Loss: 0.1096\n",
            "Batch 76 --- Loss: 0.1451\n",
            "Batch 77 --- Loss: 0.1205\n",
            "Batch 78 --- Loss: 0.1568\n",
            "Batch 79 --- Loss: 0.0460\n",
            "Batch 80 --- Loss: 0.0759\n",
            "Batch 81 --- Loss: 0.1588\n",
            "Batch 82 --- Loss: 0.0883\n",
            "Batch 83 --- Loss: 0.2489\n",
            "Batch 84 --- Loss: 0.1296\n",
            "Epoch 8 / 50 --- Average Loss: 0.1215\n",
            "Average Macro Precision: 0.3070 ---- Average Macro Recall: 0.2616 ---- Average F1 Score: 0.2476 ---- Average Loss: 0.1403\n",
            "Average No Damage Precision: 0.3671 ---- Average No Damage Recall: 0.0050 ---- Average No Damage F1: 0.0096\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8649 ---- Average Background Recall: 0.9999 ---- Average Background F1: 0.9252\n",
            "Batch 0 --- Loss: 0.1242\n",
            "Batch 1 --- Loss: 0.0892\n",
            "Batch 2 --- Loss: 0.1211\n",
            "Batch 3 --- Loss: 0.2555\n",
            "Batch 4 --- Loss: 0.1213\n",
            "Batch 5 --- Loss: 0.1454\n",
            "Batch 6 --- Loss: 0.2167\n",
            "Batch 7 --- Loss: 0.1729\n",
            "Batch 8 --- Loss: 0.0931\n",
            "Batch 9 --- Loss: 0.1465\n",
            "Batch 10 --- Loss: 0.0871\n",
            "Batch 11 --- Loss: 0.0586\n",
            "Batch 12 --- Loss: 0.1187\n",
            "Batch 13 --- Loss: 0.0911\n",
            "Batch 14 --- Loss: 0.0879\n",
            "Batch 15 --- Loss: 0.2748\n",
            "Batch 16 --- Loss: 0.1196\n",
            "Batch 17 --- Loss: 0.0694\n",
            "Batch 18 --- Loss: 0.2064\n",
            "Batch 19 --- Loss: 0.0783\n",
            "Batch 20 --- Loss: 0.1684\n",
            "Batch 21 --- Loss: 0.0913\n",
            "Batch 22 --- Loss: 0.0883\n",
            "Batch 23 --- Loss: 0.1106\n",
            "Batch 24 --- Loss: 0.1198\n",
            "Batch 25 --- Loss: 0.1602\n",
            "Batch 26 --- Loss: 0.0975\n",
            "Batch 27 --- Loss: 0.1103\n",
            "Batch 28 --- Loss: 0.1327\n",
            "Batch 29 --- Loss: 0.1036\n",
            "Batch 30 --- Loss: 0.1050\n",
            "Batch 31 --- Loss: 0.0777\n",
            "Batch 32 --- Loss: 0.1211\n",
            "Batch 33 --- Loss: 0.0964\n",
            "Batch 34 --- Loss: 0.1117\n",
            "Batch 35 --- Loss: 0.0423\n",
            "Batch 36 --- Loss: 0.1129\n",
            "Batch 37 --- Loss: 0.0545\n",
            "Batch 38 --- Loss: 0.0655\n",
            "Batch 39 --- Loss: 0.0856\n",
            "Batch 40 --- Loss: 0.1695\n",
            "Batch 41 --- Loss: 0.0393\n",
            "Batch 42 --- Loss: 0.0949\n",
            "Batch 43 --- Loss: 0.1624\n",
            "Batch 44 --- Loss: 0.1347\n",
            "Batch 45 --- Loss: 0.0672\n",
            "Batch 46 --- Loss: 0.1485\n",
            "Batch 47 --- Loss: 0.1067\n",
            "Batch 48 --- Loss: 0.1510\n",
            "Batch 49 --- Loss: 0.1763\n",
            "Batch 50 --- Loss: 0.0984\n",
            "Batch 51 --- Loss: 0.0632\n",
            "Batch 52 --- Loss: 0.1457\n",
            "Batch 53 --- Loss: 0.1088\n",
            "Batch 54 --- Loss: 0.0759\n",
            "Batch 55 --- Loss: 0.1016\n",
            "Batch 56 --- Loss: 0.1160\n",
            "Batch 57 --- Loss: 0.1567\n",
            "Batch 58 --- Loss: 0.0546\n",
            "Batch 59 --- Loss: 0.1443\n",
            "Batch 60 --- Loss: 0.0474\n",
            "Batch 61 --- Loss: 0.1425\n",
            "Batch 62 --- Loss: 0.0784\n",
            "Batch 63 --- Loss: 0.1664\n",
            "Batch 64 --- Loss: 0.1718\n",
            "Batch 65 --- Loss: 0.0961\n",
            "Batch 66 --- Loss: 0.0768\n",
            "Batch 67 --- Loss: 0.1455\n",
            "Batch 68 --- Loss: 0.0798\n",
            "Batch 69 --- Loss: 0.0929\n",
            "Batch 70 --- Loss: 0.1508\n",
            "Batch 71 --- Loss: 0.0917\n",
            "Batch 72 --- Loss: 0.1627\n",
            "Batch 73 --- Loss: 0.2240\n",
            "Batch 74 --- Loss: 0.1497\n",
            "Batch 75 --- Loss: 0.1578\n",
            "Batch 76 --- Loss: 0.0896\n",
            "Batch 77 --- Loss: 0.0956\n",
            "Batch 78 --- Loss: 0.1322\n",
            "Batch 79 --- Loss: 0.0558\n",
            "Batch 80 --- Loss: 0.0610\n",
            "Batch 81 --- Loss: 0.1431\n",
            "Batch 82 --- Loss: 0.0968\n",
            "Batch 83 --- Loss: 0.2448\n",
            "Batch 84 --- Loss: 0.0882\n",
            "Epoch 9 / 50 --- Average Loss: 0.1187\n",
            "Average Macro Precision: 0.3213 ---- Average Macro Recall: 0.2647 ---- Average F1 Score: 0.2534 ---- Average Loss: 0.1451\n",
            "Average No Damage Precision: 0.4360 ---- Average No Damage Recall: 0.0208 ---- Average No Damage F1: 0.0373\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8674 ---- Average Background Recall: 0.9998 ---- Average Background F1: 0.9267\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_9_0.25341440994195846.pt\n",
            "Batch 0 --- Loss: 0.0943\n",
            "Batch 1 --- Loss: 0.0861\n",
            "Batch 2 --- Loss: 0.1224\n",
            "Batch 3 --- Loss: 0.1568\n",
            "Batch 4 --- Loss: 0.0871\n",
            "Batch 5 --- Loss: 0.1118\n",
            "Batch 6 --- Loss: 0.2039\n",
            "Batch 7 --- Loss: 0.1519\n",
            "Batch 8 --- Loss: 0.1045\n",
            "Batch 9 --- Loss: 0.1239\n",
            "Batch 10 --- Loss: 0.0828\n",
            "Batch 11 --- Loss: 0.0556\n",
            "Batch 12 --- Loss: 0.0797\n",
            "Batch 13 --- Loss: 0.0755\n",
            "Batch 14 --- Loss: 0.0638\n",
            "Batch 15 --- Loss: 0.1718\n",
            "Batch 16 --- Loss: 0.1352\n",
            "Batch 17 --- Loss: 0.1048\n",
            "Batch 18 --- Loss: 0.1095\n",
            "Batch 19 --- Loss: 0.1152\n",
            "Batch 20 --- Loss: 0.1798\n",
            "Batch 21 --- Loss: 0.0712\n",
            "Batch 22 --- Loss: 0.0994\n",
            "Batch 23 --- Loss: 0.1255\n",
            "Batch 24 --- Loss: 0.0762\n",
            "Batch 25 --- Loss: 0.1013\n",
            "Batch 26 --- Loss: 0.0878\n",
            "Batch 27 --- Loss: 0.1216\n",
            "Batch 28 --- Loss: 0.0715\n",
            "Batch 29 --- Loss: 0.0952\n",
            "Batch 30 --- Loss: 0.0868\n",
            "Batch 31 --- Loss: 0.0741\n",
            "Batch 32 --- Loss: 0.1198\n",
            "Batch 33 --- Loss: 0.1255\n",
            "Batch 34 --- Loss: 0.0996\n",
            "Batch 35 --- Loss: 0.0494\n",
            "Batch 36 --- Loss: 0.0791\n",
            "Batch 37 --- Loss: 0.0524\n",
            "Batch 38 --- Loss: 0.0688\n",
            "Batch 39 --- Loss: 0.0782\n",
            "Batch 40 --- Loss: 0.1456\n",
            "Batch 41 --- Loss: 0.0357\n",
            "Batch 42 --- Loss: 0.0928\n",
            "Batch 43 --- Loss: 0.0807\n",
            "Batch 44 --- Loss: 0.1519\n",
            "Batch 45 --- Loss: 0.0647\n",
            "Batch 46 --- Loss: 0.1086\n",
            "Batch 47 --- Loss: 0.1395\n",
            "Batch 48 --- Loss: 0.1189\n",
            "Batch 49 --- Loss: 0.1263\n",
            "Batch 50 --- Loss: 0.0809\n",
            "Batch 51 --- Loss: 0.0850\n",
            "Batch 52 --- Loss: 0.0824\n",
            "Batch 53 --- Loss: 0.1215\n",
            "Batch 54 --- Loss: 0.0796\n",
            "Batch 55 --- Loss: 0.1253\n",
            "Batch 56 --- Loss: 0.1250\n",
            "Batch 57 --- Loss: 0.1262\n",
            "Batch 58 --- Loss: 0.0486\n",
            "Batch 59 --- Loss: 0.2189\n",
            "Batch 60 --- Loss: 0.0764\n",
            "Batch 61 --- Loss: 0.2124\n",
            "Batch 62 --- Loss: 0.0782\n",
            "Batch 63 --- Loss: 0.0940\n",
            "Batch 64 --- Loss: 0.1911\n",
            "Batch 65 --- Loss: 0.1629\n",
            "Batch 66 --- Loss: 0.0887\n",
            "Batch 67 --- Loss: 0.2333\n",
            "Batch 68 --- Loss: 0.0946\n",
            "Batch 69 --- Loss: 0.1002\n",
            "Batch 70 --- Loss: 0.1815\n",
            "Batch 71 --- Loss: 0.0721\n",
            "Batch 72 --- Loss: 0.1532\n",
            "Batch 73 --- Loss: 0.2370\n",
            "Batch 74 --- Loss: 0.1615\n",
            "Batch 75 --- Loss: 0.1207\n",
            "Batch 76 --- Loss: 0.1071\n",
            "Batch 77 --- Loss: 0.1112\n",
            "Batch 78 --- Loss: 0.1385\n",
            "Batch 79 --- Loss: 0.0640\n",
            "Batch 80 --- Loss: 0.0489\n",
            "Batch 81 --- Loss: 0.1474\n",
            "Batch 82 --- Loss: 0.0844\n",
            "Batch 83 --- Loss: 0.2117\n",
            "Batch 84 --- Loss: 0.0856\n",
            "Epoch 10 / 50 --- Average Loss: 0.1119\n",
            "Average Macro Precision: 0.3225 ---- Average Macro Recall: 0.2651 ---- Average F1 Score: 0.2631 ---- Average Loss: 0.1462\n",
            "Average No Damage Precision: 0.5080 ---- Average No Damage Recall: 0.1102 ---- Average No Damage F1: 0.1611\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8774 ---- Average Background Recall: 0.9879 ---- Average Background F1: 0.9272\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_10_0.2631058947369297.pt\n",
            "Batch 0 --- Loss: 0.1118\n",
            "Batch 1 --- Loss: 0.1027\n",
            "Batch 2 --- Loss: 0.1745\n",
            "Batch 3 --- Loss: 0.1729\n",
            "Batch 4 --- Loss: 0.1504\n",
            "Batch 5 --- Loss: 0.1199\n",
            "Batch 6 --- Loss: 0.1438\n",
            "Batch 7 --- Loss: 0.1877\n",
            "Batch 8 --- Loss: 0.1258\n",
            "Batch 9 --- Loss: 0.1151\n",
            "Batch 10 --- Loss: 0.0882\n",
            "Batch 11 --- Loss: 0.0525\n",
            "Batch 12 --- Loss: 0.0923\n",
            "Batch 13 --- Loss: 0.1324\n",
            "Batch 14 --- Loss: 0.0830\n",
            "Batch 15 --- Loss: 0.1560\n",
            "Batch 16 --- Loss: 0.1383\n",
            "Batch 17 --- Loss: 0.0629\n",
            "Batch 18 --- Loss: 0.2171\n",
            "Batch 19 --- Loss: 0.0681\n",
            "Batch 20 --- Loss: 0.1865\n",
            "Batch 21 --- Loss: 0.1151\n",
            "Batch 22 --- Loss: 0.0847\n",
            "Batch 23 --- Loss: 0.1066\n",
            "Batch 24 --- Loss: 0.1129\n",
            "Batch 25 --- Loss: 0.1485\n",
            "Batch 26 --- Loss: 0.0911\n",
            "Batch 27 --- Loss: 0.1566\n",
            "Batch 28 --- Loss: 0.0891\n",
            "Batch 29 --- Loss: 0.0930\n",
            "Batch 30 --- Loss: 0.1436\n",
            "Batch 31 --- Loss: 0.0858\n",
            "Batch 32 --- Loss: 0.1443\n",
            "Batch 33 --- Loss: 0.1337\n",
            "Batch 34 --- Loss: 0.1468\n",
            "Batch 35 --- Loss: 0.0580\n",
            "Batch 36 --- Loss: 0.0837\n",
            "Batch 37 --- Loss: 0.0535\n",
            "Batch 38 --- Loss: 0.0590\n",
            "Batch 39 --- Loss: 0.0866\n",
            "Batch 40 --- Loss: 0.0940\n",
            "Batch 41 --- Loss: 0.0506\n",
            "Batch 42 --- Loss: 0.0937\n",
            "Batch 43 --- Loss: 0.1006\n",
            "Batch 44 --- Loss: 0.1437\n",
            "Batch 45 --- Loss: 0.0786\n",
            "Batch 46 --- Loss: 0.1260\n",
            "Batch 47 --- Loss: 0.1117\n",
            "Batch 48 --- Loss: 0.1888\n",
            "Batch 49 --- Loss: 0.0834\n",
            "Batch 50 --- Loss: 0.0841\n",
            "Batch 51 --- Loss: 0.0692\n",
            "Batch 52 --- Loss: 0.0800\n",
            "Batch 53 --- Loss: 0.1018\n",
            "Batch 54 --- Loss: 0.1177\n",
            "Batch 55 --- Loss: 0.0959\n",
            "Batch 56 --- Loss: 0.0876\n",
            "Batch 57 --- Loss: 0.1602\n",
            "Batch 58 --- Loss: 0.0454\n",
            "Batch 59 --- Loss: 0.1040\n",
            "Batch 60 --- Loss: 0.0453\n",
            "Batch 61 --- Loss: 0.2014\n",
            "Batch 62 --- Loss: 0.1311\n",
            "Batch 63 --- Loss: 0.1198\n",
            "Batch 64 --- Loss: 0.1414\n",
            "Batch 65 --- Loss: 0.0875\n",
            "Batch 66 --- Loss: 0.0923\n",
            "Batch 67 --- Loss: 0.1995\n",
            "Batch 68 --- Loss: 0.0714\n",
            "Batch 69 --- Loss: 0.0967\n",
            "Batch 70 --- Loss: 0.1295\n",
            "Batch 71 --- Loss: 0.1188\n",
            "Batch 72 --- Loss: 0.2256\n",
            "Batch 73 --- Loss: 0.1744\n",
            "Batch 74 --- Loss: 0.1503\n",
            "Batch 75 --- Loss: 0.1459\n",
            "Batch 76 --- Loss: 0.1222\n",
            "Batch 77 --- Loss: 0.1176\n",
            "Batch 78 --- Loss: 0.1913\n",
            "Batch 79 --- Loss: 0.0656\n",
            "Batch 80 --- Loss: 0.0926\n",
            "Batch 81 --- Loss: 0.1630\n",
            "Batch 82 --- Loss: 0.0850\n",
            "Batch 83 --- Loss: 0.2121\n",
            "Batch 84 --- Loss: 0.1031\n",
            "Epoch 11 / 50 --- Average Loss: 0.1173\n",
            "Average Macro Precision: 0.3496 ---- Average Macro Recall: 0.2653 ---- Average F1 Score: 0.2547 ---- Average Loss: 0.1513\n",
            "Average No Damage Precision: 0.5771 ---- Average No Damage Recall: 0.0239 ---- Average No Damage F1: 0.0434\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8676 ---- Average Background Recall: 0.9998 ---- Average Background F1: 0.9268\n",
            "Batch 0 --- Loss: 0.1244\n",
            "Batch 1 --- Loss: 0.1248\n",
            "Batch 2 --- Loss: 0.0937\n",
            "Batch 3 --- Loss: 0.1841\n",
            "Batch 4 --- Loss: 0.1355\n",
            "Batch 5 --- Loss: 0.1536\n",
            "Batch 6 --- Loss: 0.1228\n",
            "Batch 7 --- Loss: 0.1656\n",
            "Batch 8 --- Loss: 0.1337\n",
            "Batch 9 --- Loss: 0.1629\n",
            "Batch 10 --- Loss: 0.1006\n",
            "Batch 11 --- Loss: 0.0621\n",
            "Batch 12 --- Loss: 0.1019\n",
            "Batch 13 --- Loss: 0.1301\n",
            "Batch 14 --- Loss: 0.0950\n",
            "Batch 15 --- Loss: 0.1976\n",
            "Batch 16 --- Loss: 0.1335\n",
            "Batch 17 --- Loss: 0.0675\n",
            "Batch 18 --- Loss: 0.1523\n",
            "Batch 19 --- Loss: 0.0678\n",
            "Batch 20 --- Loss: 0.1315\n",
            "Batch 21 --- Loss: 0.1136\n",
            "Batch 22 --- Loss: 0.1501\n",
            "Batch 23 --- Loss: 0.0824\n",
            "Batch 24 --- Loss: 0.0846\n",
            "Batch 25 --- Loss: 0.1085\n",
            "Batch 26 --- Loss: 0.1475\n",
            "Batch 27 --- Loss: 0.1563\n",
            "Batch 28 --- Loss: 0.0675\n",
            "Batch 29 --- Loss: 0.0862\n",
            "Batch 30 --- Loss: 0.1084\n",
            "Batch 31 --- Loss: 0.1017\n",
            "Batch 32 --- Loss: 0.1214\n",
            "Batch 33 --- Loss: 0.2204\n",
            "Batch 34 --- Loss: 0.1200\n",
            "Batch 35 --- Loss: 0.0459\n",
            "Batch 36 --- Loss: 0.1081\n",
            "Batch 37 --- Loss: 0.0569\n",
            "Batch 38 --- Loss: 0.0540\n",
            "Batch 39 --- Loss: 0.0864\n",
            "Batch 40 --- Loss: 0.1378\n",
            "Batch 41 --- Loss: 0.0405\n",
            "Batch 42 --- Loss: 0.0990\n",
            "Batch 43 --- Loss: 0.1148\n",
            "Batch 44 --- Loss: 0.1205\n",
            "Batch 45 --- Loss: 0.0619\n",
            "Batch 46 --- Loss: 0.1198\n",
            "Batch 47 --- Loss: 0.1107\n",
            "Batch 48 --- Loss: 0.1501\n",
            "Batch 49 --- Loss: 0.1076\n",
            "Batch 50 --- Loss: 0.0820\n",
            "Batch 51 --- Loss: 0.0566\n",
            "Batch 52 --- Loss: 0.0936\n",
            "Batch 53 --- Loss: 0.1005\n",
            "Batch 54 --- Loss: 0.1181\n",
            "Batch 55 --- Loss: 0.0990\n",
            "Batch 56 --- Loss: 0.0917\n",
            "Batch 57 --- Loss: 0.1601\n",
            "Batch 58 --- Loss: 0.0402\n",
            "Batch 59 --- Loss: 0.1063\n",
            "Batch 60 --- Loss: 0.0517\n",
            "Batch 61 --- Loss: 0.3724\n",
            "Batch 62 --- Loss: 0.1121\n",
            "Batch 63 --- Loss: 0.1013\n",
            "Batch 64 --- Loss: 0.1365\n",
            "Batch 65 --- Loss: 0.1367\n",
            "Batch 66 --- Loss: 0.0774\n",
            "Batch 67 --- Loss: 0.1535\n",
            "Batch 68 --- Loss: 0.1146\n",
            "Batch 69 --- Loss: 0.1432\n",
            "Batch 70 --- Loss: 0.1381\n",
            "Batch 71 --- Loss: 0.0687\n",
            "Batch 72 --- Loss: 0.1539\n",
            "Batch 73 --- Loss: 0.1901\n",
            "Batch 74 --- Loss: 0.1657\n",
            "Batch 75 --- Loss: 0.1128\n",
            "Batch 76 --- Loss: 0.0925\n",
            "Batch 77 --- Loss: 0.1028\n",
            "Batch 78 --- Loss: 0.1276\n",
            "Batch 79 --- Loss: 0.0455\n",
            "Batch 80 --- Loss: 0.0578\n",
            "Batch 81 --- Loss: 0.1611\n",
            "Batch 82 --- Loss: 0.1162\n",
            "Batch 83 --- Loss: 0.2107\n",
            "Batch 84 --- Loss: 0.0849\n",
            "Epoch 12 / 50 --- Average Loss: 0.1165\n",
            "Average Macro Precision: 0.3588 ---- Average Macro Recall: 0.2715 ---- Average F1 Score: 0.2653 ---- Average Loss: 0.1298\n",
            "Average No Damage Precision: 0.6194 ---- Average No Damage Recall: 0.0556 ---- Average No Damage F1: 0.0945\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8716 ---- Average Background Recall: 0.9990 ---- Average Background F1: 0.9289\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_12_0.2652864839358413.pt\n",
            "Batch 0 --- Loss: 0.1085\n",
            "Batch 1 --- Loss: 0.0764\n",
            "Batch 2 --- Loss: 0.1106\n",
            "Batch 3 --- Loss: 0.1834\n",
            "Batch 4 --- Loss: 0.1212\n",
            "Batch 5 --- Loss: 0.1173\n",
            "Batch 6 --- Loss: 0.1486\n",
            "Batch 7 --- Loss: 0.1356\n",
            "Batch 8 --- Loss: 0.0876\n",
            "Batch 9 --- Loss: 0.1581\n",
            "Batch 10 --- Loss: 0.0810\n",
            "Batch 11 --- Loss: 0.0644\n",
            "Batch 12 --- Loss: 0.1196\n",
            "Batch 13 --- Loss: 0.0820\n",
            "Batch 14 --- Loss: 0.0545\n",
            "Batch 15 --- Loss: 0.1395\n",
            "Batch 16 --- Loss: 0.1751\n",
            "Batch 17 --- Loss: 0.0762\n",
            "Batch 18 --- Loss: 0.1211\n",
            "Batch 19 --- Loss: 0.1069\n",
            "Batch 20 --- Loss: 0.1271\n",
            "Batch 21 --- Loss: 0.1111\n",
            "Batch 22 --- Loss: 0.0901\n",
            "Batch 23 --- Loss: 0.1266\n",
            "Batch 24 --- Loss: 0.0782\n",
            "Batch 25 --- Loss: 0.1671\n",
            "Batch 26 --- Loss: 0.0798\n",
            "Batch 27 --- Loss: 0.0979\n",
            "Batch 28 --- Loss: 0.0997\n",
            "Batch 29 --- Loss: 0.0845\n",
            "Batch 30 --- Loss: 0.0998\n",
            "Batch 31 --- Loss: 0.0613\n",
            "Batch 32 --- Loss: 0.1408\n",
            "Batch 33 --- Loss: 0.1247\n",
            "Batch 34 --- Loss: 0.1226\n",
            "Batch 35 --- Loss: 0.0319\n",
            "Batch 36 --- Loss: 0.0865\n",
            "Batch 37 --- Loss: 0.0829\n",
            "Batch 38 --- Loss: 0.0493\n",
            "Batch 39 --- Loss: 0.0941\n",
            "Batch 40 --- Loss: 0.0925\n",
            "Batch 41 --- Loss: 0.0369\n",
            "Batch 42 --- Loss: 0.0823\n",
            "Batch 43 --- Loss: 0.0761\n",
            "Batch 44 --- Loss: 0.1069\n",
            "Batch 45 --- Loss: 0.0663\n",
            "Batch 46 --- Loss: 0.1141\n",
            "Batch 47 --- Loss: 0.1157\n",
            "Batch 48 --- Loss: 0.1282\n",
            "Batch 49 --- Loss: 0.1943\n",
            "Batch 50 --- Loss: 0.0757\n",
            "Batch 51 --- Loss: 0.0474\n",
            "Batch 52 --- Loss: 0.0785\n",
            "Batch 53 --- Loss: 0.1095\n",
            "Batch 54 --- Loss: 0.0776\n",
            "Batch 55 --- Loss: 0.0778\n",
            "Batch 56 --- Loss: 0.0751\n",
            "Batch 57 --- Loss: 0.1419\n",
            "Batch 58 --- Loss: 0.0767\n",
            "Batch 59 --- Loss: 0.1401\n",
            "Batch 60 --- Loss: 0.0433\n",
            "Batch 61 --- Loss: 0.2319\n",
            "Batch 62 --- Loss: 0.0791\n",
            "Batch 63 --- Loss: 0.0499\n",
            "Batch 64 --- Loss: 0.2870\n",
            "Batch 65 --- Loss: 0.1292\n",
            "Batch 66 --- Loss: 0.0825\n",
            "Batch 67 --- Loss: 0.2185\n",
            "Batch 68 --- Loss: 0.0586\n",
            "Batch 69 --- Loss: 0.1511\n",
            "Batch 70 --- Loss: 0.1462\n",
            "Batch 71 --- Loss: 0.1066\n",
            "Batch 72 --- Loss: 0.1478\n",
            "Batch 73 --- Loss: 0.2328\n",
            "Batch 74 --- Loss: 0.1530\n",
            "Batch 75 --- Loss: 0.1400\n",
            "Batch 76 --- Loss: 0.0823\n",
            "Batch 77 --- Loss: 0.1254\n",
            "Batch 78 --- Loss: 0.1241\n",
            "Batch 79 --- Loss: 0.0444\n",
            "Batch 80 --- Loss: 0.0442\n",
            "Batch 81 --- Loss: 0.1657\n",
            "Batch 82 --- Loss: 0.0858\n",
            "Batch 83 --- Loss: 0.2558\n",
            "Batch 84 --- Loss: 0.0764\n",
            "Epoch 13 / 50 --- Average Loss: 0.1106\n",
            "Average Macro Precision: 0.3502 ---- Average Macro Recall: 0.2720 ---- Average F1 Score: 0.2652 ---- Average Loss: 0.1397\n",
            "Average No Damage Precision: 0.5751 ---- Average No Damage Recall: 0.0578 ---- Average No Damage F1: 0.0931\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8730 ---- Average Background Recall: 0.9991 ---- Average Background F1: 0.9299\n",
            "Batch 0 --- Loss: 0.1444\n",
            "Batch 1 --- Loss: 0.0990\n",
            "Batch 2 --- Loss: 0.1016\n",
            "Batch 3 --- Loss: 0.1394\n",
            "Batch 4 --- Loss: 0.1382\n",
            "Batch 5 --- Loss: 0.1221\n",
            "Batch 6 --- Loss: 0.1833\n",
            "Batch 7 --- Loss: 0.1446\n",
            "Batch 8 --- Loss: 0.0926\n",
            "Batch 9 --- Loss: 0.1569\n",
            "Batch 10 --- Loss: 0.1146\n",
            "Batch 11 --- Loss: 0.0775\n",
            "Batch 12 --- Loss: 0.1050\n",
            "Batch 13 --- Loss: 0.0739\n",
            "Batch 14 --- Loss: 0.0680\n",
            "Batch 15 --- Loss: 0.1454\n",
            "Batch 16 --- Loss: 0.1521\n",
            "Batch 17 --- Loss: 0.0763\n",
            "Batch 18 --- Loss: 0.1387\n",
            "Batch 19 --- Loss: 0.0726\n",
            "Batch 20 --- Loss: 0.1244\n",
            "Batch 21 --- Loss: 0.0793\n",
            "Batch 22 --- Loss: 0.1130\n",
            "Batch 23 --- Loss: 0.1163\n",
            "Batch 24 --- Loss: 0.0736\n",
            "Batch 25 --- Loss: 0.0968\n",
            "Batch 26 --- Loss: 0.1213\n",
            "Batch 27 --- Loss: 0.0958\n",
            "Batch 28 --- Loss: 0.0982\n",
            "Batch 29 --- Loss: 0.0832\n",
            "Batch 30 --- Loss: 0.0896\n",
            "Batch 31 --- Loss: 0.0596\n",
            "Batch 32 --- Loss: 0.1315\n",
            "Batch 33 --- Loss: 0.1123\n",
            "Batch 34 --- Loss: 0.1082\n",
            "Batch 35 --- Loss: 0.0324\n",
            "Batch 36 --- Loss: 0.0702\n",
            "Batch 37 --- Loss: 0.0499\n",
            "Batch 38 --- Loss: 0.0455\n",
            "Batch 39 --- Loss: 0.0905\n",
            "Batch 40 --- Loss: 0.1094\n",
            "Batch 41 --- Loss: 0.0588\n",
            "Batch 42 --- Loss: 0.1055\n",
            "Batch 43 --- Loss: 0.1643\n",
            "Batch 44 --- Loss: 0.0815\n",
            "Batch 45 --- Loss: 0.0699\n",
            "Batch 46 --- Loss: 0.1306\n",
            "Batch 47 --- Loss: 0.1426\n",
            "Batch 48 --- Loss: 0.1365\n",
            "Batch 49 --- Loss: 0.0937\n",
            "Batch 50 --- Loss: 0.0792\n",
            "Batch 51 --- Loss: 0.0680\n",
            "Batch 52 --- Loss: 0.1245\n",
            "Batch 53 --- Loss: 0.1203\n",
            "Batch 54 --- Loss: 0.0600\n",
            "Batch 55 --- Loss: 0.0708\n",
            "Batch 56 --- Loss: 0.0639\n",
            "Batch 57 --- Loss: 0.1200\n",
            "Batch 58 --- Loss: 0.0446\n",
            "Batch 59 --- Loss: 0.1153\n",
            "Batch 60 --- Loss: 0.0361\n",
            "Batch 61 --- Loss: 0.1321\n",
            "Batch 62 --- Loss: 0.0653\n",
            "Batch 63 --- Loss: 0.0803\n",
            "Batch 64 --- Loss: 0.2115\n",
            "Batch 65 --- Loss: 0.0809\n",
            "Batch 66 --- Loss: 0.0858\n",
            "Batch 67 --- Loss: 0.1498\n",
            "Batch 68 --- Loss: 0.0469\n",
            "Batch 69 --- Loss: 0.0885\n",
            "Batch 70 --- Loss: 0.1202\n",
            "Batch 71 --- Loss: 0.1110\n",
            "Batch 72 --- Loss: 0.1418\n",
            "Batch 73 --- Loss: 0.2033\n",
            "Batch 74 --- Loss: 0.3478\n",
            "Batch 75 --- Loss: 0.1352\n",
            "Batch 76 --- Loss: 0.1373\n",
            "Batch 77 --- Loss: 0.1407\n",
            "Batch 78 --- Loss: 0.1316\n",
            "Batch 79 --- Loss: 0.0370\n",
            "Batch 80 --- Loss: 0.0981\n",
            "Batch 81 --- Loss: 0.1874\n",
            "Batch 82 --- Loss: 0.1380\n",
            "Batch 83 --- Loss: 0.1806\n",
            "Batch 84 --- Loss: 0.1101\n",
            "Epoch 14 / 50 --- Average Loss: 0.1094\n",
            "Average Macro Precision: 0.2448 ---- Average Macro Recall: 0.2606 ---- Average F1 Score: 0.2456 ---- Average Loss: 0.2252\n",
            "Average No Damage Precision: 0.0564 ---- Average No Damage Recall: 0.0002 ---- Average No Damage F1: 0.0003\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8643 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9249\n",
            "Batch 0 --- Loss: 0.1247\n",
            "Batch 1 --- Loss: 0.1079\n",
            "Batch 2 --- Loss: 0.1051\n",
            "Batch 3 --- Loss: 0.1573\n",
            "Batch 4 --- Loss: 0.1456\n",
            "Batch 5 --- Loss: 0.1113\n",
            "Batch 6 --- Loss: 0.1517\n",
            "Batch 7 --- Loss: 0.1557\n",
            "Batch 8 --- Loss: 0.0890\n",
            "Batch 9 --- Loss: 0.1348\n",
            "Batch 10 --- Loss: 0.0764\n",
            "Batch 11 --- Loss: 0.0837\n",
            "Batch 12 --- Loss: 0.1096\n",
            "Batch 13 --- Loss: 0.0768\n",
            "Batch 14 --- Loss: 0.1025\n",
            "Batch 15 --- Loss: 0.1052\n",
            "Batch 16 --- Loss: 0.1396\n",
            "Batch 17 --- Loss: 0.0488\n",
            "Batch 18 --- Loss: 0.1002\n",
            "Batch 19 --- Loss: 0.0724\n",
            "Batch 20 --- Loss: 0.1472\n",
            "Batch 21 --- Loss: 0.0892\n",
            "Batch 22 --- Loss: 0.0857\n",
            "Batch 23 --- Loss: 0.0699\n",
            "Batch 24 --- Loss: 0.1037\n",
            "Batch 25 --- Loss: 0.1283\n",
            "Batch 26 --- Loss: 0.0934\n",
            "Batch 27 --- Loss: 0.1580\n",
            "Batch 28 --- Loss: 0.0647\n",
            "Batch 29 --- Loss: 0.0862\n",
            "Batch 30 --- Loss: 0.1011\n",
            "Batch 31 --- Loss: 0.0569\n",
            "Batch 32 --- Loss: 0.1024\n",
            "Batch 33 --- Loss: 0.0937\n",
            "Batch 34 --- Loss: 0.1132\n",
            "Batch 35 --- Loss: 0.0665\n",
            "Batch 36 --- Loss: 0.1091\n",
            "Batch 37 --- Loss: 0.0424\n",
            "Batch 38 --- Loss: 0.0595\n",
            "Batch 39 --- Loss: 0.0893\n",
            "Batch 40 --- Loss: 0.1120\n",
            "Batch 41 --- Loss: 0.0394\n",
            "Batch 42 --- Loss: 0.1047\n",
            "Batch 43 --- Loss: 0.1129\n",
            "Batch 44 --- Loss: 0.1071\n",
            "Batch 45 --- Loss: 0.0893\n",
            "Batch 46 --- Loss: 0.1321\n",
            "Batch 47 --- Loss: 0.1656\n",
            "Batch 48 --- Loss: 0.2114\n",
            "Batch 49 --- Loss: 0.0770\n",
            "Batch 50 --- Loss: 0.0796\n",
            "Batch 51 --- Loss: 0.0447\n",
            "Batch 52 --- Loss: 0.1158\n",
            "Batch 53 --- Loss: 0.1133\n",
            "Batch 54 --- Loss: 0.0561\n",
            "Batch 55 --- Loss: 0.1140\n",
            "Batch 56 --- Loss: 0.0687\n",
            "Batch 57 --- Loss: 0.1402\n",
            "Batch 58 --- Loss: 0.1143\n",
            "Batch 59 --- Loss: 0.0919\n",
            "Batch 60 --- Loss: 0.0431\n",
            "Batch 61 --- Loss: 0.2466\n",
            "Batch 62 --- Loss: 0.1067\n",
            "Batch 63 --- Loss: 0.0831\n",
            "Batch 64 --- Loss: 0.1493\n",
            "Batch 65 --- Loss: 0.0861\n",
            "Batch 66 --- Loss: 0.1048\n",
            "Batch 67 --- Loss: 0.1859\n",
            "Batch 68 --- Loss: 0.0679\n",
            "Batch 69 --- Loss: 0.0916\n",
            "Batch 70 --- Loss: 0.1502\n",
            "Batch 71 --- Loss: 0.0860\n",
            "Batch 72 --- Loss: 0.2110\n",
            "Batch 73 --- Loss: 0.1734\n",
            "Batch 74 --- Loss: 0.1078\n",
            "Batch 75 --- Loss: 0.1354\n",
            "Batch 76 --- Loss: 0.1605\n",
            "Batch 77 --- Loss: 0.1293\n",
            "Batch 78 --- Loss: 0.1691\n",
            "Batch 79 --- Loss: 0.0574\n",
            "Batch 80 --- Loss: 0.0482\n",
            "Batch 81 --- Loss: 0.2480\n",
            "Batch 82 --- Loss: 0.1104\n",
            "Batch 83 --- Loss: 0.1837\n",
            "Batch 84 --- Loss: 0.0761\n",
            "Epoch 15 / 50 --- Average Loss: 0.1101\n",
            "Average Macro Precision: 0.3409 ---- Average Macro Recall: 0.2621 ---- Average F1 Score: 0.2484 ---- Average Loss: 0.1858\n",
            "Average No Damage Precision: 0.5363 ---- Average No Damage Recall: 0.0074 ---- Average No Damage F1: 0.0136\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8653 ---- Average Background Recall: 0.9999 ---- Average Background F1: 0.9254\n",
            "Batch 0 --- Loss: 0.1322\n",
            "Batch 1 --- Loss: 0.0950\n",
            "Batch 2 --- Loss: 0.1278\n",
            "Batch 3 --- Loss: 0.1560\n",
            "Batch 4 --- Loss: 0.1117\n",
            "Batch 5 --- Loss: 0.0956\n",
            "Batch 6 --- Loss: 0.1140\n",
            "Batch 7 --- Loss: 0.1630\n",
            "Batch 8 --- Loss: 0.1712\n",
            "Batch 9 --- Loss: 0.0993\n",
            "Batch 10 --- Loss: 0.1135\n",
            "Batch 11 --- Loss: 0.0629\n",
            "Batch 12 --- Loss: 0.0840\n",
            "Batch 13 --- Loss: 0.1066\n",
            "Batch 14 --- Loss: 0.0549\n",
            "Batch 15 --- Loss: 0.1922\n",
            "Batch 16 --- Loss: 0.0850\n",
            "Batch 17 --- Loss: 0.0634\n",
            "Batch 18 --- Loss: 0.0964\n",
            "Batch 19 --- Loss: 0.0756\n",
            "Batch 20 --- Loss: 0.1529\n",
            "Batch 21 --- Loss: 0.1670\n",
            "Batch 22 --- Loss: 0.0856\n",
            "Batch 23 --- Loss: 0.0837\n",
            "Batch 24 --- Loss: 0.0854\n",
            "Batch 25 --- Loss: 0.0889\n",
            "Batch 26 --- Loss: 0.0795\n",
            "Batch 27 --- Loss: 0.0871\n",
            "Batch 28 --- Loss: 0.1242\n",
            "Batch 29 --- Loss: 0.2007\n",
            "Batch 30 --- Loss: 0.0869\n",
            "Batch 31 --- Loss: 0.0878\n",
            "Batch 32 --- Loss: 0.1809\n",
            "Batch 33 --- Loss: 0.0914\n",
            "Batch 34 --- Loss: 0.1018\n",
            "Batch 35 --- Loss: 0.0608\n",
            "Batch 36 --- Loss: 0.0725\n",
            "Batch 37 --- Loss: 0.0701\n",
            "Batch 38 --- Loss: 0.0440\n",
            "Batch 39 --- Loss: 0.0808\n",
            "Batch 40 --- Loss: 0.1104\n",
            "Batch 41 --- Loss: 0.0535\n",
            "Batch 42 --- Loss: 0.0901\n",
            "Batch 43 --- Loss: 0.1419\n",
            "Batch 44 --- Loss: 0.1753\n",
            "Batch 45 --- Loss: 0.0563\n",
            "Batch 46 --- Loss: 0.1295\n",
            "Batch 47 --- Loss: 0.1470\n",
            "Batch 48 --- Loss: 0.1230\n",
            "Batch 49 --- Loss: 0.1281\n",
            "Batch 50 --- Loss: 0.0751\n",
            "Batch 51 --- Loss: 0.0557\n",
            "Batch 52 --- Loss: 0.0708\n",
            "Batch 53 --- Loss: 0.0792\n",
            "Batch 54 --- Loss: 0.1631\n",
            "Batch 55 --- Loss: 0.0672\n",
            "Batch 56 --- Loss: 0.0588\n",
            "Batch 57 --- Loss: 0.1115\n",
            "Batch 58 --- Loss: 0.0437\n",
            "Batch 59 --- Loss: 0.1085\n",
            "Batch 60 --- Loss: 0.0360\n",
            "Batch 61 --- Loss: 0.1658\n",
            "Batch 62 --- Loss: 0.1156\n",
            "Batch 63 --- Loss: 0.0831\n",
            "Batch 64 --- Loss: 0.1365\n",
            "Batch 65 --- Loss: 0.1021\n",
            "Batch 66 --- Loss: 0.1090\n",
            "Batch 67 --- Loss: 0.2730\n",
            "Batch 68 --- Loss: 0.0522\n",
            "Batch 69 --- Loss: 0.1682\n",
            "Batch 70 --- Loss: 0.1866\n",
            "Batch 71 --- Loss: 0.0600\n",
            "Batch 72 --- Loss: 0.1373\n",
            "Batch 73 --- Loss: 0.2154\n",
            "Batch 74 --- Loss: 0.1000\n",
            "Batch 75 --- Loss: 0.1407\n",
            "Batch 76 --- Loss: 0.1433\n",
            "Batch 77 --- Loss: 0.1125\n",
            "Batch 78 --- Loss: 0.1646\n",
            "Batch 79 --- Loss: 0.0411\n",
            "Batch 80 --- Loss: 0.0418\n",
            "Batch 81 --- Loss: 0.1381\n",
            "Batch 82 --- Loss: 0.0841\n",
            "Batch 83 --- Loss: 0.2465\n",
            "Batch 84 --- Loss: 0.1393\n",
            "Epoch 16 / 50 --- Average Loss: 0.1107\n",
            "Average Macro Precision: 0.3696 ---- Average Macro Recall: 0.2869 ---- Average F1 Score: 0.2867 ---- Average Loss: 0.1350\n",
            "Average No Damage Precision: 0.6629 ---- Average No Damage Recall: 0.1347 ---- Average No Damage F1: 0.1962\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8822 ---- Average Background Recall: 0.9966 ---- Average Background F1: 0.9341\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_16_0.2866521080931232.pt\n",
            "Batch 0 --- Loss: 0.1032\n",
            "Batch 1 --- Loss: 0.0888\n",
            "Batch 2 --- Loss: 0.0982\n",
            "Batch 3 --- Loss: 0.1142\n",
            "Batch 4 --- Loss: 0.1186\n",
            "Batch 5 --- Loss: 0.1058\n",
            "Batch 6 --- Loss: 0.1196\n",
            "Batch 7 --- Loss: 0.1489\n",
            "Batch 8 --- Loss: 0.0871\n",
            "Batch 9 --- Loss: 0.0995\n",
            "Batch 10 --- Loss: 0.1674\n",
            "Batch 11 --- Loss: 0.0769\n",
            "Batch 12 --- Loss: 0.0754\n",
            "Batch 13 --- Loss: 0.0925\n",
            "Batch 14 --- Loss: 0.0891\n",
            "Batch 15 --- Loss: 0.1523\n",
            "Batch 16 --- Loss: 0.0792\n",
            "Batch 17 --- Loss: 0.0780\n",
            "Batch 18 --- Loss: 0.0966\n",
            "Batch 19 --- Loss: 0.0667\n",
            "Batch 20 --- Loss: 0.1416\n",
            "Batch 21 --- Loss: 0.0816\n",
            "Batch 22 --- Loss: 0.0773\n",
            "Batch 23 --- Loss: 0.0806\n",
            "Batch 24 --- Loss: 0.0663\n",
            "Batch 25 --- Loss: 0.1051\n",
            "Batch 26 --- Loss: 0.0755\n",
            "Batch 27 --- Loss: 0.1377\n",
            "Batch 28 --- Loss: 0.0825\n",
            "Batch 29 --- Loss: 0.0794\n",
            "Batch 30 --- Loss: 0.0916\n",
            "Batch 31 --- Loss: 0.0645\n",
            "Batch 32 --- Loss: 0.1633\n",
            "Batch 33 --- Loss: 0.0973\n",
            "Batch 34 --- Loss: 0.1397\n",
            "Batch 35 --- Loss: 0.0392\n",
            "Batch 36 --- Loss: 0.0653\n",
            "Batch 37 --- Loss: 0.0393\n",
            "Batch 38 --- Loss: 0.0529\n",
            "Batch 39 --- Loss: 0.0763\n",
            "Batch 40 --- Loss: 0.1201\n",
            "Batch 41 --- Loss: 0.0293\n",
            "Batch 42 --- Loss: 0.0816\n",
            "Batch 43 --- Loss: 0.0712\n",
            "Batch 44 --- Loss: 0.1014\n",
            "Batch 45 --- Loss: 0.0564\n",
            "Batch 46 --- Loss: 0.0990\n",
            "Batch 47 --- Loss: 0.1238\n",
            "Batch 48 --- Loss: 0.1705\n",
            "Batch 49 --- Loss: 0.1809\n",
            "Batch 50 --- Loss: 0.0840\n",
            "Batch 51 --- Loss: 0.0721\n",
            "Batch 52 --- Loss: 0.0658\n",
            "Batch 53 --- Loss: 0.1122\n",
            "Batch 54 --- Loss: 0.0648\n",
            "Batch 55 --- Loss: 0.0941\n",
            "Batch 56 --- Loss: 0.0583\n",
            "Batch 57 --- Loss: 0.0929\n",
            "Batch 58 --- Loss: 0.0313\n",
            "Batch 59 --- Loss: 0.0842\n",
            "Batch 60 --- Loss: 0.0430\n",
            "Batch 61 --- Loss: 0.2011\n",
            "Batch 62 --- Loss: 0.2103\n",
            "Batch 63 --- Loss: 0.1076\n",
            "Batch 64 --- Loss: 0.1597\n",
            "Batch 65 --- Loss: 0.0998\n",
            "Batch 66 --- Loss: 0.1130\n",
            "Batch 67 --- Loss: 0.1230\n",
            "Batch 68 --- Loss: 0.0469\n",
            "Batch 69 --- Loss: 0.0943\n",
            "Batch 70 --- Loss: 0.1519\n",
            "Batch 71 --- Loss: 0.0993\n",
            "Batch 72 --- Loss: 0.1422\n",
            "Batch 73 --- Loss: 0.1953\n",
            "Batch 74 --- Loss: 0.2391\n",
            "Batch 75 --- Loss: 0.0965\n",
            "Batch 76 --- Loss: 0.1058\n",
            "Batch 77 --- Loss: 0.0924\n",
            "Batch 78 --- Loss: 0.1532\n",
            "Batch 79 --- Loss: 0.0394\n",
            "Batch 80 --- Loss: 0.0456\n",
            "Batch 81 --- Loss: 0.1403\n",
            "Batch 82 --- Loss: 0.1170\n",
            "Batch 83 --- Loss: 0.1968\n",
            "Batch 84 --- Loss: 0.1158\n",
            "Epoch 17 / 50 --- Average Loss: 0.1028\n",
            "Average Macro Precision: 0.3608 ---- Average Macro Recall: 0.2863 ---- Average F1 Score: 0.2859 ---- Average Loss: 0.1306\n",
            "Average No Damage Precision: 0.6183 ---- Average No Damage Recall: 0.1304 ---- Average No Damage F1: 0.1917\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8825 ---- Average Background Recall: 0.9979 ---- Average Background F1: 0.9349\n",
            "Batch 0 --- Loss: 0.1577\n",
            "Batch 1 --- Loss: 0.0725\n",
            "Batch 2 --- Loss: 0.1194\n",
            "Batch 3 --- Loss: 0.1337\n",
            "Batch 4 --- Loss: 0.2203\n",
            "Batch 5 --- Loss: 0.1838\n",
            "Batch 6 --- Loss: 0.1487\n",
            "Batch 7 --- Loss: 0.1369\n",
            "Batch 8 --- Loss: 0.1374\n",
            "Batch 9 --- Loss: 0.1172\n",
            "Batch 10 --- Loss: 0.1256\n",
            "Batch 11 --- Loss: 0.0799\n",
            "Batch 12 --- Loss: 0.0796\n",
            "Batch 13 --- Loss: 0.1819\n",
            "Batch 14 --- Loss: 0.0510\n",
            "Batch 15 --- Loss: 0.1366\n",
            "Batch 16 --- Loss: 0.0857\n",
            "Batch 17 --- Loss: 0.0610\n",
            "Batch 18 --- Loss: 0.1351\n",
            "Batch 19 --- Loss: 0.0923\n",
            "Batch 20 --- Loss: 0.1195\n",
            "Batch 21 --- Loss: 0.0800\n",
            "Batch 22 --- Loss: 0.1154\n",
            "Batch 23 --- Loss: 0.0748\n",
            "Batch 24 --- Loss: 0.1356\n",
            "Batch 25 --- Loss: 0.1026\n",
            "Batch 26 --- Loss: 0.0754\n",
            "Batch 27 --- Loss: 0.0988\n",
            "Batch 28 --- Loss: 0.1035\n",
            "Batch 29 --- Loss: 0.1206\n",
            "Batch 30 --- Loss: 0.1145\n",
            "Batch 31 --- Loss: 0.0605\n",
            "Batch 32 --- Loss: 0.1257\n",
            "Batch 33 --- Loss: 0.1747\n",
            "Batch 34 --- Loss: 0.0934\n",
            "Batch 35 --- Loss: 0.0252\n",
            "Batch 36 --- Loss: 0.0642\n",
            "Batch 37 --- Loss: 0.0425\n",
            "Batch 38 --- Loss: 0.0481\n",
            "Batch 39 --- Loss: 0.0716\n",
            "Batch 40 --- Loss: 0.1028\n",
            "Batch 41 --- Loss: 0.0274\n",
            "Batch 42 --- Loss: 0.1336\n",
            "Batch 43 --- Loss: 0.1016\n",
            "Batch 44 --- Loss: 0.1579\n",
            "Batch 45 --- Loss: 0.0573\n",
            "Batch 46 --- Loss: 0.1356\n",
            "Batch 47 --- Loss: 0.1674\n",
            "Batch 48 --- Loss: 0.1374\n",
            "Batch 49 --- Loss: 0.0719\n",
            "Batch 50 --- Loss: 0.1008\n",
            "Batch 51 --- Loss: 0.0440\n",
            "Batch 52 --- Loss: 0.2147\n",
            "Batch 53 --- Loss: 0.0750\n",
            "Batch 54 --- Loss: 0.0478\n",
            "Batch 55 --- Loss: 0.0988\n",
            "Batch 56 --- Loss: 0.0880\n",
            "Batch 57 --- Loss: 0.1745\n",
            "Batch 58 --- Loss: 0.0491\n",
            "Batch 59 --- Loss: 0.1278\n",
            "Batch 60 --- Loss: 0.0391\n",
            "Batch 61 --- Loss: 0.2488\n",
            "Batch 62 --- Loss: 0.1078\n",
            "Batch 63 --- Loss: 0.0790\n",
            "Batch 64 --- Loss: 0.1486\n",
            "Batch 65 --- Loss: 0.1397\n",
            "Batch 66 --- Loss: 0.1292\n",
            "Batch 67 --- Loss: 0.1679\n",
            "Batch 68 --- Loss: 0.0649\n",
            "Batch 69 --- Loss: 0.0804\n",
            "Batch 70 --- Loss: 0.1391\n",
            "Batch 71 --- Loss: 0.1240\n",
            "Batch 72 --- Loss: 0.2846\n",
            "Batch 73 --- Loss: 0.2053\n",
            "Batch 74 --- Loss: 0.1512\n",
            "Batch 75 --- Loss: 0.1307\n",
            "Batch 76 --- Loss: 0.1053\n",
            "Batch 77 --- Loss: 0.0734\n",
            "Batch 78 --- Loss: 0.1240\n",
            "Batch 79 --- Loss: 0.0440\n",
            "Batch 80 --- Loss: 0.0371\n",
            "Batch 81 --- Loss: 0.1516\n",
            "Batch 82 --- Loss: 0.0683\n",
            "Batch 83 --- Loss: 0.1542\n",
            "Batch 84 --- Loss: 0.0802\n",
            "Epoch 18 / 50 --- Average Loss: 0.1117\n",
            "Average Macro Precision: 0.3796 ---- Average Macro Recall: 0.3040 ---- Average F1 Score: 0.3093 ---- Average Loss: 0.1091\n",
            "Average No Damage Precision: 0.6993 ---- Average No Damage Recall: 0.2208 ---- Average No Damage F1: 0.3014\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8958 ---- Average Background Recall: 0.9961 ---- Average Background F1: 0.9420\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_18_0.3092787608632328.pt\n",
            "Batch 0 --- Loss: 0.0949\n",
            "Batch 1 --- Loss: 0.0921\n",
            "Batch 2 --- Loss: 0.1333\n",
            "Batch 3 --- Loss: 0.1077\n",
            "Batch 4 --- Loss: 0.1776\n",
            "Batch 5 --- Loss: 0.1397\n",
            "Batch 6 --- Loss: 0.1053\n",
            "Batch 7 --- Loss: 0.1323\n",
            "Batch 8 --- Loss: 0.0782\n",
            "Batch 9 --- Loss: 0.2036\n",
            "Batch 10 --- Loss: 0.0657\n",
            "Batch 11 --- Loss: 0.0495\n",
            "Batch 12 --- Loss: 0.0753\n",
            "Batch 13 --- Loss: 0.0671\n",
            "Batch 14 --- Loss: 0.0618\n",
            "Batch 15 --- Loss: 0.1477\n",
            "Batch 16 --- Loss: 0.1195\n",
            "Batch 17 --- Loss: 0.0519\n",
            "Batch 18 --- Loss: 0.0878\n",
            "Batch 19 --- Loss: 0.0651\n",
            "Batch 20 --- Loss: 0.1406\n",
            "Batch 21 --- Loss: 0.1163\n",
            "Batch 22 --- Loss: 0.0925\n",
            "Batch 23 --- Loss: 0.1349\n",
            "Batch 24 --- Loss: 0.0662\n",
            "Batch 25 --- Loss: 0.1652\n",
            "Batch 26 --- Loss: 0.0812\n",
            "Batch 27 --- Loss: 0.1510\n",
            "Batch 28 --- Loss: 0.1531\n",
            "Batch 29 --- Loss: 0.0704\n",
            "Batch 30 --- Loss: 0.0856\n",
            "Batch 31 --- Loss: 0.0740\n",
            "Batch 32 --- Loss: 0.1235\n",
            "Batch 33 --- Loss: 0.0872\n",
            "Batch 34 --- Loss: 0.0950\n",
            "Batch 35 --- Loss: 0.0373\n",
            "Batch 36 --- Loss: 0.0785\n",
            "Batch 37 --- Loss: 0.0576\n",
            "Batch 38 --- Loss: 0.0384\n",
            "Batch 39 --- Loss: 0.0719\n",
            "Batch 40 --- Loss: 0.0836\n",
            "Batch 41 --- Loss: 0.0350\n",
            "Batch 42 --- Loss: 0.0997\n",
            "Batch 43 --- Loss: 0.1516\n",
            "Batch 44 --- Loss: 0.1011\n",
            "Batch 45 --- Loss: 0.0687\n",
            "Batch 46 --- Loss: 0.1815\n",
            "Batch 47 --- Loss: 0.0847\n",
            "Batch 48 --- Loss: 0.1638\n",
            "Batch 49 --- Loss: 0.1235\n",
            "Batch 50 --- Loss: 0.0710\n",
            "Batch 51 --- Loss: 0.0939\n",
            "Batch 52 --- Loss: 0.0633\n",
            "Batch 53 --- Loss: 0.0792\n",
            "Batch 54 --- Loss: 0.0823\n",
            "Batch 55 --- Loss: 0.1035\n",
            "Batch 56 --- Loss: 0.0620\n",
            "Batch 57 --- Loss: 0.1407\n",
            "Batch 58 --- Loss: 0.0620\n",
            "Batch 59 --- Loss: 0.1569\n",
            "Batch 60 --- Loss: 0.0411\n",
            "Batch 61 --- Loss: 0.1158\n",
            "Batch 62 --- Loss: 0.1021\n",
            "Batch 63 --- Loss: 0.1135\n",
            "Batch 64 --- Loss: 0.1527\n",
            "Batch 65 --- Loss: 0.0757\n",
            "Batch 66 --- Loss: 0.0631\n",
            "Batch 67 --- Loss: 0.1513\n",
            "Batch 68 --- Loss: 0.0475\n",
            "Batch 69 --- Loss: 0.0806\n",
            "Batch 70 --- Loss: 0.1083\n",
            "Batch 71 --- Loss: 0.0965\n",
            "Batch 72 --- Loss: 0.1350\n",
            "Batch 73 --- Loss: 0.2364\n",
            "Batch 74 --- Loss: 0.0993\n",
            "Batch 75 --- Loss: 0.0932\n",
            "Batch 76 --- Loss: 0.0851\n",
            "Batch 77 --- Loss: 0.1322\n",
            "Batch 78 --- Loss: 0.1090\n",
            "Batch 79 --- Loss: 0.0328\n",
            "Batch 80 --- Loss: 0.0376\n",
            "Batch 81 --- Loss: 0.1670\n",
            "Batch 82 --- Loss: 0.0668\n",
            "Batch 83 --- Loss: 0.1895\n",
            "Batch 84 --- Loss: 0.0796\n",
            "Epoch 19 / 50 --- Average Loss: 0.1011\n",
            "Average Macro Precision: 0.3744 ---- Average Macro Recall: 0.2752 ---- Average F1 Score: 0.2715 ---- Average Loss: 0.1388\n",
            "Average No Damage Precision: 0.6949 ---- Average No Damage Recall: 0.0741 ---- Average No Damage F1: 0.1243\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8739 ---- Average Background Recall: 0.9990 ---- Average Background F1: 0.9303\n",
            "Batch 0 --- Loss: 0.0849\n",
            "Batch 1 --- Loss: 0.0955\n",
            "Batch 2 --- Loss: 0.1114\n",
            "Batch 3 --- Loss: 0.1507\n",
            "Batch 4 --- Loss: 0.1308\n",
            "Batch 5 --- Loss: 0.0987\n",
            "Batch 6 --- Loss: 0.1545\n",
            "Batch 7 --- Loss: 0.2097\n",
            "Batch 8 --- Loss: 0.1011\n",
            "Batch 9 --- Loss: 0.1448\n",
            "Batch 10 --- Loss: 0.0793\n",
            "Batch 11 --- Loss: 0.0561\n",
            "Batch 12 --- Loss: 0.1542\n",
            "Batch 13 --- Loss: 0.1878\n",
            "Batch 14 --- Loss: 0.0421\n",
            "Batch 15 --- Loss: 0.1087\n",
            "Batch 16 --- Loss: 0.1018\n",
            "Batch 17 --- Loss: 0.0523\n",
            "Batch 18 --- Loss: 0.1159\n",
            "Batch 19 --- Loss: 0.0641\n",
            "Batch 20 --- Loss: 0.1376\n",
            "Batch 21 --- Loss: 0.0948\n",
            "Batch 22 --- Loss: 0.0919\n",
            "Batch 23 --- Loss: 0.0848\n",
            "Batch 24 --- Loss: 0.0693\n",
            "Batch 25 --- Loss: 0.1152\n",
            "Batch 26 --- Loss: 0.0765\n",
            "Batch 27 --- Loss: 0.0896\n",
            "Batch 28 --- Loss: 0.0851\n",
            "Batch 29 --- Loss: 0.0838\n",
            "Batch 30 --- Loss: 0.0910\n",
            "Batch 31 --- Loss: 0.0987\n",
            "Batch 32 --- Loss: 0.1141\n",
            "Batch 33 --- Loss: 0.0917\n",
            "Batch 34 --- Loss: 0.1140\n",
            "Batch 35 --- Loss: 0.0306\n",
            "Batch 36 --- Loss: 0.0620\n",
            "Batch 37 --- Loss: 0.1132\n",
            "Batch 38 --- Loss: 0.0502\n",
            "Batch 39 --- Loss: 0.0692\n",
            "Batch 40 --- Loss: 0.1628\n",
            "Batch 41 --- Loss: 0.0259\n",
            "Batch 42 --- Loss: 0.1101\n",
            "Batch 43 --- Loss: 0.0822\n",
            "Batch 44 --- Loss: 0.1105\n",
            "Batch 45 --- Loss: 0.0583\n",
            "Batch 46 --- Loss: 0.1055\n",
            "Batch 47 --- Loss: 0.1124\n",
            "Batch 48 --- Loss: 0.1229\n",
            "Batch 49 --- Loss: 0.1173\n",
            "Batch 50 --- Loss: 0.0699\n",
            "Batch 51 --- Loss: 0.1130\n",
            "Batch 52 --- Loss: 0.1012\n",
            "Batch 53 --- Loss: 0.0841\n",
            "Batch 54 --- Loss: 0.0664\n",
            "Batch 55 --- Loss: 0.0966\n",
            "Batch 56 --- Loss: 0.0905\n",
            "Batch 57 --- Loss: 0.1641\n",
            "Batch 58 --- Loss: 0.0363\n",
            "Batch 59 --- Loss: 0.0903\n",
            "Batch 60 --- Loss: 0.1111\n",
            "Batch 61 --- Loss: 0.1198\n",
            "Batch 62 --- Loss: 0.0690\n",
            "Batch 63 --- Loss: 0.0744\n",
            "Batch 64 --- Loss: 0.1175\n",
            "Batch 65 --- Loss: 0.0743\n",
            "Batch 66 --- Loss: 0.0759\n",
            "Batch 67 --- Loss: 0.1140\n",
            "Batch 68 --- Loss: 0.0525\n",
            "Batch 69 --- Loss: 0.0703\n",
            "Batch 70 --- Loss: 0.1433\n",
            "Batch 71 --- Loss: 0.0999\n",
            "Batch 72 --- Loss: 0.1724\n",
            "Batch 73 --- Loss: 0.1535\n",
            "Batch 74 --- Loss: 0.0964\n",
            "Batch 75 --- Loss: 0.1417\n",
            "Batch 76 --- Loss: 0.1249\n",
            "Batch 77 --- Loss: 0.1858\n",
            "Batch 78 --- Loss: 0.1665\n",
            "Batch 79 --- Loss: 0.0259\n",
            "Batch 80 --- Loss: 0.0312\n",
            "Batch 81 --- Loss: 0.1374\n",
            "Batch 82 --- Loss: 0.1007\n",
            "Batch 83 --- Loss: 0.2338\n",
            "Batch 84 --- Loss: 0.0782\n",
            "Epoch 20 / 50 --- Average Loss: 0.1023\n",
            "Average Macro Precision: 0.2487 ---- Average Macro Recall: 0.2610 ---- Average F1 Score: 0.2464 ---- Average Loss: 0.1949\n",
            "Average No Damage Precision: 0.0757 ---- Average No Damage Recall: 0.0022 ---- Average No Damage F1: 0.0041\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8647 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9251\n",
            "Batch 0 --- Loss: 0.0966\n",
            "Batch 1 --- Loss: 0.1167\n",
            "Batch 2 --- Loss: 0.0960\n",
            "Batch 3 --- Loss: 0.1241\n",
            "Batch 4 --- Loss: 0.1397\n",
            "Batch 5 --- Loss: 0.1080\n",
            "Batch 6 --- Loss: 0.1485\n",
            "Batch 7 --- Loss: 0.1353\n",
            "Batch 8 --- Loss: 0.1072\n",
            "Batch 9 --- Loss: 0.0988\n",
            "Batch 10 --- Loss: 0.0722\n",
            "Batch 11 --- Loss: 0.0815\n",
            "Batch 12 --- Loss: 0.1139\n",
            "Batch 13 --- Loss: 0.0674\n",
            "Batch 14 --- Loss: 0.0552\n",
            "Batch 15 --- Loss: 0.1877\n",
            "Batch 16 --- Loss: 0.1463\n",
            "Batch 17 --- Loss: 0.0572\n",
            "Batch 18 --- Loss: 0.1914\n",
            "Batch 19 --- Loss: 0.0612\n",
            "Batch 20 --- Loss: 0.1295\n",
            "Batch 21 --- Loss: 0.0818\n",
            "Batch 22 --- Loss: 0.0850\n",
            "Batch 23 --- Loss: 0.1154\n",
            "Batch 24 --- Loss: 0.0623\n",
            "Batch 25 --- Loss: 0.0904\n",
            "Batch 26 --- Loss: 0.0718\n",
            "Batch 27 --- Loss: 0.0831\n",
            "Batch 28 --- Loss: 0.0866\n",
            "Batch 29 --- Loss: 0.1582\n",
            "Batch 30 --- Loss: 0.0915\n",
            "Batch 31 --- Loss: 0.0483\n",
            "Batch 32 --- Loss: 0.1485\n",
            "Batch 33 --- Loss: 0.1066\n",
            "Batch 34 --- Loss: 0.1087\n",
            "Batch 35 --- Loss: 0.0262\n",
            "Batch 36 --- Loss: 0.0627\n",
            "Batch 37 --- Loss: 0.0797\n",
            "Batch 38 --- Loss: 0.0520\n",
            "Batch 39 --- Loss: 0.0727\n",
            "Batch 40 --- Loss: 0.0790\n",
            "Batch 41 --- Loss: 0.0253\n",
            "Batch 42 --- Loss: 0.0835\n",
            "Batch 43 --- Loss: 0.0974\n",
            "Batch 44 --- Loss: 0.0989\n",
            "Batch 45 --- Loss: 0.0478\n",
            "Batch 46 --- Loss: 0.0902\n",
            "Batch 47 --- Loss: 0.1287\n",
            "Batch 48 --- Loss: 0.1057\n",
            "Batch 49 --- Loss: 0.1756\n",
            "Batch 50 --- Loss: 0.0687\n",
            "Batch 51 --- Loss: 0.0348\n",
            "Batch 52 --- Loss: 0.1030\n",
            "Batch 53 --- Loss: 0.1097\n",
            "Batch 54 --- Loss: 0.0527\n",
            "Batch 55 --- Loss: 0.0971\n",
            "Batch 56 --- Loss: 0.0478\n",
            "Batch 57 --- Loss: 0.0804\n",
            "Batch 58 --- Loss: 0.0525\n",
            "Batch 59 --- Loss: 0.0996\n",
            "Batch 60 --- Loss: 0.0350\n",
            "Batch 61 --- Loss: 0.0980\n",
            "Batch 62 --- Loss: 0.1056\n",
            "Batch 63 --- Loss: 0.0722\n",
            "Batch 64 --- Loss: 0.1502\n",
            "Batch 65 --- Loss: 0.1016\n",
            "Batch 66 --- Loss: 0.0712\n",
            "Batch 67 --- Loss: 0.1144\n",
            "Batch 68 --- Loss: 0.0506\n",
            "Batch 69 --- Loss: 0.0729\n",
            "Batch 70 --- Loss: 0.1826\n",
            "Batch 71 --- Loss: 0.0516\n",
            "Batch 72 --- Loss: 0.1309\n",
            "Batch 73 --- Loss: 0.2264\n",
            "Batch 74 --- Loss: 0.1381\n",
            "Batch 75 --- Loss: 0.0845\n",
            "Batch 76 --- Loss: 0.1158\n",
            "Batch 77 --- Loss: 0.1062\n",
            "Batch 78 --- Loss: 0.2387\n",
            "Batch 79 --- Loss: 0.0404\n",
            "Batch 80 --- Loss: 0.0311\n",
            "Batch 81 --- Loss: 0.1512\n",
            "Batch 82 --- Loss: 0.0830\n",
            "Batch 83 --- Loss: 0.1625\n",
            "Batch 84 --- Loss: 0.0696\n",
            "Epoch 21 / 50 --- Average Loss: 0.0980\n",
            "Average Macro Precision: 0.3791 ---- Average Macro Recall: 0.2969 ---- Average F1 Score: 0.3022 ---- Average Loss: 0.1147\n",
            "Average No Damage Precision: 0.7045 ---- Average No Damage Recall: 0.1837 ---- Average No Damage F1: 0.2697\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8879 ---- Average Background Recall: 0.9978 ---- Average Background F1: 0.9380\n",
            "Batch 0 --- Loss: 0.0719\n",
            "Batch 1 --- Loss: 0.1023\n",
            "Batch 2 --- Loss: 0.0962\n",
            "Batch 3 --- Loss: 0.1064\n",
            "Batch 4 --- Loss: 0.1937\n",
            "Batch 5 --- Loss: 0.1004\n",
            "Batch 6 --- Loss: 0.1285\n",
            "Batch 7 --- Loss: 0.1237\n",
            "Batch 8 --- Loss: 0.0747\n",
            "Batch 9 --- Loss: 0.1006\n",
            "Batch 10 --- Loss: 0.1031\n",
            "Batch 11 --- Loss: 0.0433\n",
            "Batch 12 --- Loss: 0.0759\n",
            "Batch 13 --- Loss: 0.0630\n",
            "Batch 14 --- Loss: 0.0799\n",
            "Batch 15 --- Loss: 0.0953\n",
            "Batch 16 --- Loss: 0.1199\n",
            "Batch 17 --- Loss: 0.0839\n",
            "Batch 18 --- Loss: 0.1408\n",
            "Batch 19 --- Loss: 0.0883\n",
            "Batch 20 --- Loss: 0.1417\n",
            "Batch 21 --- Loss: 0.1670\n",
            "Batch 22 --- Loss: 0.0643\n",
            "Batch 23 --- Loss: 0.0936\n",
            "Batch 24 --- Loss: 0.0686\n",
            "Batch 25 --- Loss: 0.0884\n",
            "Batch 26 --- Loss: 0.1233\n",
            "Batch 27 --- Loss: 0.1135\n",
            "Batch 28 --- Loss: 0.0637\n",
            "Batch 29 --- Loss: 0.0783\n",
            "Batch 30 --- Loss: 0.2111\n",
            "Batch 31 --- Loss: 0.0711\n",
            "Batch 32 --- Loss: 0.0840\n",
            "Batch 33 --- Loss: 0.1984\n",
            "Batch 34 --- Loss: 0.1096\n",
            "Batch 35 --- Loss: 0.0313\n",
            "Batch 36 --- Loss: 0.1164\n",
            "Batch 37 --- Loss: 0.0497\n",
            "Batch 38 --- Loss: 0.0504\n",
            "Batch 39 --- Loss: 0.0673\n",
            "Batch 40 --- Loss: 0.1187\n",
            "Batch 41 --- Loss: 0.0446\n",
            "Batch 42 --- Loss: 0.0859\n",
            "Batch 43 --- Loss: 0.0726\n",
            "Batch 44 --- Loss: 0.1165\n",
            "Batch 45 --- Loss: 0.0567\n",
            "Batch 46 --- Loss: 0.0877\n",
            "Batch 47 --- Loss: 0.1557\n",
            "Batch 48 --- Loss: 0.1003\n",
            "Batch 49 --- Loss: 0.0661\n",
            "Batch 50 --- Loss: 0.0686\n",
            "Batch 51 --- Loss: 0.0335\n",
            "Batch 52 --- Loss: 0.0615\n",
            "Batch 53 --- Loss: 0.1553\n",
            "Batch 54 --- Loss: 0.0504\n",
            "Batch 55 --- Loss: 0.0556\n",
            "Batch 56 --- Loss: 0.0544\n",
            "Batch 57 --- Loss: 0.0954\n",
            "Batch 58 --- Loss: 0.0252\n",
            "Batch 59 --- Loss: 0.1059\n",
            "Batch 60 --- Loss: 0.0301\n",
            "Batch 61 --- Loss: 0.1020\n",
            "Batch 62 --- Loss: 0.1109\n",
            "Batch 63 --- Loss: 0.0754\n",
            "Batch 64 --- Loss: 0.1518\n",
            "Batch 65 --- Loss: 0.1555\n",
            "Batch 66 --- Loss: 0.1157\n",
            "Batch 67 --- Loss: 0.1470\n",
            "Batch 68 --- Loss: 0.0586\n",
            "Batch 69 --- Loss: 0.0812\n",
            "Batch 70 --- Loss: 0.1424\n",
            "Batch 71 --- Loss: 0.0587\n",
            "Batch 72 --- Loss: 0.1938\n",
            "Batch 73 --- Loss: 0.1334\n",
            "Batch 74 --- Loss: 0.1472\n",
            "Batch 75 --- Loss: 0.1552\n",
            "Batch 76 --- Loss: 0.1262\n",
            "Batch 77 --- Loss: 0.1232\n",
            "Batch 78 --- Loss: 0.1514\n",
            "Batch 79 --- Loss: 0.0342\n",
            "Batch 80 --- Loss: 0.0464\n",
            "Batch 81 --- Loss: 0.1990\n",
            "Batch 82 --- Loss: 0.0769\n",
            "Batch 83 --- Loss: 0.2352\n",
            "Batch 84 --- Loss: 0.0682\n",
            "Epoch 22 / 50 --- Average Loss: 0.1002\n",
            "Average Macro Precision: 0.2967 ---- Average Macro Recall: 0.2610 ---- Average F1 Score: 0.2464 ---- Average Loss: 0.1878\n",
            "Average No Damage Precision: 0.3158 ---- Average No Damage Recall: 0.0019 ---- Average No Damage F1: 0.0038\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8646 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9250\n",
            "Batch 0 --- Loss: 0.0916\n",
            "Batch 1 --- Loss: 0.0714\n",
            "Batch 2 --- Loss: 0.1157\n",
            "Batch 3 --- Loss: 0.1490\n",
            "Batch 4 --- Loss: 0.1274\n",
            "Batch 5 --- Loss: 0.1518\n",
            "Batch 6 --- Loss: 0.1491\n",
            "Batch 7 --- Loss: 0.1841\n",
            "Batch 8 --- Loss: 0.0942\n",
            "Batch 9 --- Loss: 0.1115\n",
            "Batch 10 --- Loss: 0.0911\n",
            "Batch 11 --- Loss: 0.0419\n",
            "Batch 12 --- Loss: 0.1885\n",
            "Batch 13 --- Loss: 0.0693\n",
            "Batch 14 --- Loss: 0.0458\n",
            "Batch 15 --- Loss: 0.1293\n",
            "Batch 16 --- Loss: 0.0914\n",
            "Batch 17 --- Loss: 0.0465\n",
            "Batch 18 --- Loss: 0.1298\n",
            "Batch 19 --- Loss: 0.0585\n",
            "Batch 20 --- Loss: 0.1496\n",
            "Batch 21 --- Loss: 0.0823\n",
            "Batch 22 --- Loss: 0.0766\n",
            "Batch 23 --- Loss: 0.0890\n",
            "Batch 24 --- Loss: 0.0723\n",
            "Batch 25 --- Loss: 0.1245\n",
            "Batch 26 --- Loss: 0.1054\n",
            "Batch 27 --- Loss: 0.0918\n",
            "Batch 28 --- Loss: 0.0603\n",
            "Batch 29 --- Loss: 0.1265\n",
            "Batch 30 --- Loss: 0.2169\n",
            "Batch 31 --- Loss: 0.1278\n",
            "Batch 32 --- Loss: 0.1293\n",
            "Batch 33 --- Loss: 0.1185\n",
            "Batch 34 --- Loss: 0.0890\n",
            "Batch 35 --- Loss: 0.0356\n",
            "Batch 36 --- Loss: 0.0690\n",
            "Batch 37 --- Loss: 0.0362\n",
            "Batch 38 --- Loss: 0.0758\n",
            "Batch 39 --- Loss: 0.1285\n",
            "Batch 40 --- Loss: 0.1367\n",
            "Batch 41 --- Loss: 0.0356\n",
            "Batch 42 --- Loss: 0.1205\n",
            "Batch 43 --- Loss: 0.0849\n",
            "Batch 44 --- Loss: 0.1300\n",
            "Batch 45 --- Loss: 0.0597\n",
            "Batch 46 --- Loss: 0.1118\n",
            "Batch 47 --- Loss: 0.0945\n",
            "Batch 48 --- Loss: 0.0921\n",
            "Batch 49 --- Loss: 0.1116\n",
            "Batch 50 --- Loss: 0.0683\n",
            "Batch 51 --- Loss: 0.0621\n",
            "Batch 52 --- Loss: 0.0647\n",
            "Batch 53 --- Loss: 0.1108\n",
            "Batch 54 --- Loss: 0.0633\n",
            "Batch 55 --- Loss: 0.1835\n",
            "Batch 56 --- Loss: 0.1528\n",
            "Batch 57 --- Loss: 0.1325\n",
            "Batch 58 --- Loss: 0.0313\n",
            "Batch 59 --- Loss: 0.1539\n",
            "Batch 60 --- Loss: 0.0629\n",
            "Batch 61 --- Loss: 0.1141\n",
            "Batch 62 --- Loss: 0.1550\n",
            "Batch 63 --- Loss: 0.0812\n",
            "Batch 64 --- Loss: 0.1316\n",
            "Batch 65 --- Loss: 0.0813\n",
            "Batch 66 --- Loss: 0.1007\n",
            "Batch 67 --- Loss: 0.2161\n",
            "Batch 68 --- Loss: 0.0864\n",
            "Batch 69 --- Loss: 0.1039\n",
            "Batch 70 --- Loss: 0.2018\n",
            "Batch 71 --- Loss: 0.0633\n",
            "Batch 72 --- Loss: 0.1497\n",
            "Batch 73 --- Loss: 0.1972\n",
            "Batch 74 --- Loss: 0.1942\n",
            "Batch 75 --- Loss: 0.1052\n",
            "Batch 76 --- Loss: 0.1189\n",
            "Batch 77 --- Loss: 0.0959\n",
            "Batch 78 --- Loss: 0.1210\n",
            "Batch 79 --- Loss: 0.0382\n",
            "Batch 80 --- Loss: 0.0423\n",
            "Batch 81 --- Loss: 0.1410\n",
            "Batch 82 --- Loss: 0.0851\n",
            "Batch 83 --- Loss: 0.3828\n",
            "Batch 84 --- Loss: 0.0673\n",
            "Epoch 23 / 50 --- Average Loss: 0.1092\n",
            "Average Macro Precision: 0.3494 ---- Average Macro Recall: 0.2642 ---- Average F1 Score: 0.2525 ---- Average Loss: 0.1607\n",
            "Average No Damage Precision: 0.5775 ---- Average No Damage Recall: 0.0181 ---- Average No Damage F1: 0.0335\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8666 ---- Average Background Recall: 0.9999 ---- Average Background F1: 0.9262\n",
            "Batch 0 --- Loss: 0.1236\n",
            "Batch 1 --- Loss: 0.0936\n",
            "Batch 2 --- Loss: 0.0866\n",
            "Batch 3 --- Loss: 0.1274\n",
            "Batch 4 --- Loss: 0.1310\n",
            "Batch 5 --- Loss: 0.1631\n",
            "Batch 6 --- Loss: 0.1701\n",
            "Batch 7 --- Loss: 0.1519\n",
            "Batch 8 --- Loss: 0.0893\n",
            "Batch 9 --- Loss: 0.1167\n",
            "Batch 10 --- Loss: 0.0989\n",
            "Batch 11 --- Loss: 0.0590\n",
            "Batch 12 --- Loss: 0.0781\n",
            "Batch 13 --- Loss: 0.0724\n",
            "Batch 14 --- Loss: 0.0528\n",
            "Batch 15 --- Loss: 0.1517\n",
            "Batch 16 --- Loss: 0.1297\n",
            "Batch 17 --- Loss: 0.0474\n",
            "Batch 18 --- Loss: 0.1138\n",
            "Batch 19 --- Loss: 0.1360\n",
            "Batch 20 --- Loss: 0.1284\n",
            "Batch 21 --- Loss: 0.1116\n",
            "Batch 22 --- Loss: 0.1383\n",
            "Batch 23 --- Loss: 0.0787\n",
            "Batch 24 --- Loss: 0.0714\n",
            "Batch 25 --- Loss: 0.1024\n",
            "Batch 26 --- Loss: 0.1292\n",
            "Batch 27 --- Loss: 0.0856\n",
            "Batch 28 --- Loss: 0.0617\n",
            "Batch 29 --- Loss: 0.0802\n",
            "Batch 30 --- Loss: 0.0818\n",
            "Batch 31 --- Loss: 0.1157\n",
            "Batch 32 --- Loss: 0.0943\n",
            "Batch 33 --- Loss: 0.0773\n",
            "Batch 34 --- Loss: 0.1152\n",
            "Batch 35 --- Loss: 0.0333\n",
            "Batch 36 --- Loss: 0.0876\n",
            "Batch 37 --- Loss: 0.0418\n",
            "Batch 38 --- Loss: 0.0462\n",
            "Batch 39 --- Loss: 0.0899\n",
            "Batch 40 --- Loss: 0.0909\n",
            "Batch 41 --- Loss: 0.0359\n",
            "Batch 42 --- Loss: 0.0942\n",
            "Batch 43 --- Loss: 0.0694\n",
            "Batch 44 --- Loss: 0.1022\n",
            "Batch 45 --- Loss: 0.0790\n",
            "Batch 46 --- Loss: 0.1423\n",
            "Batch 47 --- Loss: 0.1030\n",
            "Batch 48 --- Loss: 0.1136\n",
            "Batch 49 --- Loss: 0.1897\n",
            "Batch 50 --- Loss: 0.0698\n",
            "Batch 51 --- Loss: 0.0474\n",
            "Batch 52 --- Loss: 0.0625\n",
            "Batch 53 --- Loss: 0.1159\n",
            "Batch 54 --- Loss: 0.1201\n",
            "Batch 55 --- Loss: 0.0688\n",
            "Batch 56 --- Loss: 0.0573\n",
            "Batch 57 --- Loss: 0.0785\n",
            "Batch 58 --- Loss: 0.0317\n",
            "Batch 59 --- Loss: 0.0887\n",
            "Batch 60 --- Loss: 0.0370\n",
            "Batch 61 --- Loss: 0.0953\n",
            "Batch 62 --- Loss: 0.0606\n",
            "Batch 63 --- Loss: 0.0698\n",
            "Batch 64 --- Loss: 0.1516\n",
            "Batch 65 --- Loss: 0.0695\n",
            "Batch 66 --- Loss: 0.1243\n",
            "Batch 67 --- Loss: 0.1129\n",
            "Batch 68 --- Loss: 0.0560\n",
            "Batch 69 --- Loss: 0.2205\n",
            "Batch 70 --- Loss: 0.1402\n",
            "Batch 71 --- Loss: 0.0787\n",
            "Batch 72 --- Loss: 0.1300\n",
            "Batch 73 --- Loss: 0.1956\n",
            "Batch 74 --- Loss: 0.1589\n",
            "Batch 75 --- Loss: 0.1445\n",
            "Batch 76 --- Loss: 0.0847\n",
            "Batch 77 --- Loss: 0.0962\n",
            "Batch 78 --- Loss: 0.1153\n",
            "Batch 79 --- Loss: 0.0632\n",
            "Batch 80 --- Loss: 0.0517\n",
            "Batch 81 --- Loss: 0.1848\n",
            "Batch 82 --- Loss: 0.0784\n",
            "Batch 83 --- Loss: 0.2229\n",
            "Batch 84 --- Loss: 0.1135\n",
            "Epoch 24 / 50 --- Average Loss: 0.1009\n",
            "Average Macro Precision: 0.3779 ---- Average Macro Recall: 0.2894 ---- Average F1 Score: 0.2919 ---- Average Loss: 0.1333\n",
            "Average No Damage Precision: 0.7021 ---- Average No Damage Recall: 0.1464 ---- Average No Damage F1: 0.2203\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8845 ---- Average Background Recall: 0.9976 ---- Average Background F1: 0.9361\n",
            "Batch 0 --- Loss: 0.1357\n",
            "Batch 1 --- Loss: 0.0724\n",
            "Batch 2 --- Loss: 0.1585\n",
            "Batch 3 --- Loss: 0.1227\n",
            "Batch 4 --- Loss: 0.1078\n",
            "Batch 5 --- Loss: 0.1181\n",
            "Batch 6 --- Loss: 0.1469\n",
            "Batch 7 --- Loss: 0.1043\n",
            "Batch 8 --- Loss: 0.1138\n",
            "Batch 9 --- Loss: 0.1931\n",
            "Batch 10 --- Loss: 0.1028\n",
            "Batch 11 --- Loss: 0.0572\n",
            "Batch 12 --- Loss: 0.0985\n",
            "Batch 13 --- Loss: 0.0747\n",
            "Batch 14 --- Loss: 0.0474\n",
            "Batch 15 --- Loss: 0.1360\n",
            "Batch 16 --- Loss: 0.1662\n",
            "Batch 17 --- Loss: 0.0679\n",
            "Batch 18 --- Loss: 0.0919\n",
            "Batch 19 --- Loss: 0.0932\n",
            "Batch 20 --- Loss: 0.1292\n",
            "Batch 21 --- Loss: 0.1122\n",
            "Batch 22 --- Loss: 0.0753\n",
            "Batch 23 --- Loss: 0.0700\n",
            "Batch 24 --- Loss: 0.1138\n",
            "Batch 25 --- Loss: 0.0820\n",
            "Batch 26 --- Loss: 0.0985\n",
            "Batch 27 --- Loss: 0.1307\n",
            "Batch 28 --- Loss: 0.1236\n",
            "Batch 29 --- Loss: 0.0914\n",
            "Batch 30 --- Loss: 0.1671\n",
            "Batch 31 --- Loss: 0.0543\n",
            "Batch 32 --- Loss: 0.0961\n",
            "Batch 33 --- Loss: 0.0765\n",
            "Batch 34 --- Loss: 0.0966\n",
            "Batch 35 --- Loss: 0.0273\n",
            "Batch 36 --- Loss: 0.0686\n",
            "Batch 37 --- Loss: 0.0352\n",
            "Batch 38 --- Loss: 0.0559\n",
            "Batch 39 --- Loss: 0.1402\n",
            "Batch 40 --- Loss: 0.1981\n",
            "Batch 41 --- Loss: 0.0336\n",
            "Batch 42 --- Loss: 0.0723\n",
            "Batch 43 --- Loss: 0.1400\n",
            "Batch 44 --- Loss: 0.1463\n",
            "Batch 45 --- Loss: 0.0626\n",
            "Batch 46 --- Loss: 0.0981\n",
            "Batch 47 --- Loss: 0.2265\n",
            "Batch 48 --- Loss: 0.1853\n",
            "Batch 49 --- Loss: 0.1259\n",
            "Batch 50 --- Loss: 0.0728\n",
            "Batch 51 --- Loss: 0.0436\n",
            "Batch 52 --- Loss: 0.0975\n",
            "Batch 53 --- Loss: 0.0907\n",
            "Batch 54 --- Loss: 0.0640\n",
            "Batch 55 --- Loss: 0.0634\n",
            "Batch 56 --- Loss: 0.0821\n",
            "Batch 57 --- Loss: 0.1050\n",
            "Batch 58 --- Loss: 0.0300\n",
            "Batch 59 --- Loss: 0.1661\n",
            "Batch 60 --- Loss: 0.0542\n",
            "Batch 61 --- Loss: 0.1558\n",
            "Batch 62 --- Loss: 0.1038\n",
            "Batch 63 --- Loss: 0.0681\n",
            "Batch 64 --- Loss: 0.1016\n",
            "Batch 65 --- Loss: 0.1037\n",
            "Batch 66 --- Loss: 0.0703\n",
            "Batch 67 --- Loss: 0.1550\n",
            "Batch 68 --- Loss: 0.0765\n",
            "Batch 69 --- Loss: 0.0762\n",
            "Batch 70 --- Loss: 0.1748\n",
            "Batch 71 --- Loss: 0.0809\n",
            "Batch 72 --- Loss: 0.1266\n",
            "Batch 73 --- Loss: 0.1861\n",
            "Batch 74 --- Loss: 0.0929\n",
            "Batch 75 --- Loss: 0.1033\n",
            "Batch 76 --- Loss: 0.1261\n",
            "Batch 77 --- Loss: 0.0765\n",
            "Batch 78 --- Loss: 0.2191\n",
            "Batch 79 --- Loss: 0.0302\n",
            "Batch 80 --- Loss: 0.0480\n",
            "Batch 81 --- Loss: 0.1462\n",
            "Batch 82 --- Loss: 0.0759\n",
            "Batch 83 --- Loss: 0.3860\n",
            "Batch 84 --- Loss: 0.0689\n",
            "Epoch 25 / 50 --- Average Loss: 0.1066\n",
            "Average Macro Precision: 0.2782 ---- Average Macro Recall: 0.2613 ---- Average F1 Score: 0.2470 ---- Average Loss: 0.1923\n",
            "Average No Damage Precision: 0.2234 ---- Average No Damage Recall: 0.0036 ---- Average No Damage F1: 0.0069\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8648 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9252\n",
            "Batch 0 --- Loss: 0.0628\n",
            "Batch 1 --- Loss: 0.1140\n",
            "Batch 2 --- Loss: 0.1109\n",
            "Batch 3 --- Loss: 0.1263\n",
            "Batch 4 --- Loss: 0.1155\n",
            "Batch 5 --- Loss: 0.0984\n",
            "Batch 6 --- Loss: 0.0984\n",
            "Batch 7 --- Loss: 0.1478\n",
            "Batch 8 --- Loss: 0.1046\n",
            "Batch 9 --- Loss: 0.1485\n",
            "Batch 10 --- Loss: 0.0825\n",
            "Batch 11 --- Loss: 0.1039\n",
            "Batch 12 --- Loss: 0.1714\n",
            "Batch 13 --- Loss: 0.0773\n",
            "Batch 14 --- Loss: 0.0484\n",
            "Batch 15 --- Loss: 0.1522\n",
            "Batch 16 --- Loss: 0.1100\n",
            "Batch 17 --- Loss: 0.0596\n",
            "Batch 18 --- Loss: 0.0922\n",
            "Batch 19 --- Loss: 0.0890\n",
            "Batch 20 --- Loss: 0.1497\n",
            "Batch 21 --- Loss: 0.0757\n",
            "Batch 22 --- Loss: 0.0797\n",
            "Batch 23 --- Loss: 0.0874\n",
            "Batch 24 --- Loss: 0.0691\n",
            "Batch 25 --- Loss: 0.0828\n",
            "Batch 26 --- Loss: 0.1187\n",
            "Batch 27 --- Loss: 0.0796\n",
            "Batch 28 --- Loss: 0.0622\n",
            "Batch 29 --- Loss: 0.1206\n",
            "Batch 30 --- Loss: 0.1269\n",
            "Batch 31 --- Loss: 0.0543\n",
            "Batch 32 --- Loss: 0.1117\n",
            "Batch 33 --- Loss: 0.1253\n",
            "Batch 34 --- Loss: 0.1017\n",
            "Batch 35 --- Loss: 0.0240\n",
            "Batch 36 --- Loss: 0.0638\n",
            "Batch 37 --- Loss: 0.1059\n",
            "Batch 38 --- Loss: 0.0304\n",
            "Batch 39 --- Loss: 0.0714\n",
            "Batch 40 --- Loss: 0.1228\n",
            "Batch 41 --- Loss: 0.0404\n",
            "Batch 42 --- Loss: 0.0695\n",
            "Batch 43 --- Loss: 0.0902\n",
            "Batch 44 --- Loss: 0.1354\n",
            "Batch 45 --- Loss: 0.0563\n",
            "Batch 46 --- Loss: 0.1274\n",
            "Batch 47 --- Loss: 0.0809\n",
            "Batch 48 --- Loss: 0.1178\n",
            "Batch 49 --- Loss: 0.0684\n",
            "Batch 50 --- Loss: 0.0683\n",
            "Batch 51 --- Loss: 0.0280\n",
            "Batch 52 --- Loss: 0.1156\n",
            "Batch 53 --- Loss: 0.1050\n",
            "Batch 54 --- Loss: 0.0475\n",
            "Batch 55 --- Loss: 0.1611\n",
            "Batch 56 --- Loss: 0.0599\n",
            "Batch 57 --- Loss: 0.1300\n",
            "Batch 58 --- Loss: 0.0347\n",
            "Batch 59 --- Loss: 0.0827\n",
            "Batch 60 --- Loss: 0.0322\n",
            "Batch 61 --- Loss: 0.2067\n",
            "Batch 62 --- Loss: 0.1099\n",
            "Batch 63 --- Loss: 0.0861\n",
            "Batch 64 --- Loss: 0.1455\n",
            "Batch 65 --- Loss: 0.1149\n",
            "Batch 66 --- Loss: 0.1096\n",
            "Batch 67 --- Loss: 0.1241\n",
            "Batch 68 --- Loss: 0.0566\n",
            "Batch 69 --- Loss: 0.1605\n",
            "Batch 70 --- Loss: 0.1653\n",
            "Batch 71 --- Loss: 0.0672\n",
            "Batch 72 --- Loss: 0.1748\n",
            "Batch 73 --- Loss: 0.1472\n",
            "Batch 74 --- Loss: 0.1081\n",
            "Batch 75 --- Loss: 0.1093\n",
            "Batch 76 --- Loss: 0.1320\n",
            "Batch 77 --- Loss: 0.0767\n",
            "Batch 78 --- Loss: 0.1470\n",
            "Batch 79 --- Loss: 0.0451\n",
            "Batch 80 --- Loss: 0.0446\n",
            "Batch 81 --- Loss: 0.1404\n",
            "Batch 82 --- Loss: 0.1031\n",
            "Batch 83 --- Loss: 0.2586\n",
            "Batch 84 --- Loss: 0.0779\n",
            "Epoch 26 / 50 --- Average Loss: 0.1005\n",
            "Average Macro Precision: 0.2627 ---- Average Macro Recall: 0.2607 ---- Average F1 Score: 0.2458 ---- Average Loss: 0.3138\n",
            "Average No Damage Precision: 0.1462 ---- Average No Damage Recall: 0.0004 ---- Average No Damage F1: 0.0009\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8644 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9249\n",
            "Batch 0 --- Loss: 0.0886\n",
            "Batch 1 --- Loss: 0.1153\n",
            "Batch 2 --- Loss: 0.1684\n",
            "Batch 3 --- Loss: 0.1448\n",
            "Batch 4 --- Loss: 0.1342\n",
            "Batch 5 --- Loss: 0.1178\n",
            "Batch 6 --- Loss: 0.1585\n",
            "Batch 7 --- Loss: 0.1158\n",
            "Batch 8 --- Loss: 0.2816\n",
            "Batch 9 --- Loss: 0.1530\n",
            "Batch 10 --- Loss: 0.1019\n",
            "Batch 11 --- Loss: 0.1191\n",
            "Batch 12 --- Loss: 0.1093\n",
            "Batch 13 --- Loss: 0.1075\n",
            "Batch 14 --- Loss: 0.0815\n",
            "Batch 15 --- Loss: 0.1798\n",
            "Batch 16 --- Loss: 0.1643\n",
            "Batch 17 --- Loss: 0.1019\n",
            "Batch 18 --- Loss: 0.2525\n",
            "Batch 19 --- Loss: 0.0887\n",
            "Batch 20 --- Loss: 0.1289\n",
            "Batch 21 --- Loss: 0.0886\n",
            "Batch 22 --- Loss: 0.0901\n",
            "Batch 23 --- Loss: 0.0742\n",
            "Batch 24 --- Loss: 0.0974\n",
            "Batch 25 --- Loss: 0.2246\n",
            "Batch 26 --- Loss: 0.1035\n",
            "Batch 27 --- Loss: 0.2109\n",
            "Batch 28 --- Loss: 0.1186\n",
            "Batch 29 --- Loss: 0.0881\n",
            "Batch 30 --- Loss: 0.0910\n",
            "Batch 31 --- Loss: 0.0553\n",
            "Batch 32 --- Loss: 0.1154\n",
            "Batch 33 --- Loss: 0.1018\n",
            "Batch 34 --- Loss: 0.0878\n",
            "Batch 35 --- Loss: 0.0341\n",
            "Batch 36 --- Loss: 0.0727\n",
            "Batch 37 --- Loss: 0.0960\n",
            "Batch 38 --- Loss: 0.0490\n",
            "Batch 39 --- Loss: 0.0807\n",
            "Batch 40 --- Loss: 0.0906\n",
            "Batch 41 --- Loss: 0.0287\n",
            "Batch 42 --- Loss: 0.0724\n",
            "Batch 43 --- Loss: 0.0677\n",
            "Batch 44 --- Loss: 0.1029\n",
            "Batch 45 --- Loss: 0.0670\n",
            "Batch 46 --- Loss: 0.1144\n",
            "Batch 47 --- Loss: 0.0853\n",
            "Batch 48 --- Loss: 0.1220\n",
            "Batch 49 --- Loss: 0.1296\n",
            "Batch 50 --- Loss: 0.0787\n",
            "Batch 51 --- Loss: 0.0383\n",
            "Batch 52 --- Loss: 0.1206\n",
            "Batch 53 --- Loss: 0.1007\n",
            "Batch 54 --- Loss: 0.1196\n",
            "Batch 55 --- Loss: 0.1737\n",
            "Batch 56 --- Loss: 0.0643\n",
            "Batch 57 --- Loss: 0.0930\n",
            "Batch 58 --- Loss: 0.0352\n",
            "Batch 59 --- Loss: 0.0823\n",
            "Batch 60 --- Loss: 0.0313\n",
            "Batch 61 --- Loss: 0.1770\n",
            "Batch 62 --- Loss: 0.0631\n",
            "Batch 63 --- Loss: 0.1102\n",
            "Batch 64 --- Loss: 0.1711\n",
            "Batch 65 --- Loss: 0.1027\n",
            "Batch 66 --- Loss: 0.0836\n",
            "Batch 67 --- Loss: 0.1221\n",
            "Batch 68 --- Loss: 0.0534\n",
            "Batch 69 --- Loss: 0.1124\n",
            "Batch 70 --- Loss: 0.1676\n",
            "Batch 71 --- Loss: 0.0552\n",
            "Batch 72 --- Loss: 0.2144\n",
            "Batch 73 --- Loss: 0.1866\n",
            "Batch 74 --- Loss: 0.1293\n",
            "Batch 75 --- Loss: 0.1363\n",
            "Batch 76 --- Loss: 0.1898\n",
            "Batch 77 --- Loss: 0.0778\n",
            "Batch 78 --- Loss: 0.1180\n",
            "Batch 79 --- Loss: 0.0257\n",
            "Batch 80 --- Loss: 0.0478\n",
            "Batch 81 --- Loss: 0.2070\n",
            "Batch 82 --- Loss: 0.0837\n",
            "Batch 83 --- Loss: 0.3007\n",
            "Batch 84 --- Loss: 0.1115\n",
            "Epoch 27 / 50 --- Average Loss: 0.1136\n",
            "Average Macro Precision: 0.3611 ---- Average Macro Recall: 0.2670 ---- Average F1 Score: 0.2576 ---- Average Loss: 0.1430\n",
            "Average No Damage Precision: 0.6341 ---- Average No Damage Recall: 0.0323 ---- Average No Damage F1: 0.0577\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8681 ---- Average Background Recall: 0.9997 ---- Average Background F1: 0.9271\n",
            "Batch 0 --- Loss: 0.1107\n",
            "Batch 1 --- Loss: 0.1057\n",
            "Batch 2 --- Loss: 0.1042\n",
            "Batch 3 --- Loss: 0.1584\n",
            "Batch 4 --- Loss: 0.1351\n",
            "Batch 5 --- Loss: 0.1008\n",
            "Batch 6 --- Loss: 0.0996\n",
            "Batch 7 --- Loss: 0.1654\n",
            "Batch 8 --- Loss: 0.1000\n",
            "Batch 9 --- Loss: 0.1302\n",
            "Batch 10 --- Loss: 0.1978\n",
            "Batch 11 --- Loss: 0.0867\n",
            "Batch 12 --- Loss: 0.1106\n",
            "Batch 13 --- Loss: 0.0674\n",
            "Batch 14 --- Loss: 0.0633\n",
            "Batch 15 --- Loss: 0.1003\n",
            "Batch 16 --- Loss: 0.0791\n",
            "Batch 17 --- Loss: 0.0919\n",
            "Batch 18 --- Loss: 0.0923\n",
            "Batch 19 --- Loss: 0.0842\n",
            "Batch 20 --- Loss: 0.1330\n",
            "Batch 21 --- Loss: 0.0951\n",
            "Batch 22 --- Loss: 0.0670\n",
            "Batch 23 --- Loss: 0.0686\n",
            "Batch 24 --- Loss: 0.0890\n",
            "Batch 25 --- Loss: 0.0741\n",
            "Batch 26 --- Loss: 0.0637\n",
            "Batch 27 --- Loss: 0.1483\n",
            "Batch 28 --- Loss: 0.0794\n",
            "Batch 29 --- Loss: 0.1625\n",
            "Batch 30 --- Loss: 0.1346\n",
            "Batch 31 --- Loss: 0.1030\n",
            "Batch 32 --- Loss: 0.1294\n",
            "Batch 33 --- Loss: 0.0863\n",
            "Batch 34 --- Loss: 0.0890\n",
            "Batch 35 --- Loss: 0.0181\n",
            "Batch 36 --- Loss: 0.0559\n",
            "Batch 37 --- Loss: 0.0409\n",
            "Batch 38 --- Loss: 0.0359\n",
            "Batch 39 --- Loss: 0.0807\n",
            "Batch 40 --- Loss: 0.2048\n",
            "Batch 41 --- Loss: 0.0435\n",
            "Batch 42 --- Loss: 0.0643\n",
            "Batch 43 --- Loss: 0.0706\n",
            "Batch 44 --- Loss: 0.1041\n",
            "Batch 45 --- Loss: 0.0620\n",
            "Batch 46 --- Loss: 0.1422\n",
            "Batch 47 --- Loss: 0.1410\n",
            "Batch 48 --- Loss: 0.1185\n",
            "Batch 49 --- Loss: 0.1156\n",
            "Batch 50 --- Loss: 0.1094\n",
            "Batch 51 --- Loss: 0.0424\n",
            "Batch 52 --- Loss: 0.0638\n",
            "Batch 53 --- Loss: 0.0792\n",
            "Batch 54 --- Loss: 0.0726\n",
            "Batch 55 --- Loss: 0.0802\n",
            "Batch 56 --- Loss: 0.1130\n",
            "Batch 57 --- Loss: 0.1416\n",
            "Batch 58 --- Loss: 0.0348\n",
            "Batch 59 --- Loss: 0.2107\n",
            "Batch 60 --- Loss: 0.0494\n",
            "Batch 61 --- Loss: 0.1532\n",
            "Batch 62 --- Loss: 0.0622\n",
            "Batch 63 --- Loss: 0.1179\n",
            "Batch 64 --- Loss: 0.1100\n",
            "Batch 65 --- Loss: 0.0732\n",
            "Batch 66 --- Loss: 0.1021\n",
            "Batch 67 --- Loss: 0.1272\n",
            "Batch 68 --- Loss: 0.0611\n",
            "Batch 69 --- Loss: 0.0788\n",
            "Batch 70 --- Loss: 0.1226\n",
            "Batch 71 --- Loss: 0.0586\n",
            "Batch 72 --- Loss: 0.1674\n",
            "Batch 73 --- Loss: 0.1819\n",
            "Batch 74 --- Loss: 0.1340\n",
            "Batch 75 --- Loss: 0.0934\n",
            "Batch 76 --- Loss: 0.1991\n",
            "Batch 77 --- Loss: 0.0742\n",
            "Batch 78 --- Loss: 0.1252\n",
            "Batch 79 --- Loss: 0.0493\n",
            "Batch 80 --- Loss: 0.0391\n",
            "Batch 81 --- Loss: 0.1755\n",
            "Batch 82 --- Loss: 0.1027\n",
            "Batch 83 --- Loss: 0.1999\n",
            "Batch 84 --- Loss: 0.1067\n",
            "Epoch 28 / 50 --- Average Loss: 0.1026\n",
            "Average Macro Precision: 0.3138 ---- Average Macro Recall: 0.2628 ---- Average F1 Score: 0.2499 ---- Average Loss: 0.1518\n",
            "Average No Damage Precision: 0.4003 ---- Average No Damage Recall: 0.0110 ---- Average No Damage F1: 0.0205\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8658 ---- Average Background Recall: 0.9999 ---- Average Background F1: 0.9258\n",
            "Batch 0 --- Loss: 0.0937\n",
            "Batch 1 --- Loss: 0.0887\n",
            "Batch 2 --- Loss: 0.1308\n",
            "Batch 3 --- Loss: 0.1998\n",
            "Batch 4 --- Loss: 0.1126\n",
            "Batch 5 --- Loss: 0.1036\n",
            "Batch 6 --- Loss: 0.1257\n",
            "Batch 7 --- Loss: 0.1176\n",
            "Batch 8 --- Loss: 0.1071\n",
            "Batch 9 --- Loss: 0.1282\n",
            "Batch 10 --- Loss: 0.0779\n",
            "Batch 11 --- Loss: 0.0478\n",
            "Batch 12 --- Loss: 0.1311\n",
            "Batch 13 --- Loss: 0.0883\n",
            "Batch 14 --- Loss: 0.0560\n",
            "Batch 15 --- Loss: 0.1501\n",
            "Batch 16 --- Loss: 0.1415\n",
            "Batch 17 --- Loss: 0.0803\n",
            "Batch 18 --- Loss: 0.1518\n",
            "Batch 19 --- Loss: 0.0682\n",
            "Batch 20 --- Loss: 0.1208\n",
            "Batch 21 --- Loss: 0.0978\n",
            "Batch 22 --- Loss: 0.1066\n",
            "Batch 23 --- Loss: 0.1569\n",
            "Batch 24 --- Loss: 0.0775\n",
            "Batch 25 --- Loss: 0.1750\n",
            "Batch 26 --- Loss: 0.0851\n",
            "Batch 27 --- Loss: 0.0907\n",
            "Batch 28 --- Loss: 0.0641\n",
            "Batch 29 --- Loss: 0.1273\n",
            "Batch 30 --- Loss: 0.1107\n",
            "Batch 31 --- Loss: 0.1031\n",
            "Batch 32 --- Loss: 0.1431\n",
            "Batch 33 --- Loss: 0.1332\n",
            "Batch 34 --- Loss: 0.1022\n",
            "Batch 35 --- Loss: 0.0264\n",
            "Batch 36 --- Loss: 0.1133\n",
            "Batch 37 --- Loss: 0.0368\n",
            "Batch 38 --- Loss: 0.0501\n",
            "Batch 39 --- Loss: 0.0652\n",
            "Batch 40 --- Loss: 0.1204\n",
            "Batch 41 --- Loss: 0.0282\n",
            "Batch 42 --- Loss: 0.1094\n",
            "Batch 43 --- Loss: 0.2027\n",
            "Batch 44 --- Loss: 0.1236\n",
            "Batch 45 --- Loss: 0.0556\n",
            "Batch 46 --- Loss: 0.1271\n",
            "Batch 47 --- Loss: 0.0981\n",
            "Batch 48 --- Loss: 0.1458\n",
            "Batch 49 --- Loss: 0.1369\n",
            "Batch 50 --- Loss: 0.0622\n",
            "Batch 51 --- Loss: 0.0639\n",
            "Batch 52 --- Loss: 0.0667\n",
            "Batch 53 --- Loss: 0.1037\n",
            "Batch 54 --- Loss: 0.0731\n",
            "Batch 55 --- Loss: 0.0686\n",
            "Batch 56 --- Loss: 0.0620\n",
            "Batch 57 --- Loss: 0.1132\n",
            "Batch 58 --- Loss: 0.0569\n",
            "Batch 59 --- Loss: 0.0888\n",
            "Batch 60 --- Loss: 0.0400\n",
            "Batch 61 --- Loss: 0.2018\n",
            "Batch 62 --- Loss: 0.0702\n",
            "Batch 63 --- Loss: 0.0677\n",
            "Batch 64 --- Loss: 0.2415\n",
            "Batch 65 --- Loss: 0.0648\n",
            "Batch 66 --- Loss: 0.0692\n",
            "Batch 67 --- Loss: 0.1721\n",
            "Batch 68 --- Loss: 0.0834\n",
            "Batch 69 --- Loss: 0.0718\n",
            "Batch 70 --- Loss: 0.1456\n",
            "Batch 71 --- Loss: 0.0560\n",
            "Batch 72 --- Loss: 0.1735\n",
            "Batch 73 --- Loss: 0.1351\n",
            "Batch 74 --- Loss: 0.1396\n",
            "Batch 75 --- Loss: 0.0928\n",
            "Batch 76 --- Loss: 0.1649\n",
            "Batch 77 --- Loss: 0.0635\n",
            "Batch 78 --- Loss: 0.1124\n",
            "Batch 79 --- Loss: 0.0387\n",
            "Batch 80 --- Loss: 0.0482\n",
            "Batch 81 --- Loss: 0.1886\n",
            "Batch 82 --- Loss: 0.0917\n",
            "Batch 83 --- Loss: 0.2201\n",
            "Batch 84 --- Loss: 0.1230\n",
            "Epoch 29 / 50 --- Average Loss: 0.1055\n",
            "Average Macro Precision: 0.3619 ---- Average Macro Recall: 0.2889 ---- Average F1 Score: 0.2908 ---- Average Loss: 0.1209\n",
            "Average No Damage Precision: 0.6223 ---- Average No Damage Recall: 0.1433 ---- Average No Damage F1: 0.2148\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8843 ---- Average Background Recall: 0.9982 ---- Average Background F1: 0.9363\n",
            "Batch 0 --- Loss: 0.0961\n",
            "Batch 1 --- Loss: 0.0676\n",
            "Batch 2 --- Loss: 0.1069\n",
            "Batch 3 --- Loss: 0.1649\n",
            "Batch 4 --- Loss: 0.1549\n",
            "Batch 5 --- Loss: 0.0999\n",
            "Batch 6 --- Loss: 0.0948\n",
            "Batch 7 --- Loss: 0.1101\n",
            "Batch 8 --- Loss: 0.0756\n",
            "Batch 9 --- Loss: 0.1293\n",
            "Batch 10 --- Loss: 0.1672\n",
            "Batch 11 --- Loss: 0.0833\n",
            "Batch 12 --- Loss: 0.1512\n",
            "Batch 13 --- Loss: 0.0774\n",
            "Batch 14 --- Loss: 0.0423\n",
            "Batch 15 --- Loss: 0.1901\n",
            "Batch 16 --- Loss: 0.1193\n",
            "Batch 17 --- Loss: 0.1025\n",
            "Batch 18 --- Loss: 0.1956\n",
            "Batch 19 --- Loss: 0.0583\n",
            "Batch 20 --- Loss: 0.1403\n",
            "Batch 21 --- Loss: 0.0814\n",
            "Batch 22 --- Loss: 0.0905\n",
            "Batch 23 --- Loss: 0.0849\n",
            "Batch 24 --- Loss: 0.0908\n",
            "Batch 25 --- Loss: 0.0839\n",
            "Batch 26 --- Loss: 0.0679\n",
            "Batch 27 --- Loss: 0.0906\n",
            "Batch 28 --- Loss: 0.0611\n",
            "Batch 29 --- Loss: 0.1161\n",
            "Batch 30 --- Loss: 0.1110\n",
            "Batch 31 --- Loss: 0.0542\n",
            "Batch 32 --- Loss: 0.1024\n",
            "Batch 33 --- Loss: 0.0843\n",
            "Batch 34 --- Loss: 0.0938\n",
            "Batch 35 --- Loss: 0.1108\n",
            "Batch 36 --- Loss: 0.0758\n",
            "Batch 37 --- Loss: 0.0389\n",
            "Batch 38 --- Loss: 0.0336\n",
            "Batch 39 --- Loss: 0.0759\n",
            "Batch 40 --- Loss: 0.0885\n",
            "Batch 41 --- Loss: 0.0341\n",
            "Batch 42 --- Loss: 0.0688\n",
            "Batch 43 --- Loss: 0.0632\n",
            "Batch 44 --- Loss: 0.1028\n",
            "Batch 45 --- Loss: 0.0542\n",
            "Batch 46 --- Loss: 0.1893\n",
            "Batch 47 --- Loss: 0.0818\n",
            "Batch 48 --- Loss: 0.1625\n",
            "Batch 49 --- Loss: 0.1292\n",
            "Batch 50 --- Loss: 0.0460\n",
            "Batch 51 --- Loss: 0.0320\n",
            "Batch 52 --- Loss: 0.0620\n",
            "Batch 53 --- Loss: 0.0744\n",
            "Batch 54 --- Loss: 0.1195\n",
            "Batch 55 --- Loss: 0.0896\n",
            "Batch 56 --- Loss: 0.0469\n",
            "Batch 57 --- Loss: 0.1655\n",
            "Batch 58 --- Loss: 0.0346\n",
            "Batch 59 --- Loss: 0.0827\n",
            "Batch 60 --- Loss: 0.0320\n",
            "Batch 61 --- Loss: 0.1031\n",
            "Batch 62 --- Loss: 0.0541\n",
            "Batch 63 --- Loss: 0.0710\n",
            "Batch 64 --- Loss: 0.2080\n",
            "Batch 65 --- Loss: 0.1256\n",
            "Batch 66 --- Loss: 0.0817\n",
            "Batch 67 --- Loss: 0.1120\n",
            "Batch 68 --- Loss: 0.0478\n",
            "Batch 69 --- Loss: 0.1606\n",
            "Batch 70 --- Loss: 0.1479\n",
            "Batch 71 --- Loss: 0.0888\n",
            "Batch 72 --- Loss: 0.1172\n",
            "Batch 73 --- Loss: 0.1309\n",
            "Batch 74 --- Loss: 0.1392\n",
            "Batch 75 --- Loss: 0.1294\n",
            "Batch 76 --- Loss: 0.0802\n",
            "Batch 77 --- Loss: 0.0707\n",
            "Batch 78 --- Loss: 0.1649\n",
            "Batch 79 --- Loss: 0.0315\n",
            "Batch 80 --- Loss: 0.0418\n",
            "Batch 81 --- Loss: 0.1360\n",
            "Batch 82 --- Loss: 0.1226\n",
            "Batch 83 --- Loss: 0.2726\n",
            "Batch 84 --- Loss: 0.0618\n",
            "Epoch 30 / 50 --- Average Loss: 0.0992\n",
            "Average Macro Precision: 0.3418 ---- Average Macro Recall: 0.2638 ---- Average F1 Score: 0.2517 ---- Average Loss: 0.1418\n",
            "Average No Damage Precision: 0.5395 ---- Average No Damage Recall: 0.0159 ---- Average No Damage F1: 0.0293\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8663 ---- Average Background Recall: 0.9999 ---- Average Background F1: 0.9261\n",
            "Batch 0 --- Loss: 0.1387\n",
            "Batch 1 --- Loss: 0.1105\n",
            "Batch 2 --- Loss: 0.0928\n",
            "Batch 3 --- Loss: 0.1207\n",
            "Batch 4 --- Loss: 0.1089\n",
            "Batch 5 --- Loss: 0.1568\n",
            "Batch 6 --- Loss: 0.1262\n",
            "Batch 7 --- Loss: 0.2346\n",
            "Batch 8 --- Loss: 0.1289\n",
            "Batch 9 --- Loss: 0.1138\n",
            "Batch 10 --- Loss: 0.0660\n",
            "Batch 11 --- Loss: 0.0433\n",
            "Batch 12 --- Loss: 0.0886\n",
            "Batch 13 --- Loss: 0.0727\n",
            "Batch 14 --- Loss: 0.0406\n",
            "Batch 15 --- Loss: 0.1764\n",
            "Batch 16 --- Loss: 0.0733\n",
            "Batch 17 --- Loss: 0.0494\n",
            "Batch 18 --- Loss: 0.1237\n",
            "Batch 19 --- Loss: 0.0570\n",
            "Batch 20 --- Loss: 0.1305\n",
            "Batch 21 --- Loss: 0.0747\n",
            "Batch 22 --- Loss: 0.0732\n",
            "Batch 23 --- Loss: 0.0879\n",
            "Batch 24 --- Loss: 0.0570\n",
            "Batch 25 --- Loss: 0.1837\n",
            "Batch 26 --- Loss: 0.0709\n",
            "Batch 27 --- Loss: 0.1319\n",
            "Batch 28 --- Loss: 0.0690\n",
            "Batch 29 --- Loss: 0.1275\n",
            "Batch 30 --- Loss: 0.1069\n",
            "Batch 31 --- Loss: 0.0560\n",
            "Batch 32 --- Loss: 0.1299\n",
            "Batch 33 --- Loss: 0.0701\n",
            "Batch 34 --- Loss: 0.1124\n",
            "Batch 35 --- Loss: 0.0262\n",
            "Batch 36 --- Loss: 0.0586\n",
            "Batch 37 --- Loss: 0.0428\n",
            "Batch 38 --- Loss: 0.0315\n",
            "Batch 39 --- Loss: 0.0695\n",
            "Batch 40 --- Loss: 0.1019\n",
            "Batch 41 --- Loss: 0.0428\n",
            "Batch 42 --- Loss: 0.1012\n",
            "Batch 43 --- Loss: 0.0884\n",
            "Batch 44 --- Loss: 0.1062\n",
            "Batch 45 --- Loss: 0.0538\n",
            "Batch 46 --- Loss: 0.0899\n",
            "Batch 47 --- Loss: 0.0734\n",
            "Batch 48 --- Loss: 0.1688\n",
            "Batch 49 --- Loss: 0.0735\n",
            "Batch 50 --- Loss: 0.0925\n",
            "Batch 51 --- Loss: 0.0324\n",
            "Batch 52 --- Loss: 0.1083\n",
            "Batch 53 --- Loss: 0.1054\n",
            "Batch 54 --- Loss: 0.0421\n",
            "Batch 55 --- Loss: 0.0534\n",
            "Batch 56 --- Loss: 0.0561\n",
            "Batch 57 --- Loss: 0.1301\n",
            "Batch 58 --- Loss: 0.0725\n",
            "Batch 59 --- Loss: 0.1146\n",
            "Batch 60 --- Loss: 0.0327\n",
            "Batch 61 --- Loss: 0.1428\n",
            "Batch 62 --- Loss: 0.1641\n",
            "Batch 63 --- Loss: 0.0619\n",
            "Batch 64 --- Loss: 0.1319\n",
            "Batch 65 --- Loss: 0.1144\n",
            "Batch 66 --- Loss: 0.0671\n",
            "Batch 67 --- Loss: 0.1188\n",
            "Batch 68 --- Loss: 0.0524\n",
            "Batch 69 --- Loss: 0.1051\n",
            "Batch 70 --- Loss: 0.1433\n",
            "Batch 71 --- Loss: 0.0513\n",
            "Batch 72 --- Loss: 0.1147\n",
            "Batch 73 --- Loss: 0.1843\n",
            "Batch 74 --- Loss: 0.1852\n",
            "Batch 75 --- Loss: 0.0901\n",
            "Batch 76 --- Loss: 0.0758\n",
            "Batch 77 --- Loss: 0.0719\n",
            "Batch 78 --- Loss: 0.1242\n",
            "Batch 79 --- Loss: 0.0478\n",
            "Batch 80 --- Loss: 0.0569\n",
            "Batch 81 --- Loss: 0.1788\n",
            "Batch 82 --- Loss: 0.0745\n",
            "Batch 83 --- Loss: 0.2555\n",
            "Batch 84 --- Loss: 0.0637\n",
            "Epoch 31 / 50 --- Average Loss: 0.0971\n",
            "Average Macro Precision: 0.3817 ---- Average Macro Recall: 0.3339 ---- Average F1 Score: 0.3378 ---- Average Loss: 0.1002\n",
            "Average No Damage Precision: 0.6835 ---- Average No Damage Recall: 0.3798 ---- Average No Damage F1: 0.4333\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.9219 ---- Average Background Recall: 0.9866 ---- Average Background F1: 0.9525\n",
            "Saved Model to drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_31_0.33777622661215284.pt\n",
            "Batch 0 --- Loss: 0.0914\n",
            "Batch 1 --- Loss: 0.0735\n",
            "Batch 2 --- Loss: 0.1078\n",
            "Batch 3 --- Loss: 0.1004\n",
            "Batch 4 --- Loss: 0.2510\n",
            "Batch 5 --- Loss: 0.0890\n",
            "Batch 6 --- Loss: 0.0917\n",
            "Batch 7 --- Loss: 0.1072\n",
            "Batch 8 --- Loss: 0.0791\n",
            "Batch 9 --- Loss: 0.1092\n",
            "Batch 10 --- Loss: 0.0673\n",
            "Batch 11 --- Loss: 0.0390\n",
            "Batch 12 --- Loss: 0.1175\n",
            "Batch 13 --- Loss: 0.1083\n",
            "Batch 14 --- Loss: 0.0419\n",
            "Batch 15 --- Loss: 0.1577\n",
            "Batch 16 --- Loss: 0.0747\n",
            "Batch 17 --- Loss: 0.0434\n",
            "Batch 18 --- Loss: 0.1099\n",
            "Batch 19 --- Loss: 0.1325\n",
            "Batch 20 --- Loss: 0.1302\n",
            "Batch 21 --- Loss: 0.0731\n",
            "Batch 22 --- Loss: 0.0713\n",
            "Batch 23 --- Loss: 0.0597\n",
            "Batch 24 --- Loss: 0.0946\n",
            "Batch 25 --- Loss: 0.0812\n",
            "Batch 26 --- Loss: 0.0760\n",
            "Batch 27 --- Loss: 0.0759\n",
            "Batch 28 --- Loss: 0.0703\n",
            "Batch 29 --- Loss: 0.0784\n",
            "Batch 30 --- Loss: 0.1087\n",
            "Batch 31 --- Loss: 0.0494\n",
            "Batch 32 --- Loss: 0.0791\n",
            "Batch 33 --- Loss: 0.0930\n",
            "Batch 34 --- Loss: 0.1160\n",
            "Batch 35 --- Loss: 0.0198\n",
            "Batch 36 --- Loss: 0.0709\n",
            "Batch 37 --- Loss: 0.0348\n",
            "Batch 38 --- Loss: 0.0279\n",
            "Batch 39 --- Loss: 0.0703\n",
            "Batch 40 --- Loss: 0.1611\n",
            "Batch 41 --- Loss: 0.0272\n",
            "Batch 42 --- Loss: 0.0895\n",
            "Batch 43 --- Loss: 0.0607\n",
            "Batch 44 --- Loss: 0.1417\n",
            "Batch 45 --- Loss: 0.0503\n",
            "Batch 46 --- Loss: 0.0890\n",
            "Batch 47 --- Loss: 0.0753\n",
            "Batch 48 --- Loss: 0.1098\n",
            "Batch 49 --- Loss: 0.0653\n",
            "Batch 50 --- Loss: 0.0655\n",
            "Batch 51 --- Loss: 0.0384\n",
            "Batch 52 --- Loss: 0.0573\n",
            "Batch 53 --- Loss: 0.0692\n",
            "Batch 54 --- Loss: 0.0457\n",
            "Batch 55 --- Loss: 0.0514\n",
            "Batch 56 --- Loss: 0.0455\n",
            "Batch 57 --- Loss: 0.2524\n",
            "Batch 58 --- Loss: 0.0490\n",
            "Batch 59 --- Loss: 0.0702\n",
            "Batch 60 --- Loss: 0.0278\n",
            "Batch 61 --- Loss: 0.0968\n",
            "Batch 62 --- Loss: 0.0979\n",
            "Batch 63 --- Loss: 0.1266\n",
            "Batch 64 --- Loss: 0.1081\n",
            "Batch 65 --- Loss: 0.0732\n",
            "Batch 66 --- Loss: 0.1195\n",
            "Batch 67 --- Loss: 0.1171\n",
            "Batch 68 --- Loss: 0.0679\n",
            "Batch 69 --- Loss: 0.1063\n",
            "Batch 70 --- Loss: 0.1579\n",
            "Batch 71 --- Loss: 0.0537\n",
            "Batch 72 --- Loss: 0.1867\n",
            "Batch 73 --- Loss: 0.1292\n",
            "Batch 74 --- Loss: 0.0888\n",
            "Batch 75 --- Loss: 0.0798\n",
            "Batch 76 --- Loss: 0.0768\n",
            "Batch 77 --- Loss: 0.0626\n",
            "Batch 78 --- Loss: 0.1686\n",
            "Batch 79 --- Loss: 0.0280\n",
            "Batch 80 --- Loss: 0.0447\n",
            "Batch 81 --- Loss: 0.2230\n",
            "Batch 82 --- Loss: 0.1002\n",
            "Batch 83 --- Loss: 0.2138\n",
            "Batch 84 --- Loss: 0.0758\n",
            "Epoch 32 / 50 --- Average Loss: 0.0908\n",
            "Average Macro Precision: 0.3350 ---- Average Macro Recall: 0.2635 ---- Average F1 Score: 0.2512 ---- Average Loss: 0.1655\n",
            "Average No Damage Precision: 0.5055 ---- Average No Damage Recall: 0.0147 ---- Average No Damage F1: 0.0269\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8662 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9260\n",
            "Batch 0 --- Loss: 0.1391\n",
            "Batch 1 --- Loss: 0.0867\n",
            "Batch 2 --- Loss: 0.0874\n",
            "Batch 3 --- Loss: 0.1117\n",
            "Batch 4 --- Loss: 0.1301\n",
            "Batch 5 --- Loss: 0.1208\n",
            "Batch 6 --- Loss: 0.0905\n",
            "Batch 7 --- Loss: 0.1011\n",
            "Batch 8 --- Loss: 0.1318\n",
            "Batch 9 --- Loss: 0.0901\n",
            "Batch 10 --- Loss: 0.1095\n",
            "Batch 11 --- Loss: 0.0464\n",
            "Batch 12 --- Loss: 0.0786\n",
            "Batch 13 --- Loss: 0.0967\n",
            "Batch 14 --- Loss: 0.0920\n",
            "Batch 15 --- Loss: 0.1401\n",
            "Batch 16 --- Loss: 0.0718\n",
            "Batch 17 --- Loss: 0.0478\n",
            "Batch 18 --- Loss: 0.1265\n",
            "Batch 19 --- Loss: 0.0681\n",
            "Batch 20 --- Loss: 0.1130\n",
            "Batch 21 --- Loss: 0.0699\n",
            "Batch 22 --- Loss: 0.1641\n",
            "Batch 23 --- Loss: 0.0913\n",
            "Batch 24 --- Loss: 0.0604\n",
            "Batch 25 --- Loss: 0.1652\n",
            "Batch 26 --- Loss: 0.1084\n",
            "Batch 27 --- Loss: 0.1334\n",
            "Batch 28 --- Loss: 0.0620\n",
            "Batch 29 --- Loss: 0.0845\n",
            "Batch 30 --- Loss: 0.1315\n",
            "Batch 31 --- Loss: 0.1225\n",
            "Batch 32 --- Loss: 0.0836\n",
            "Batch 33 --- Loss: 0.0944\n",
            "Batch 34 --- Loss: 0.0976\n",
            "Batch 35 --- Loss: 0.0430\n",
            "Batch 36 --- Loss: 0.0571\n",
            "Batch 37 --- Loss: 0.0348\n",
            "Batch 38 --- Loss: 0.0334\n",
            "Batch 39 --- Loss: 0.0637\n",
            "Batch 40 --- Loss: 0.0847\n",
            "Batch 41 --- Loss: 0.0277\n",
            "Batch 42 --- Loss: 0.1224\n",
            "Batch 43 --- Loss: 0.0584\n",
            "Batch 44 --- Loss: 0.1679\n",
            "Batch 45 --- Loss: 0.0436\n",
            "Batch 46 --- Loss: 0.1034\n",
            "Batch 47 --- Loss: 0.0751\n",
            "Batch 48 --- Loss: 0.1050\n",
            "Batch 49 --- Loss: 0.1658\n",
            "Batch 50 --- Loss: 0.0660\n",
            "Batch 51 --- Loss: 0.0649\n",
            "Batch 52 --- Loss: 0.0537\n",
            "Batch 53 --- Loss: 0.1161\n",
            "Batch 54 --- Loss: 0.0676\n",
            "Batch 55 --- Loss: 0.1068\n",
            "Batch 56 --- Loss: 0.0908\n",
            "Batch 57 --- Loss: 0.0834\n",
            "Batch 58 --- Loss: 0.0287\n",
            "Batch 59 --- Loss: 0.0904\n",
            "Batch 60 --- Loss: 0.0339\n",
            "Batch 61 --- Loss: 0.0942\n",
            "Batch 62 --- Loss: 0.0782\n",
            "Batch 63 --- Loss: 0.0686\n",
            "Batch 64 --- Loss: 0.1595\n",
            "Batch 65 --- Loss: 0.1157\n",
            "Batch 66 --- Loss: 0.1116\n",
            "Batch 67 --- Loss: 0.1631\n",
            "Batch 68 --- Loss: 0.0532\n",
            "Batch 69 --- Loss: 0.0669\n",
            "Batch 70 --- Loss: 0.1110\n",
            "Batch 71 --- Loss: 0.0426\n",
            "Batch 72 --- Loss: 0.1912\n",
            "Batch 73 --- Loss: 0.1972\n",
            "Batch 74 --- Loss: 0.0853\n",
            "Batch 75 --- Loss: 0.0895\n",
            "Batch 76 --- Loss: 0.1062\n",
            "Batch 77 --- Loss: 0.1194\n",
            "Batch 78 --- Loss: 0.1510\n",
            "Batch 79 --- Loss: 0.0306\n",
            "Batch 80 --- Loss: 0.0464\n",
            "Batch 81 --- Loss: 0.1329\n",
            "Batch 82 --- Loss: 0.1022\n",
            "Batch 83 --- Loss: 0.1972\n",
            "Batch 84 --- Loss: 0.0669\n",
            "Epoch 33 / 50 --- Average Loss: 0.0955\n",
            "Average Macro Precision: 0.3357 ---- Average Macro Recall: 0.2647 ---- Average F1 Score: 0.2532 ---- Average Loss: 0.1548\n",
            "Average No Damage Precision: 0.5089 ---- Average No Damage Recall: 0.0204 ---- Average No Damage F1: 0.0368\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8667 ---- Average Background Recall: 0.9999 ---- Average Background F1: 0.9263\n",
            "Batch 0 --- Loss: 0.0715\n",
            "Batch 1 --- Loss: 0.0860\n",
            "Batch 2 --- Loss: 0.0891\n",
            "Batch 3 --- Loss: 0.1083\n",
            "Batch 4 --- Loss: 0.1060\n",
            "Batch 5 --- Loss: 0.1089\n",
            "Batch 6 --- Loss: 0.0898\n",
            "Batch 7 --- Loss: 0.1620\n",
            "Batch 8 --- Loss: 0.0705\n",
            "Batch 9 --- Loss: 0.1440\n",
            "Batch 10 --- Loss: 0.1083\n",
            "Batch 11 --- Loss: 0.0883\n",
            "Batch 12 --- Loss: 0.0703\n",
            "Batch 13 --- Loss: 0.0863\n",
            "Batch 14 --- Loss: 0.0913\n",
            "Batch 15 --- Loss: 0.1927\n",
            "Batch 16 --- Loss: 0.1162\n",
            "Batch 17 --- Loss: 0.0487\n",
            "Batch 18 --- Loss: 0.0923\n",
            "Batch 19 --- Loss: 0.0487\n",
            "Batch 20 --- Loss: 0.1351\n",
            "Batch 21 --- Loss: 0.0967\n",
            "Batch 22 --- Loss: 0.0741\n",
            "Batch 23 --- Loss: 0.1125\n",
            "Batch 24 --- Loss: 0.0624\n",
            "Batch 25 --- Loss: 0.1077\n",
            "Batch 26 --- Loss: 0.0903\n",
            "Batch 27 --- Loss: 0.1907\n",
            "Batch 28 --- Loss: 0.0525\n",
            "Batch 29 --- Loss: 0.0786\n",
            "Batch 30 --- Loss: 0.0800\n",
            "Batch 31 --- Loss: 0.0503\n",
            "Batch 32 --- Loss: 0.1087\n",
            "Batch 33 --- Loss: 0.0575\n",
            "Batch 34 --- Loss: 0.0946\n",
            "Batch 35 --- Loss: 0.0257\n",
            "Batch 36 --- Loss: 0.1157\n",
            "Batch 37 --- Loss: 0.1005\n",
            "Batch 38 --- Loss: 0.0408\n",
            "Batch 39 --- Loss: 0.0658\n",
            "Batch 40 --- Loss: 0.0685\n",
            "Batch 41 --- Loss: 0.0193\n",
            "Batch 42 --- Loss: 0.1089\n",
            "Batch 43 --- Loss: 0.0992\n",
            "Batch 44 --- Loss: 0.0989\n",
            "Batch 45 --- Loss: 0.0570\n",
            "Batch 46 --- Loss: 0.1236\n",
            "Batch 47 --- Loss: 0.1053\n",
            "Batch 48 --- Loss: 0.1121\n",
            "Batch 49 --- Loss: 0.0651\n",
            "Batch 50 --- Loss: 0.0529\n",
            "Batch 51 --- Loss: 0.0315\n",
            "Batch 52 --- Loss: 0.0514\n",
            "Batch 53 --- Loss: 0.0741\n",
            "Batch 54 --- Loss: 0.0664\n",
            "Batch 55 --- Loss: 0.0887\n",
            "Batch 56 --- Loss: 0.0573\n",
            "Batch 57 --- Loss: 0.1117\n",
            "Batch 58 --- Loss: 0.0252\n",
            "Batch 59 --- Loss: 0.0915\n",
            "Batch 60 --- Loss: 0.0550\n",
            "Batch 61 --- Loss: 0.1998\n",
            "Batch 62 --- Loss: 0.0944\n",
            "Batch 63 --- Loss: 0.0585\n",
            "Batch 64 --- Loss: 0.0915\n",
            "Batch 65 --- Loss: 0.1816\n",
            "Batch 66 --- Loss: 0.0652\n",
            "Batch 67 --- Loss: 0.1677\n",
            "Batch 68 --- Loss: 0.0542\n",
            "Batch 69 --- Loss: 0.1053\n",
            "Batch 70 --- Loss: 0.1020\n",
            "Batch 71 --- Loss: 0.0779\n",
            "Batch 72 --- Loss: 0.1199\n",
            "Batch 73 --- Loss: 0.1799\n",
            "Batch 74 --- Loss: 0.1285\n",
            "Batch 75 --- Loss: 0.0820\n",
            "Batch 76 --- Loss: 0.0765\n",
            "Batch 77 --- Loss: 0.0666\n",
            "Batch 78 --- Loss: 0.0973\n",
            "Batch 79 --- Loss: 0.0352\n",
            "Batch 80 --- Loss: 0.0444\n",
            "Batch 81 --- Loss: 0.1897\n",
            "Batch 82 --- Loss: 0.0725\n",
            "Batch 83 --- Loss: 0.2092\n",
            "Batch 84 --- Loss: 0.0683\n",
            "Epoch 34 / 50 --- Average Loss: 0.0924\n",
            "Average Macro Precision: 0.3772 ---- Average Macro Recall: 0.3036 ---- Average F1 Score: 0.3087 ---- Average Loss: 0.1232\n",
            "Average No Damage Precision: 0.6871 ---- Average No Damage Recall: 0.2192 ---- Average No Damage F1: 0.2989\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8958 ---- Average Background Recall: 0.9956 ---- Average Background F1: 0.9418\n",
            "Batch 0 --- Loss: 0.1059\n",
            "Batch 1 --- Loss: 0.1077\n",
            "Batch 2 --- Loss: 0.1138\n",
            "Batch 3 --- Loss: 0.1492\n",
            "Batch 4 --- Loss: 0.1039\n",
            "Batch 5 --- Loss: 0.0948\n",
            "Batch 6 --- Loss: 0.1344\n",
            "Batch 7 --- Loss: 0.1191\n",
            "Batch 8 --- Loss: 0.1002\n",
            "Batch 9 --- Loss: 0.1094\n",
            "Batch 10 --- Loss: 0.0693\n",
            "Batch 11 --- Loss: 0.0378\n",
            "Batch 12 --- Loss: 0.0667\n",
            "Batch 13 --- Loss: 0.0839\n",
            "Batch 14 --- Loss: 0.0636\n",
            "Batch 15 --- Loss: 0.0900\n",
            "Batch 16 --- Loss: 0.1047\n",
            "Batch 17 --- Loss: 0.0426\n",
            "Batch 18 --- Loss: 0.1512\n",
            "Batch 19 --- Loss: 0.0581\n",
            "Batch 20 --- Loss: 0.1320\n",
            "Batch 21 --- Loss: 0.0641\n",
            "Batch 22 --- Loss: 0.0748\n",
            "Batch 23 --- Loss: 0.0568\n",
            "Batch 24 --- Loss: 0.0596\n",
            "Batch 25 --- Loss: 0.1249\n",
            "Batch 26 --- Loss: 0.0718\n",
            "Batch 27 --- Loss: 0.0744\n",
            "Batch 28 --- Loss: 0.0569\n",
            "Batch 29 --- Loss: 0.0720\n",
            "Batch 30 --- Loss: 0.1053\n",
            "Batch 31 --- Loss: 0.1071\n",
            "Batch 32 --- Loss: 0.1072\n",
            "Batch 33 --- Loss: 0.0783\n",
            "Batch 34 --- Loss: 0.0986\n",
            "Batch 35 --- Loss: 0.0203\n",
            "Batch 36 --- Loss: 0.0527\n",
            "Batch 37 --- Loss: 0.0350\n",
            "Batch 38 --- Loss: 0.0430\n",
            "Batch 39 --- Loss: 0.1235\n",
            "Batch 40 --- Loss: 0.0720\n",
            "Batch 41 --- Loss: 0.0249\n",
            "Batch 42 --- Loss: 0.0622\n",
            "Batch 43 --- Loss: 0.0881\n",
            "Batch 44 --- Loss: 0.0982\n",
            "Batch 45 --- Loss: 0.0714\n",
            "Batch 46 --- Loss: 0.1632\n",
            "Batch 47 --- Loss: 0.1055\n",
            "Batch 48 --- Loss: 0.1070\n",
            "Batch 49 --- Loss: 0.0582\n",
            "Batch 50 --- Loss: 0.0709\n",
            "Batch 51 --- Loss: 0.0323\n",
            "Batch 52 --- Loss: 0.0600\n",
            "Batch 53 --- Loss: 0.1017\n",
            "Batch 54 --- Loss: 0.0918\n",
            "Batch 55 --- Loss: 0.0640\n",
            "Batch 56 --- Loss: 0.0807\n",
            "Batch 57 --- Loss: 0.1108\n",
            "Batch 58 --- Loss: 0.0282\n",
            "Batch 59 --- Loss: 0.1651\n",
            "Batch 60 --- Loss: 0.0250\n",
            "Batch 61 --- Loss: 0.0902\n",
            "Batch 62 --- Loss: 0.0560\n",
            "Batch 63 --- Loss: 0.0689\n",
            "Batch 64 --- Loss: 0.1347\n",
            "Batch 65 --- Loss: 0.0991\n",
            "Batch 66 --- Loss: 0.1546\n",
            "Batch 67 --- Loss: 0.1685\n",
            "Batch 68 --- Loss: 0.0459\n",
            "Batch 69 --- Loss: 0.1205\n",
            "Batch 70 --- Loss: 0.1365\n",
            "Batch 71 --- Loss: 0.0461\n",
            "Batch 72 --- Loss: 0.1237\n",
            "Batch 73 --- Loss: 0.1774\n",
            "Batch 74 --- Loss: 0.1438\n",
            "Batch 75 --- Loss: 0.1264\n",
            "Batch 76 --- Loss: 0.1500\n",
            "Batch 77 --- Loss: 0.0717\n",
            "Batch 78 --- Loss: 0.1238\n",
            "Batch 79 --- Loss: 0.0336\n",
            "Batch 80 --- Loss: 0.0296\n",
            "Batch 81 --- Loss: 0.1331\n",
            "Batch 82 --- Loss: 0.0899\n",
            "Batch 83 --- Loss: 0.1984\n",
            "Batch 84 --- Loss: 0.0607\n",
            "Epoch 35 / 50 --- Average Loss: 0.0909\n",
            "Average Macro Precision: 0.3619 ---- Average Macro Recall: 0.3129 ---- Average F1 Score: 0.3174 ---- Average Loss: 0.1046\n",
            "Average No Damage Precision: 0.6008 ---- Average No Damage Recall: 0.2687 ---- Average No Damage F1: 0.3375\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.9057 ---- Average Background Recall: 0.9930 ---- Average Background F1: 0.9465\n",
            "Batch 0 --- Loss: 0.1687\n",
            "Batch 1 --- Loss: 0.0852\n",
            "Batch 2 --- Loss: 0.1060\n",
            "Batch 3 --- Loss: 0.1392\n",
            "Batch 4 --- Loss: 0.1188\n",
            "Batch 5 --- Loss: 0.1021\n",
            "Batch 6 --- Loss: 0.1319\n",
            "Batch 7 --- Loss: 0.2045\n",
            "Batch 8 --- Loss: 0.0898\n",
            "Batch 9 --- Loss: 0.0962\n",
            "Batch 10 --- Loss: 0.0796\n",
            "Batch 11 --- Loss: 0.0504\n",
            "Batch 12 --- Loss: 0.0893\n",
            "Batch 13 --- Loss: 0.1093\n",
            "Batch 14 --- Loss: 0.0825\n",
            "Batch 15 --- Loss: 0.0899\n",
            "Batch 16 --- Loss: 0.0715\n",
            "Batch 17 --- Loss: 0.0627\n",
            "Batch 18 --- Loss: 0.1760\n",
            "Batch 19 --- Loss: 0.0534\n",
            "Batch 20 --- Loss: 0.1692\n",
            "Batch 21 --- Loss: 0.0960\n",
            "Batch 22 --- Loss: 0.1071\n",
            "Batch 23 --- Loss: 0.1048\n",
            "Batch 24 --- Loss: 0.0456\n",
            "Batch 25 --- Loss: 0.1085\n",
            "Batch 26 --- Loss: 0.0608\n",
            "Batch 27 --- Loss: 0.1667\n",
            "Batch 28 --- Loss: 0.0542\n",
            "Batch 29 --- Loss: 0.0983\n",
            "Batch 30 --- Loss: 0.1688\n",
            "Batch 31 --- Loss: 0.0985\n",
            "Batch 32 --- Loss: 0.0796\n",
            "Batch 33 --- Loss: 0.1388\n",
            "Batch 34 --- Loss: 0.1006\n",
            "Batch 35 --- Loss: 0.0306\n",
            "Batch 36 --- Loss: 0.0728\n",
            "Batch 37 --- Loss: 0.0336\n",
            "Batch 38 --- Loss: 0.0461\n",
            "Batch 39 --- Loss: 0.0820\n",
            "Batch 40 --- Loss: 0.1723\n",
            "Batch 41 --- Loss: 0.0405\n",
            "Batch 42 --- Loss: 0.0736\n",
            "Batch 43 --- Loss: 0.1005\n",
            "Batch 44 --- Loss: 0.1591\n",
            "Batch 45 --- Loss: 0.0495\n",
            "Batch 46 --- Loss: 0.0794\n",
            "Batch 47 --- Loss: 0.1200\n",
            "Batch 48 --- Loss: 0.1065\n",
            "Batch 49 --- Loss: 0.0664\n",
            "Batch 50 --- Loss: 0.0603\n",
            "Batch 51 --- Loss: 0.0239\n",
            "Batch 52 --- Loss: 0.1246\n",
            "Batch 53 --- Loss: 0.1020\n",
            "Batch 54 --- Loss: 0.0733\n",
            "Batch 55 --- Loss: 0.0460\n",
            "Batch 56 --- Loss: 0.0784\n",
            "Batch 57 --- Loss: 0.0928\n",
            "Batch 58 --- Loss: 0.0286\n",
            "Batch 59 --- Loss: 0.1823\n",
            "Batch 60 --- Loss: 0.0247\n",
            "Batch 61 --- Loss: 0.1891\n",
            "Batch 62 --- Loss: 0.0637\n",
            "Batch 63 --- Loss: 0.0605\n",
            "Batch 64 --- Loss: 0.1418\n",
            "Batch 65 --- Loss: 0.1347\n",
            "Batch 66 --- Loss: 0.1432\n",
            "Batch 67 --- Loss: 0.1133\n",
            "Batch 68 --- Loss: 0.0623\n",
            "Batch 69 --- Loss: 0.0926\n",
            "Batch 70 --- Loss: 0.0996\n",
            "Batch 71 --- Loss: 0.0453\n",
            "Batch 72 --- Loss: 0.1817\n",
            "Batch 73 --- Loss: 0.1862\n",
            "Batch 74 --- Loss: 0.0889\n",
            "Batch 75 --- Loss: 0.1278\n",
            "Batch 76 --- Loss: 0.1255\n",
            "Batch 77 --- Loss: 0.1092\n",
            "Batch 78 --- Loss: 0.1213\n",
            "Batch 79 --- Loss: 0.0228\n",
            "Batch 80 --- Loss: 0.0330\n",
            "Batch 81 --- Loss: 0.1294\n",
            "Batch 82 --- Loss: 0.1028\n",
            "Batch 83 --- Loss: 0.1445\n",
            "Batch 84 --- Loss: 0.0595\n",
            "Epoch 36 / 50 --- Average Loss: 0.0983\n",
            "Average Macro Precision: 0.3307 ---- Average Macro Recall: 0.2625 ---- Average F1 Score: 0.2495 ---- Average Loss: 0.1563\n",
            "Average No Damage Precision: 0.4849 ---- Average No Damage Recall: 0.0101 ---- Average No Damage F1: 0.0189\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8657 ---- Average Background Recall: 0.9995 ---- Average Background F1: 0.9255\n",
            "Batch 0 --- Loss: 0.1104\n",
            "Batch 1 --- Loss: 0.1508\n",
            "Batch 2 --- Loss: 0.0974\n",
            "Batch 3 --- Loss: 0.0937\n",
            "Batch 4 --- Loss: 0.2292\n",
            "Batch 5 --- Loss: 0.1465\n",
            "Batch 6 --- Loss: 0.0918\n",
            "Batch 7 --- Loss: 0.1452\n",
            "Batch 8 --- Loss: 0.1655\n",
            "Batch 9 --- Loss: 0.1523\n",
            "Batch 10 --- Loss: 0.1142\n",
            "Batch 11 --- Loss: 0.0351\n",
            "Batch 12 --- Loss: 0.0714\n",
            "Batch 13 --- Loss: 0.0768\n",
            "Batch 14 --- Loss: 0.0356\n",
            "Batch 15 --- Loss: 0.1463\n",
            "Batch 16 --- Loss: 0.0737\n",
            "Batch 17 --- Loss: 0.0642\n",
            "Batch 18 --- Loss: 0.0896\n",
            "Batch 19 --- Loss: 0.0674\n",
            "Batch 20 --- Loss: 0.1318\n",
            "Batch 21 --- Loss: 0.1015\n",
            "Batch 22 --- Loss: 0.0740\n",
            "Batch 23 --- Loss: 0.0831\n",
            "Batch 24 --- Loss: 0.0612\n",
            "Batch 25 --- Loss: 0.1141\n",
            "Batch 26 --- Loss: 0.0606\n",
            "Batch 27 --- Loss: 0.1164\n",
            "Batch 28 --- Loss: 0.0532\n",
            "Batch 29 --- Loss: 0.0987\n",
            "Batch 30 --- Loss: 0.0779\n",
            "Batch 31 --- Loss: 0.0784\n",
            "Batch 32 --- Loss: 0.1114\n",
            "Batch 33 --- Loss: 0.1055\n",
            "Batch 34 --- Loss: 0.1207\n",
            "Batch 35 --- Loss: 0.0242\n",
            "Batch 36 --- Loss: 0.0688\n",
            "Batch 37 --- Loss: 0.0405\n",
            "Batch 38 --- Loss: 0.0365\n",
            "Batch 39 --- Loss: 0.0830\n",
            "Batch 40 --- Loss: 0.0716\n",
            "Batch 41 --- Loss: 0.0276\n",
            "Batch 42 --- Loss: 0.1349\n",
            "Batch 43 --- Loss: 0.0590\n",
            "Batch 44 --- Loss: 0.1263\n",
            "Batch 45 --- Loss: 0.0497\n",
            "Batch 46 --- Loss: 0.0820\n",
            "Batch 47 --- Loss: 0.1325\n",
            "Batch 48 --- Loss: 0.1428\n",
            "Batch 49 --- Loss: 0.2091\n",
            "Batch 50 --- Loss: 0.0334\n",
            "Batch 51 --- Loss: 0.0291\n",
            "Batch 52 --- Loss: 0.1170\n",
            "Batch 53 --- Loss: 0.1456\n",
            "Batch 54 --- Loss: 0.0962\n",
            "Batch 55 --- Loss: 0.0991\n",
            "Batch 56 --- Loss: 0.1234\n",
            "Batch 57 --- Loss: 0.1566\n",
            "Batch 58 --- Loss: 0.0281\n",
            "Batch 59 --- Loss: 0.1706\n",
            "Batch 60 --- Loss: 0.0666\n",
            "Batch 61 --- Loss: 0.1800\n",
            "Batch 62 --- Loss: 0.0635\n",
            "Batch 63 --- Loss: 0.0910\n",
            "Batch 64 --- Loss: 0.3242\n",
            "Batch 65 --- Loss: 0.0632\n",
            "Batch 66 --- Loss: 0.0660\n",
            "Batch 67 --- Loss: 0.1041\n",
            "Batch 68 --- Loss: 0.0561\n",
            "Batch 69 --- Loss: 0.1058\n",
            "Batch 70 --- Loss: 0.1427\n",
            "Batch 71 --- Loss: 0.0522\n",
            "Batch 72 --- Loss: 0.2632\n",
            "Batch 73 --- Loss: 0.1308\n",
            "Batch 74 --- Loss: 0.1541\n",
            "Batch 75 --- Loss: 0.0820\n",
            "Batch 76 --- Loss: 0.0719\n",
            "Batch 77 --- Loss: 0.1695\n",
            "Batch 78 --- Loss: 0.2563\n",
            "Batch 79 --- Loss: 0.0278\n",
            "Batch 80 --- Loss: 0.0392\n",
            "Batch 81 --- Loss: 0.1516\n",
            "Batch 82 --- Loss: 0.0867\n",
            "Batch 83 --- Loss: 0.2527\n",
            "Batch 84 --- Loss: 0.1149\n",
            "Epoch 37 / 50 --- Average Loss: 0.1053\n",
            "Average Macro Precision: 0.3701 ---- Average Macro Recall: 0.2925 ---- Average F1 Score: 0.2941 ---- Average Loss: 0.1188\n",
            "Average No Damage Precision: 0.6647 ---- Average No Damage Recall: 0.1638 ---- Average No Damage F1: 0.2339\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8827 ---- Average Background Recall: 0.9954 ---- Average Background F1: 0.9337\n",
            "Batch 0 --- Loss: 0.0891\n",
            "Batch 1 --- Loss: 0.0897\n",
            "Batch 2 --- Loss: 0.0998\n",
            "Batch 3 --- Loss: 0.1348\n",
            "Batch 4 --- Loss: 0.1488\n",
            "Batch 5 --- Loss: 0.1026\n",
            "Batch 6 --- Loss: 0.1380\n",
            "Batch 7 --- Loss: 0.1461\n",
            "Batch 8 --- Loss: 0.0694\n",
            "Batch 9 --- Loss: 0.1351\n",
            "Batch 10 --- Loss: 0.0766\n",
            "Batch 11 --- Loss: 0.0451\n",
            "Batch 12 --- Loss: 0.0670\n",
            "Batch 13 --- Loss: 0.1120\n",
            "Batch 14 --- Loss: 0.0383\n",
            "Batch 15 --- Loss: 0.0881\n",
            "Batch 16 --- Loss: 0.0709\n",
            "Batch 17 --- Loss: 0.0591\n",
            "Batch 18 --- Loss: 0.1323\n",
            "Batch 19 --- Loss: 0.0470\n",
            "Batch 20 --- Loss: 0.1418\n",
            "Batch 21 --- Loss: 0.0746\n",
            "Batch 22 --- Loss: 0.0895\n",
            "Batch 23 --- Loss: 0.0599\n",
            "Batch 24 --- Loss: 0.1086\n",
            "Batch 25 --- Loss: 0.1411\n",
            "Batch 26 --- Loss: 0.0663\n",
            "Batch 27 --- Loss: 0.1372\n",
            "Batch 28 --- Loss: 0.0994\n",
            "Batch 29 --- Loss: 0.1223\n",
            "Batch 30 --- Loss: 0.0727\n",
            "Batch 31 --- Loss: 0.1821\n",
            "Batch 32 --- Loss: 0.0889\n",
            "Batch 33 --- Loss: 0.0936\n",
            "Batch 34 --- Loss: 0.1025\n",
            "Batch 35 --- Loss: 0.0268\n",
            "Batch 36 --- Loss: 0.0886\n",
            "Batch 37 --- Loss: 0.0634\n",
            "Batch 38 --- Loss: 0.0428\n",
            "Batch 39 --- Loss: 0.0694\n",
            "Batch 40 --- Loss: 0.1170\n",
            "Batch 41 --- Loss: 0.0258\n",
            "Batch 42 --- Loss: 0.0845\n",
            "Batch 43 --- Loss: 0.1490\n",
            "Batch 44 --- Loss: 0.2029\n",
            "Batch 45 --- Loss: 0.0444\n",
            "Batch 46 --- Loss: 0.1328\n",
            "Batch 47 --- Loss: 0.1069\n",
            "Batch 48 --- Loss: 0.1167\n",
            "Batch 49 --- Loss: 0.1115\n",
            "Batch 50 --- Loss: 0.0922\n",
            "Batch 51 --- Loss: 0.0319\n",
            "Batch 52 --- Loss: 0.1716\n",
            "Batch 53 --- Loss: 0.1141\n",
            "Batch 54 --- Loss: 0.0524\n",
            "Batch 55 --- Loss: 0.0891\n",
            "Batch 56 --- Loss: 0.0847\n",
            "Batch 57 --- Loss: 0.1302\n",
            "Batch 58 --- Loss: 0.0298\n",
            "Batch 59 --- Loss: 0.1110\n",
            "Batch 60 --- Loss: 0.0241\n",
            "Batch 61 --- Loss: 0.1598\n",
            "Batch 62 --- Loss: 0.0953\n",
            "Batch 63 --- Loss: 0.0681\n",
            "Batch 64 --- Loss: 0.1267\n",
            "Batch 65 --- Loss: 0.0941\n",
            "Batch 66 --- Loss: 0.0790\n",
            "Batch 67 --- Loss: 0.1139\n",
            "Batch 68 --- Loss: 0.0467\n",
            "Batch 69 --- Loss: 0.1333\n",
            "Batch 70 --- Loss: 0.1116\n",
            "Batch 71 --- Loss: 0.0671\n",
            "Batch 72 --- Loss: 0.1401\n",
            "Batch 73 --- Loss: 0.1870\n",
            "Batch 74 --- Loss: 0.1348\n",
            "Batch 75 --- Loss: 0.0819\n",
            "Batch 76 --- Loss: 0.1120\n",
            "Batch 77 --- Loss: 0.1078\n",
            "Batch 78 --- Loss: 0.1063\n",
            "Batch 79 --- Loss: 0.0255\n",
            "Batch 80 --- Loss: 0.0451\n",
            "Batch 81 --- Loss: 0.1776\n",
            "Batch 82 --- Loss: 0.1402\n",
            "Batch 83 --- Loss: 0.2290\n",
            "Batch 84 --- Loss: 0.0711\n",
            "Epoch 38 / 50 --- Average Loss: 0.0993\n",
            "Average Macro Precision: 0.3847 ---- Average Macro Recall: 0.3168 ---- Average F1 Score: 0.3268 ---- Average Loss: 0.1090\n",
            "Average No Damage Precision: 0.7172 ---- Average No Damage Recall: 0.2860 ---- Average No Damage F1: 0.3850\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.9033 ---- Average Background Recall: 0.9951 ---- Average Background F1: 0.9459\n",
            "Batch 0 --- Loss: 0.0861\n",
            "Batch 1 --- Loss: 0.1033\n",
            "Batch 2 --- Loss: 0.1000\n",
            "Batch 3 --- Loss: 0.1883\n",
            "Batch 4 --- Loss: 0.1914\n",
            "Batch 5 --- Loss: 0.1145\n",
            "Batch 6 --- Loss: 0.1893\n",
            "Batch 7 --- Loss: 0.1996\n",
            "Batch 8 --- Loss: 0.1022\n",
            "Batch 9 --- Loss: 0.0944\n",
            "Batch 10 --- Loss: 0.0652\n",
            "Batch 11 --- Loss: 0.0810\n",
            "Batch 12 --- Loss: 0.0696\n",
            "Batch 13 --- Loss: 0.0763\n",
            "Batch 14 --- Loss: 0.0464\n",
            "Batch 15 --- Loss: 0.1263\n",
            "Batch 16 --- Loss: 0.0729\n",
            "Batch 17 --- Loss: 0.1060\n",
            "Batch 18 --- Loss: 0.1337\n",
            "Batch 19 --- Loss: 0.0550\n",
            "Batch 20 --- Loss: 0.1296\n",
            "Batch 21 --- Loss: 0.1408\n",
            "Batch 22 --- Loss: 0.0722\n",
            "Batch 23 --- Loss: 0.0568\n",
            "Batch 24 --- Loss: 0.1062\n",
            "Batch 25 --- Loss: 0.1246\n",
            "Batch 26 --- Loss: 0.0690\n",
            "Batch 27 --- Loss: 0.0832\n",
            "Batch 28 --- Loss: 0.1129\n",
            "Batch 29 --- Loss: 0.0994\n",
            "Batch 30 --- Loss: 0.1754\n",
            "Batch 31 --- Loss: 0.0531\n",
            "Batch 32 --- Loss: 0.1139\n",
            "Batch 33 --- Loss: 0.0856\n",
            "Batch 34 --- Loss: 0.0967\n",
            "Batch 35 --- Loss: 0.0207\n",
            "Batch 36 --- Loss: 0.0813\n",
            "Batch 37 --- Loss: 0.0814\n",
            "Batch 38 --- Loss: 0.0560\n",
            "Batch 39 --- Loss: 0.0739\n",
            "Batch 40 --- Loss: 0.1562\n",
            "Batch 41 --- Loss: 0.0209\n",
            "Batch 42 --- Loss: 0.1130\n",
            "Batch 43 --- Loss: 0.0873\n",
            "Batch 44 --- Loss: 0.1587\n",
            "Batch 45 --- Loss: 0.0957\n",
            "Batch 46 --- Loss: 0.1160\n",
            "Batch 47 --- Loss: 0.1730\n",
            "Batch 48 --- Loss: 0.1493\n",
            "Batch 49 --- Loss: 0.1320\n",
            "Batch 50 --- Loss: 0.0845\n",
            "Batch 51 --- Loss: 0.0256\n",
            "Batch 52 --- Loss: 0.0600\n",
            "Batch 53 --- Loss: 0.1021\n",
            "Batch 54 --- Loss: 0.0632\n",
            "Batch 55 --- Loss: 0.0519\n",
            "Batch 56 --- Loss: 0.0453\n",
            "Batch 57 --- Loss: 0.1474\n",
            "Batch 58 --- Loss: 0.0326\n",
            "Batch 59 --- Loss: 0.0961\n",
            "Batch 60 --- Loss: 0.0262\n",
            "Batch 61 --- Loss: 0.0965\n",
            "Batch 62 --- Loss: 0.0588\n",
            "Batch 63 --- Loss: 0.0725\n",
            "Batch 64 --- Loss: 0.0927\n",
            "Batch 65 --- Loss: 0.0662\n",
            "Batch 66 --- Loss: 0.1018\n",
            "Batch 67 --- Loss: 0.1681\n",
            "Batch 68 --- Loss: 0.0441\n",
            "Batch 69 --- Loss: 0.1136\n",
            "Batch 70 --- Loss: 0.1157\n",
            "Batch 71 --- Loss: 0.1163\n",
            "Batch 72 --- Loss: 0.1244\n",
            "Batch 73 --- Loss: 0.1241\n",
            "Batch 74 --- Loss: 0.0842\n",
            "Batch 75 --- Loss: 0.1241\n",
            "Batch 76 --- Loss: 0.1064\n",
            "Batch 77 --- Loss: 0.1106\n",
            "Batch 78 --- Loss: 0.1106\n",
            "Batch 79 --- Loss: 0.0386\n",
            "Batch 80 --- Loss: 0.0447\n",
            "Batch 81 --- Loss: 0.2089\n",
            "Batch 82 --- Loss: 0.1301\n",
            "Batch 83 --- Loss: 0.1632\n",
            "Batch 84 --- Loss: 0.1274\n",
            "Epoch 39 / 50 --- Average Loss: 0.1002\n",
            "Average Macro Precision: 0.3497 ---- Average Macro Recall: 0.2756 ---- Average F1 Score: 0.2707 ---- Average Loss: 0.1343\n",
            "Average No Damage Precision: 0.5702 ---- Average No Damage Recall: 0.0766 ---- Average No Damage F1: 0.1195\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8755 ---- Average Background Recall: 0.9986 ---- Average Background F1: 0.9312\n",
            "Batch 0 --- Loss: 0.1366\n",
            "Batch 1 --- Loss: 0.0795\n",
            "Batch 2 --- Loss: 0.0882\n",
            "Batch 3 --- Loss: 0.1000\n",
            "Batch 4 --- Loss: 0.1019\n",
            "Batch 5 --- Loss: 0.1005\n",
            "Batch 6 --- Loss: 0.1317\n",
            "Batch 7 --- Loss: 0.1126\n",
            "Batch 8 --- Loss: 0.1113\n",
            "Batch 9 --- Loss: 0.1482\n",
            "Batch 10 --- Loss: 0.0666\n",
            "Batch 11 --- Loss: 0.0392\n",
            "Batch 12 --- Loss: 0.0765\n",
            "Batch 13 --- Loss: 0.0883\n",
            "Batch 14 --- Loss: 0.0596\n",
            "Batch 15 --- Loss: 0.0883\n",
            "Batch 16 --- Loss: 0.1140\n",
            "Batch 17 --- Loss: 0.0414\n",
            "Batch 18 --- Loss: 0.0816\n",
            "Batch 19 --- Loss: 0.1135\n",
            "Batch 20 --- Loss: 0.1172\n",
            "Batch 21 --- Loss: 0.0669\n",
            "Batch 22 --- Loss: 0.1238\n",
            "Batch 23 --- Loss: 0.0930\n",
            "Batch 24 --- Loss: 0.1005\n",
            "Batch 25 --- Loss: 0.1880\n",
            "Batch 26 --- Loss: 0.0638\n",
            "Batch 27 --- Loss: 0.0782\n",
            "Batch 28 --- Loss: 0.1009\n",
            "Batch 29 --- Loss: 0.1339\n",
            "Batch 30 --- Loss: 0.0832\n",
            "Batch 31 --- Loss: 0.0544\n",
            "Batch 32 --- Loss: 0.0794\n",
            "Batch 33 --- Loss: 0.0945\n",
            "Batch 34 --- Loss: 0.0898\n",
            "Batch 35 --- Loss: 0.0256\n",
            "Batch 36 --- Loss: 0.0693\n",
            "Batch 37 --- Loss: 0.1064\n",
            "Batch 38 --- Loss: 0.0373\n",
            "Batch 39 --- Loss: 0.0668\n",
            "Batch 40 --- Loss: 0.0768\n",
            "Batch 41 --- Loss: 0.0179\n",
            "Batch 42 --- Loss: 0.1516\n",
            "Batch 43 --- Loss: 0.0946\n",
            "Batch 44 --- Loss: 0.1110\n",
            "Batch 45 --- Loss: 0.1203\n",
            "Batch 46 --- Loss: 0.0780\n",
            "Batch 47 --- Loss: 0.0798\n",
            "Batch 48 --- Loss: 0.1198\n",
            "Batch 49 --- Loss: 0.1029\n",
            "Batch 50 --- Loss: 0.0613\n",
            "Batch 51 --- Loss: 0.0228\n",
            "Batch 52 --- Loss: 0.0504\n",
            "Batch 53 --- Loss: 0.1024\n",
            "Batch 54 --- Loss: 0.0432\n",
            "Batch 55 --- Loss: 0.0586\n",
            "Batch 56 --- Loss: 0.0850\n",
            "Batch 57 --- Loss: 0.1533\n",
            "Batch 58 --- Loss: 0.0280\n",
            "Batch 59 --- Loss: 0.1061\n",
            "Batch 60 --- Loss: 0.0313\n",
            "Batch 61 --- Loss: 0.1046\n",
            "Batch 62 --- Loss: 0.0559\n",
            "Batch 63 --- Loss: 0.0827\n",
            "Batch 64 --- Loss: 0.0940\n",
            "Batch 65 --- Loss: 0.0637\n",
            "Batch 66 --- Loss: 0.1019\n",
            "Batch 67 --- Loss: 0.1055\n",
            "Batch 68 --- Loss: 0.0458\n",
            "Batch 69 --- Loss: 0.1336\n",
            "Batch 70 --- Loss: 0.1408\n",
            "Batch 71 --- Loss: 0.0489\n",
            "Batch 72 --- Loss: 0.2072\n",
            "Batch 73 --- Loss: 0.2506\n",
            "Batch 74 --- Loss: 0.1313\n",
            "Batch 75 --- Loss: 0.1818\n",
            "Batch 76 --- Loss: 0.0649\n",
            "Batch 77 --- Loss: 0.1027\n",
            "Batch 78 --- Loss: 0.1314\n",
            "Batch 79 --- Loss: 0.0321\n",
            "Batch 80 --- Loss: 0.0314\n",
            "Batch 81 --- Loss: 0.1881\n",
            "Batch 82 --- Loss: 0.0924\n",
            "Batch 83 --- Loss: 0.1828\n",
            "Batch 84 --- Loss: 0.0598\n",
            "Epoch 40 / 50 --- Average Loss: 0.0939\n",
            "Average Macro Precision: 0.3336 ---- Average Macro Recall: 0.2631 ---- Average F1 Score: 0.2505 ---- Average Loss: 0.2125\n",
            "Average No Damage Precision: 0.4994 ---- Average No Damage Recall: 0.0129 ---- Average No Damage F1: 0.0237\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8658 ---- Average Background Recall: 0.9998 ---- Average Background F1: 0.9257\n",
            "Batch 0 --- Loss: 0.1015\n",
            "Batch 1 --- Loss: 0.1108\n",
            "Batch 2 --- Loss: 0.0820\n",
            "Batch 3 --- Loss: 0.1399\n",
            "Batch 4 --- Loss: 0.1367\n",
            "Batch 5 --- Loss: 0.1184\n",
            "Batch 6 --- Loss: 0.0946\n",
            "Batch 7 --- Loss: 0.1100\n",
            "Batch 8 --- Loss: 0.0991\n",
            "Batch 9 --- Loss: 0.0855\n",
            "Batch 10 --- Loss: 0.1044\n",
            "Batch 11 --- Loss: 0.0682\n",
            "Batch 12 --- Loss: 0.0628\n",
            "Batch 13 --- Loss: 0.0535\n",
            "Batch 14 --- Loss: 0.0827\n",
            "Batch 15 --- Loss: 0.0842\n",
            "Batch 16 --- Loss: 0.0679\n",
            "Batch 17 --- Loss: 0.0559\n",
            "Batch 18 --- Loss: 0.0819\n",
            "Batch 19 --- Loss: 0.0554\n",
            "Batch 20 --- Loss: 0.1110\n",
            "Batch 21 --- Loss: 0.1212\n",
            "Batch 22 --- Loss: 0.0941\n",
            "Batch 23 --- Loss: 0.0909\n",
            "Batch 24 --- Loss: 0.0603\n",
            "Batch 25 --- Loss: 0.1673\n",
            "Batch 26 --- Loss: 0.1097\n",
            "Batch 27 --- Loss: 0.0771\n",
            "Batch 28 --- Loss: 0.0628\n",
            "Batch 29 --- Loss: 0.1083\n",
            "Batch 30 --- Loss: 0.1409\n",
            "Batch 31 --- Loss: 0.1110\n",
            "Batch 32 --- Loss: 0.0745\n",
            "Batch 33 --- Loss: 0.0937\n",
            "Batch 34 --- Loss: 0.1062\n",
            "Batch 35 --- Loss: 0.0204\n",
            "Batch 36 --- Loss: 0.0571\n",
            "Batch 37 --- Loss: 0.0309\n",
            "Batch 38 --- Loss: 0.0535\n",
            "Batch 39 --- Loss: 0.0726\n",
            "Batch 40 --- Loss: 0.0845\n",
            "Batch 41 --- Loss: 0.0477\n",
            "Batch 42 --- Loss: 0.0658\n",
            "Batch 43 --- Loss: 0.1266\n",
            "Batch 44 --- Loss: 0.1706\n",
            "Batch 45 --- Loss: 0.0495\n",
            "Batch 46 --- Loss: 0.1529\n",
            "Batch 47 --- Loss: 0.0943\n",
            "Batch 48 --- Loss: 0.1115\n",
            "Batch 49 --- Loss: 0.0630\n",
            "Batch 50 --- Loss: 0.0814\n",
            "Batch 51 --- Loss: 0.0259\n",
            "Batch 52 --- Loss: 0.0546\n",
            "Batch 53 --- Loss: 0.0647\n",
            "Batch 54 --- Loss: 0.1041\n",
            "Batch 55 --- Loss: 0.1261\n",
            "Batch 56 --- Loss: 0.0453\n",
            "Batch 57 --- Loss: 0.0722\n",
            "Batch 58 --- Loss: 0.0283\n",
            "Batch 59 --- Loss: 0.0741\n",
            "Batch 60 --- Loss: 0.0358\n",
            "Batch 61 --- Loss: 0.0813\n",
            "Batch 62 --- Loss: 0.0985\n",
            "Batch 63 --- Loss: 0.0458\n",
            "Batch 64 --- Loss: 0.1512\n",
            "Batch 65 --- Loss: 0.0947\n",
            "Batch 66 --- Loss: 0.1171\n",
            "Batch 67 --- Loss: 0.1439\n",
            "Batch 68 --- Loss: 0.0407\n",
            "Batch 69 --- Loss: 0.2330\n",
            "Batch 70 --- Loss: 0.1045\n",
            "Batch 71 --- Loss: 0.0881\n",
            "Batch 72 --- Loss: 0.1835\n",
            "Batch 73 --- Loss: 0.1347\n",
            "Batch 74 --- Loss: 0.1717\n",
            "Batch 75 --- Loss: 0.1403\n",
            "Batch 76 --- Loss: 0.1535\n",
            "Batch 77 --- Loss: 0.0955\n",
            "Batch 78 --- Loss: 0.1721\n",
            "Batch 79 --- Loss: 0.0303\n",
            "Batch 80 --- Loss: 0.0469\n",
            "Batch 81 --- Loss: 0.2178\n",
            "Batch 82 --- Loss: 0.0805\n",
            "Batch 83 --- Loss: 0.2130\n",
            "Batch 84 --- Loss: 0.0711\n",
            "Epoch 41 / 50 --- Average Loss: 0.0959\n",
            "Average Macro Precision: 0.3547 ---- Average Macro Recall: 0.2850 ---- Average F1 Score: 0.2854 ---- Average Loss: 0.1959\n",
            "Average No Damage Precision: 0.5878 ---- Average No Damage Recall: 0.1238 ---- Average No Damage F1: 0.1887\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8828 ---- Average Background Recall: 0.9981 ---- Average Background F1: 0.9355\n",
            "Batch 0 --- Loss: 0.1650\n",
            "Batch 1 --- Loss: 0.1216\n",
            "Batch 2 --- Loss: 0.1226\n",
            "Batch 3 --- Loss: 0.1383\n",
            "Batch 4 --- Loss: 0.1058\n",
            "Batch 5 --- Loss: 0.1440\n",
            "Batch 6 --- Loss: 0.1088\n",
            "Batch 7 --- Loss: 0.1204\n",
            "Batch 8 --- Loss: 0.0828\n",
            "Batch 9 --- Loss: 0.0965\n",
            "Batch 10 --- Loss: 0.0682\n",
            "Batch 11 --- Loss: 0.0380\n",
            "Batch 12 --- Loss: 0.0640\n",
            "Batch 13 --- Loss: 0.0849\n",
            "Batch 14 --- Loss: 0.0693\n",
            "Batch 15 --- Loss: 0.0850\n",
            "Batch 16 --- Loss: 0.0749\n",
            "Batch 17 --- Loss: 0.0696\n",
            "Batch 18 --- Loss: 0.0829\n",
            "Batch 19 --- Loss: 0.0569\n",
            "Batch 20 --- Loss: 0.1032\n",
            "Batch 21 --- Loss: 0.0669\n",
            "Batch 22 --- Loss: 0.0762\n",
            "Batch 23 --- Loss: 0.0525\n",
            "Batch 24 --- Loss: 0.0501\n",
            "Batch 25 --- Loss: 0.0804\n",
            "Batch 26 --- Loss: 0.0983\n",
            "Batch 27 --- Loss: 0.0724\n",
            "Batch 28 --- Loss: 0.0905\n",
            "Batch 29 --- Loss: 0.0715\n",
            "Batch 30 --- Loss: 0.0791\n",
            "Batch 31 --- Loss: 0.0479\n",
            "Batch 32 --- Loss: 0.1979\n",
            "Batch 33 --- Loss: 0.1000\n",
            "Batch 34 --- Loss: 0.0972\n",
            "Batch 35 --- Loss: 0.0168\n",
            "Batch 36 --- Loss: 0.0816\n",
            "Batch 37 --- Loss: 0.0451\n",
            "Batch 38 --- Loss: 0.0259\n",
            "Batch 39 --- Loss: 0.0662\n",
            "Batch 40 --- Loss: 0.1293\n",
            "Batch 41 --- Loss: 0.0291\n",
            "Batch 42 --- Loss: 0.1007\n",
            "Batch 43 --- Loss: 0.0685\n",
            "Batch 44 --- Loss: 0.1098\n",
            "Batch 45 --- Loss: 0.0482\n",
            "Batch 46 --- Loss: 0.2594\n",
            "Batch 47 --- Loss: 0.1076\n",
            "Batch 48 --- Loss: 0.1475\n",
            "Batch 49 --- Loss: 0.1146\n",
            "Batch 50 --- Loss: 0.0663\n",
            "Batch 51 --- Loss: 0.0297\n",
            "Batch 52 --- Loss: 0.1091\n",
            "Batch 53 --- Loss: 0.0665\n",
            "Batch 54 --- Loss: 0.0963\n",
            "Batch 55 --- Loss: 0.1091\n",
            "Batch 56 --- Loss: 0.0486\n",
            "Batch 57 --- Loss: 0.0956\n",
            "Batch 58 --- Loss: 0.0272\n",
            "Batch 59 --- Loss: 0.1074\n",
            "Batch 60 --- Loss: 0.0299\n",
            "Batch 61 --- Loss: 0.1048\n",
            "Batch 62 --- Loss: 0.1103\n",
            "Batch 63 --- Loss: 0.0884\n",
            "Batch 64 --- Loss: 0.1447\n",
            "Batch 65 --- Loss: 0.0967\n",
            "Batch 66 --- Loss: 0.1531\n",
            "Batch 67 --- Loss: 0.0989\n",
            "Batch 68 --- Loss: 0.0432\n",
            "Batch 69 --- Loss: 0.0827\n",
            "Batch 70 --- Loss: 0.1816\n",
            "Batch 71 --- Loss: 0.0799\n",
            "Batch 72 --- Loss: 0.1389\n",
            "Batch 73 --- Loss: 0.1810\n",
            "Batch 74 --- Loss: 0.1513\n",
            "Batch 75 --- Loss: 0.0930\n",
            "Batch 76 --- Loss: 0.0655\n",
            "Batch 77 --- Loss: 0.0669\n",
            "Batch 78 --- Loss: 0.0789\n",
            "Batch 79 --- Loss: 0.0284\n",
            "Batch 80 --- Loss: 0.0230\n",
            "Batch 81 --- Loss: 0.1885\n",
            "Batch 82 --- Loss: 0.1119\n",
            "Batch 83 --- Loss: 0.2658\n",
            "Batch 84 --- Loss: 0.0681\n",
            "Epoch 42 / 50 --- Average Loss: 0.0937\n",
            "Average Macro Precision: 0.3303 ---- Average Macro Recall: 0.2653 ---- Average F1 Score: 0.2545 ---- Average Loss: 0.1975\n",
            "Average No Damage Precision: 0.4812 ---- Average No Damage Recall: 0.0238 ---- Average No Damage F1: 0.0426\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8675 ---- Average Background Recall: 0.9994 ---- Average Background F1: 0.9266\n",
            "Batch 0 --- Loss: 0.0666\n",
            "Batch 1 --- Loss: 0.0768\n",
            "Batch 2 --- Loss: 0.1186\n",
            "Batch 3 --- Loss: 0.1439\n",
            "Batch 4 --- Loss: 0.0969\n",
            "Batch 5 --- Loss: 0.0948\n",
            "Batch 6 --- Loss: 0.1413\n",
            "Batch 7 --- Loss: 0.1298\n",
            "Batch 8 --- Loss: 0.0721\n",
            "Batch 9 --- Loss: 0.1461\n",
            "Batch 10 --- Loss: 0.0651\n",
            "Batch 11 --- Loss: 0.0428\n",
            "Batch 12 --- Loss: 0.0796\n",
            "Batch 13 --- Loss: 0.0582\n",
            "Batch 14 --- Loss: 0.0385\n",
            "Batch 15 --- Loss: 0.0794\n",
            "Batch 16 --- Loss: 0.0709\n",
            "Batch 17 --- Loss: 0.0444\n",
            "Batch 18 --- Loss: 0.0820\n",
            "Batch 19 --- Loss: 0.1081\n",
            "Batch 20 --- Loss: 0.1156\n",
            "Batch 21 --- Loss: 0.0652\n",
            "Batch 22 --- Loss: 0.1073\n",
            "Batch 23 --- Loss: 0.0817\n",
            "Batch 24 --- Loss: 0.0590\n",
            "Batch 25 --- Loss: 0.1276\n",
            "Batch 26 --- Loss: 0.0797\n",
            "Batch 27 --- Loss: 0.0773\n",
            "Batch 28 --- Loss: 0.0510\n",
            "Batch 29 --- Loss: 0.1167\n",
            "Batch 30 --- Loss: 0.0978\n",
            "Batch 31 --- Loss: 0.0523\n",
            "Batch 32 --- Loss: 0.1688\n",
            "Batch 33 --- Loss: 0.1440\n",
            "Batch 34 --- Loss: 0.1004\n",
            "Batch 35 --- Loss: 0.0289\n",
            "Batch 36 --- Loss: 0.1224\n",
            "Batch 37 --- Loss: 0.0391\n",
            "Batch 38 --- Loss: 0.0342\n",
            "Batch 39 --- Loss: 0.0609\n",
            "Batch 40 --- Loss: 0.1175\n",
            "Batch 41 --- Loss: 0.0403\n",
            "Batch 42 --- Loss: 0.0717\n",
            "Batch 43 --- Loss: 0.0619\n",
            "Batch 44 --- Loss: 0.1857\n",
            "Batch 45 --- Loss: 0.0566\n",
            "Batch 46 --- Loss: 0.1589\n",
            "Batch 47 --- Loss: 0.0835\n",
            "Batch 48 --- Loss: 0.0771\n",
            "Batch 49 --- Loss: 0.0746\n",
            "Batch 50 --- Loss: 0.0658\n",
            "Batch 51 --- Loss: 0.0491\n",
            "Batch 52 --- Loss: 0.0526\n",
            "Batch 53 --- Loss: 0.0670\n",
            "Batch 54 --- Loss: 0.0627\n",
            "Batch 55 --- Loss: 0.1606\n",
            "Batch 56 --- Loss: 0.0500\n",
            "Batch 57 --- Loss: 0.1698\n",
            "Batch 58 --- Loss: 0.0244\n",
            "Batch 59 --- Loss: 0.0781\n",
            "Batch 60 --- Loss: 0.0310\n",
            "Batch 61 --- Loss: 0.1409\n",
            "Batch 62 --- Loss: 0.0641\n",
            "Batch 63 --- Loss: 0.0692\n",
            "Batch 64 --- Loss: 0.0965\n",
            "Batch 65 --- Loss: 0.1340\n",
            "Batch 66 --- Loss: 0.1023\n",
            "Batch 67 --- Loss: 0.1088\n",
            "Batch 68 --- Loss: 0.0394\n",
            "Batch 69 --- Loss: 0.0705\n",
            "Batch 70 --- Loss: 0.2217\n",
            "Batch 71 --- Loss: 0.1577\n",
            "Batch 72 --- Loss: 0.1341\n",
            "Batch 73 --- Loss: 0.1810\n",
            "Batch 74 --- Loss: 0.1247\n",
            "Batch 75 --- Loss: 0.1389\n",
            "Batch 76 --- Loss: 0.0686\n",
            "Batch 77 --- Loss: 0.0576\n",
            "Batch 78 --- Loss: 0.1627\n",
            "Batch 79 --- Loss: 0.0347\n",
            "Batch 80 --- Loss: 0.0456\n",
            "Batch 81 --- Loss: 0.1963\n",
            "Batch 82 --- Loss: 0.0790\n",
            "Batch 83 --- Loss: 0.1336\n",
            "Batch 84 --- Loss: 0.0557\n",
            "Epoch 43 / 50 --- Average Loss: 0.0923\n",
            "Average Macro Precision: 0.2926 ---- Average Macro Recall: 0.2617 ---- Average F1 Score: 0.2478 ---- Average Loss: 0.2325\n",
            "Average No Damage Precision: 0.2950 ---- Average No Damage Recall: 0.0056 ---- Average No Damage F1: 0.0107\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8649 ---- Average Background Recall: 0.9999 ---- Average Background F1: 0.9252\n",
            "Batch 0 --- Loss: 0.1126\n",
            "Batch 1 --- Loss: 0.0826\n",
            "Batch 2 --- Loss: 0.1051\n",
            "Batch 3 --- Loss: 0.2352\n",
            "Batch 4 --- Loss: 0.1298\n",
            "Batch 5 --- Loss: 0.0830\n",
            "Batch 6 --- Loss: 0.0965\n",
            "Batch 7 --- Loss: 0.0999\n",
            "Batch 8 --- Loss: 0.0714\n",
            "Batch 9 --- Loss: 0.1707\n",
            "Batch 10 --- Loss: 0.0728\n",
            "Batch 11 --- Loss: 0.0641\n",
            "Batch 12 --- Loss: 0.1189\n",
            "Batch 13 --- Loss: 0.0951\n",
            "Batch 14 --- Loss: 0.0363\n",
            "Batch 15 --- Loss: 0.1912\n",
            "Batch 16 --- Loss: 0.0931\n",
            "Batch 17 --- Loss: 0.0414\n",
            "Batch 18 --- Loss: 0.2151\n",
            "Batch 19 --- Loss: 0.0490\n",
            "Batch 20 --- Loss: 0.1090\n",
            "Batch 21 --- Loss: 0.1105\n",
            "Batch 22 --- Loss: 0.1631\n",
            "Batch 23 --- Loss: 0.0554\n",
            "Batch 24 --- Loss: 0.0578\n",
            "Batch 25 --- Loss: 0.0765\n",
            "Batch 26 --- Loss: 0.0622\n",
            "Batch 27 --- Loss: 0.0718\n",
            "Batch 28 --- Loss: 0.0598\n",
            "Batch 29 --- Loss: 0.0635\n",
            "Batch 30 --- Loss: 0.0761\n",
            "Batch 31 --- Loss: 0.0518\n",
            "Batch 32 --- Loss: 0.0709\n",
            "Batch 33 --- Loss: 0.1173\n",
            "Batch 34 --- Loss: 0.0924\n",
            "Batch 35 --- Loss: 0.0240\n",
            "Batch 36 --- Loss: 0.0594\n",
            "Batch 37 --- Loss: 0.0845\n",
            "Batch 38 --- Loss: 0.0317\n",
            "Batch 39 --- Loss: 0.0656\n",
            "Batch 40 --- Loss: 0.0688\n",
            "Batch 41 --- Loss: 0.0207\n",
            "Batch 42 --- Loss: 0.0663\n",
            "Batch 43 --- Loss: 0.0952\n",
            "Batch 44 --- Loss: 0.1570\n",
            "Batch 45 --- Loss: 0.0467\n",
            "Batch 46 --- Loss: 0.1215\n",
            "Batch 47 --- Loss: 0.0739\n",
            "Batch 48 --- Loss: 0.1216\n",
            "Batch 49 --- Loss: 0.0614\n",
            "Batch 50 --- Loss: 0.0536\n",
            "Batch 51 --- Loss: 0.0435\n",
            "Batch 52 --- Loss: 0.0542\n",
            "Batch 53 --- Loss: 0.0687\n",
            "Batch 54 --- Loss: 0.0960\n",
            "Batch 55 --- Loss: 0.0515\n",
            "Batch 56 --- Loss: 0.0798\n",
            "Batch 57 --- Loss: 0.1072\n",
            "Batch 58 --- Loss: 0.0238\n",
            "Batch 59 --- Loss: 0.0675\n",
            "Batch 60 --- Loss: 0.0221\n",
            "Batch 61 --- Loss: 0.0986\n",
            "Batch 62 --- Loss: 0.1043\n",
            "Batch 63 --- Loss: 0.0688\n",
            "Batch 64 --- Loss: 0.0915\n",
            "Batch 65 --- Loss: 0.0825\n",
            "Batch 66 --- Loss: 0.1097\n",
            "Batch 67 --- Loss: 0.1768\n",
            "Batch 68 --- Loss: 0.0362\n",
            "Batch 69 --- Loss: 0.1210\n",
            "Batch 70 --- Loss: 0.0963\n",
            "Batch 71 --- Loss: 0.0504\n",
            "Batch 72 --- Loss: 0.1942\n",
            "Batch 73 --- Loss: 0.2307\n",
            "Batch 74 --- Loss: 0.1620\n",
            "Batch 75 --- Loss: 0.1887\n",
            "Batch 76 --- Loss: 0.0760\n",
            "Batch 77 --- Loss: 0.1288\n",
            "Batch 78 --- Loss: 0.1734\n",
            "Batch 79 --- Loss: 0.0351\n",
            "Batch 80 --- Loss: 0.0427\n",
            "Batch 81 --- Loss: 0.1520\n",
            "Batch 82 --- Loss: 0.0792\n",
            "Batch 83 --- Loss: 0.1893\n",
            "Batch 84 --- Loss: 0.0692\n",
            "Epoch 44 / 50 --- Average Loss: 0.0932\n",
            "Average Macro Precision: 0.2335 ---- Average Macro Recall: 0.2606 ---- Average F1 Score: 0.2456 ---- Average Loss: 0.2179\n",
            "Average No Damage Precision: 0.0000 ---- Average No Damage Recall: 0.0000 ---- Average No Damage F1: 0.0000\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8643 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9249\n",
            "Batch 0 --- Loss: 0.0900\n",
            "Batch 1 --- Loss: 0.1100\n",
            "Batch 2 --- Loss: 0.1438\n",
            "Batch 3 --- Loss: 0.1988\n",
            "Batch 4 --- Loss: 0.0965\n",
            "Batch 5 --- Loss: 0.0927\n",
            "Batch 6 --- Loss: 0.1293\n",
            "Batch 7 --- Loss: 0.1094\n",
            "Batch 8 --- Loss: 0.1609\n",
            "Batch 9 --- Loss: 0.0952\n",
            "Batch 10 --- Loss: 0.1105\n",
            "Batch 11 --- Loss: 0.0333\n",
            "Batch 12 --- Loss: 0.0920\n",
            "Batch 13 --- Loss: 0.0774\n",
            "Batch 14 --- Loss: 0.0713\n",
            "Batch 15 --- Loss: 0.1366\n",
            "Batch 16 --- Loss: 0.0672\n",
            "Batch 17 --- Loss: 0.0929\n",
            "Batch 18 --- Loss: 0.1143\n",
            "Batch 19 --- Loss: 0.0448\n",
            "Batch 20 --- Loss: 0.1714\n",
            "Batch 21 --- Loss: 0.1026\n",
            "Batch 22 --- Loss: 0.2463\n",
            "Batch 23 --- Loss: 0.0642\n",
            "Batch 24 --- Loss: 0.0844\n",
            "Batch 25 --- Loss: 0.0854\n",
            "Batch 26 --- Loss: 0.1023\n",
            "Batch 27 --- Loss: 0.1112\n",
            "Batch 28 --- Loss: 0.0525\n",
            "Batch 29 --- Loss: 0.0688\n",
            "Batch 30 --- Loss: 0.1315\n",
            "Batch 31 --- Loss: 0.1033\n",
            "Batch 32 --- Loss: 0.0826\n",
            "Batch 33 --- Loss: 0.1321\n",
            "Batch 34 --- Loss: 0.1010\n",
            "Batch 35 --- Loss: 0.0456\n",
            "Batch 36 --- Loss: 0.1136\n",
            "Batch 37 --- Loss: 0.0396\n",
            "Batch 38 --- Loss: 0.0382\n",
            "Batch 39 --- Loss: 0.0655\n",
            "Batch 40 --- Loss: 0.1200\n",
            "Batch 41 --- Loss: 0.0216\n",
            "Batch 42 --- Loss: 0.0933\n",
            "Batch 43 --- Loss: 0.1299\n",
            "Batch 44 --- Loss: 0.1018\n",
            "Batch 45 --- Loss: 0.1251\n",
            "Batch 46 --- Loss: 0.0832\n",
            "Batch 47 --- Loss: 0.1396\n",
            "Batch 48 --- Loss: 0.1056\n",
            "Batch 49 --- Loss: 0.0614\n",
            "Batch 50 --- Loss: 0.0922\n",
            "Batch 51 --- Loss: 0.1219\n",
            "Batch 52 --- Loss: 0.1222\n",
            "Batch 53 --- Loss: 0.0670\n",
            "Batch 54 --- Loss: 0.0721\n",
            "Batch 55 --- Loss: 0.0503\n",
            "Batch 56 --- Loss: 0.1325\n",
            "Batch 57 --- Loss: 0.0735\n",
            "Batch 58 --- Loss: 0.0265\n",
            "Batch 59 --- Loss: 0.0763\n",
            "Batch 60 --- Loss: 0.0387\n",
            "Batch 61 --- Loss: 0.1291\n",
            "Batch 62 --- Loss: 0.1570\n",
            "Batch 63 --- Loss: 0.0641\n",
            "Batch 64 --- Loss: 0.2152\n",
            "Batch 65 --- Loss: 0.0967\n",
            "Batch 66 --- Loss: 0.1043\n",
            "Batch 67 --- Loss: 0.1556\n",
            "Batch 68 --- Loss: 0.0418\n",
            "Batch 69 --- Loss: 0.1044\n",
            "Batch 70 --- Loss: 0.1972\n",
            "Batch 71 --- Loss: 0.1126\n",
            "Batch 72 --- Loss: 0.2287\n",
            "Batch 73 --- Loss: 0.1290\n",
            "Batch 74 --- Loss: 0.1865\n",
            "Batch 75 --- Loss: 0.0906\n",
            "Batch 76 --- Loss: 0.1125\n",
            "Batch 77 --- Loss: 0.0609\n",
            "Batch 78 --- Loss: 0.1761\n",
            "Batch 79 --- Loss: 0.0341\n",
            "Batch 80 --- Loss: 0.0529\n",
            "Batch 81 --- Loss: 0.1340\n",
            "Batch 82 --- Loss: 0.0604\n",
            "Batch 83 --- Loss: 0.1838\n",
            "Batch 84 --- Loss: 0.0587\n",
            "Epoch 45 / 50 --- Average Loss: 0.1029\n",
            "Average Macro Precision: 0.3528 ---- Average Macro Recall: 0.2721 ---- Average F1 Score: 0.2655 ---- Average Loss: 0.1427\n",
            "Average No Damage Precision: 0.5893 ---- Average No Damage Recall: 0.0580 ---- Average No Damage F1: 0.0956\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8716 ---- Average Background Recall: 0.9994 ---- Average Background F1: 0.9290\n",
            "Batch 0 --- Loss: 0.0792\n",
            "Batch 1 --- Loss: 0.0677\n",
            "Batch 2 --- Loss: 0.1325\n",
            "Batch 3 --- Loss: 0.1555\n",
            "Batch 4 --- Loss: 0.2345\n",
            "Batch 5 --- Loss: 0.0711\n",
            "Batch 6 --- Loss: 0.1402\n",
            "Batch 7 --- Loss: 0.0984\n",
            "Batch 8 --- Loss: 0.2423\n",
            "Batch 9 --- Loss: 0.1508\n",
            "Batch 10 --- Loss: 0.1641\n",
            "Batch 11 --- Loss: 0.0385\n",
            "Batch 12 --- Loss: 0.0993\n",
            "Batch 13 --- Loss: 0.0941\n",
            "Batch 14 --- Loss: 0.0599\n",
            "Batch 15 --- Loss: 0.0853\n",
            "Batch 16 --- Loss: 0.1442\n",
            "Batch 17 --- Loss: 0.0433\n",
            "Batch 18 --- Loss: 0.1285\n",
            "Batch 19 --- Loss: 0.0535\n",
            "Batch 20 --- Loss: 0.1546\n",
            "Batch 21 --- Loss: 0.1111\n",
            "Batch 22 --- Loss: 0.0691\n",
            "Batch 23 --- Loss: 0.0767\n",
            "Batch 24 --- Loss: 0.0560\n",
            "Batch 25 --- Loss: 0.1012\n",
            "Batch 26 --- Loss: 0.0612\n",
            "Batch 27 --- Loss: 0.0718\n",
            "Batch 28 --- Loss: 0.0587\n",
            "Batch 29 --- Loss: 0.0671\n",
            "Batch 30 --- Loss: 0.1070\n",
            "Batch 31 --- Loss: 0.0492\n",
            "Batch 32 --- Loss: 0.1094\n",
            "Batch 33 --- Loss: 0.0934\n",
            "Batch 34 --- Loss: 0.0890\n",
            "Batch 35 --- Loss: 0.0265\n",
            "Batch 36 --- Loss: 0.0516\n",
            "Batch 37 --- Loss: 0.0303\n",
            "Batch 38 --- Loss: 0.0427\n",
            "Batch 39 --- Loss: 0.0736\n",
            "Batch 40 --- Loss: 0.0704\n",
            "Batch 41 --- Loss: 0.0308\n",
            "Batch 42 --- Loss: 0.0843\n",
            "Batch 43 --- Loss: 0.1454\n",
            "Batch 44 --- Loss: 0.1428\n",
            "Batch 45 --- Loss: 0.0550\n",
            "Batch 46 --- Loss: 0.0938\n",
            "Batch 47 --- Loss: 0.1191\n",
            "Batch 48 --- Loss: 0.0959\n",
            "Batch 49 --- Loss: 0.1811\n",
            "Batch 50 --- Loss: 0.1202\n",
            "Batch 51 --- Loss: 0.0359\n",
            "Batch 52 --- Loss: 0.1090\n",
            "Batch 53 --- Loss: 0.0617\n",
            "Batch 54 --- Loss: 0.0512\n",
            "Batch 55 --- Loss: 0.1211\n",
            "Batch 56 --- Loss: 0.0766\n",
            "Batch 57 --- Loss: 0.0698\n",
            "Batch 58 --- Loss: 0.0282\n",
            "Batch 59 --- Loss: 0.1845\n",
            "Batch 60 --- Loss: 0.0287\n",
            "Batch 61 --- Loss: 0.0900\n",
            "Batch 62 --- Loss: 0.0619\n",
            "Batch 63 --- Loss: 0.0588\n",
            "Batch 64 --- Loss: 0.0909\n",
            "Batch 65 --- Loss: 0.0978\n",
            "Batch 66 --- Loss: 0.0724\n",
            "Batch 67 --- Loss: 0.1106\n",
            "Batch 68 --- Loss: 0.0441\n",
            "Batch 69 --- Loss: 0.0639\n",
            "Batch 70 --- Loss: 0.1445\n",
            "Batch 71 --- Loss: 0.0893\n",
            "Batch 72 --- Loss: 0.1826\n",
            "Batch 73 --- Loss: 0.1861\n",
            "Batch 74 --- Loss: 0.2093\n",
            "Batch 75 --- Loss: 0.1231\n",
            "Batch 76 --- Loss: 0.0670\n",
            "Batch 77 --- Loss: 0.1005\n",
            "Batch 78 --- Loss: 0.3409\n",
            "Batch 79 --- Loss: 0.0300\n",
            "Batch 80 --- Loss: 0.0199\n",
            "Batch 81 --- Loss: 0.1189\n",
            "Batch 82 --- Loss: 0.0777\n",
            "Batch 83 --- Loss: 0.1904\n",
            "Batch 84 --- Loss: 0.0654\n",
            "Epoch 46 / 50 --- Average Loss: 0.0979\n",
            "Average Macro Precision: 0.2335 ---- Average Macro Recall: 0.2606 ---- Average F1 Score: 0.2456 ---- Average Loss: 0.2837\n",
            "Average No Damage Precision: 0.0000 ---- Average No Damage Recall: 0.0000 ---- Average No Damage F1: 0.0000\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8643 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9249\n",
            "Batch 0 --- Loss: 0.1261\n",
            "Batch 1 --- Loss: 0.0805\n",
            "Batch 2 --- Loss: 0.0884\n",
            "Batch 3 --- Loss: 0.1524\n",
            "Batch 4 --- Loss: 0.1375\n",
            "Batch 5 --- Loss: 0.0990\n",
            "Batch 6 --- Loss: 0.1061\n",
            "Batch 7 --- Loss: 0.1791\n",
            "Batch 8 --- Loss: 0.0814\n",
            "Batch 9 --- Loss: 0.1268\n",
            "Batch 10 --- Loss: 0.0677\n",
            "Batch 11 --- Loss: 0.0969\n",
            "Batch 12 --- Loss: 0.0722\n",
            "Batch 13 --- Loss: 0.1279\n",
            "Batch 14 --- Loss: 0.0411\n",
            "Batch 15 --- Loss: 0.1361\n",
            "Batch 16 --- Loss: 0.1109\n",
            "Batch 17 --- Loss: 0.0714\n",
            "Batch 18 --- Loss: 0.1396\n",
            "Batch 19 --- Loss: 0.0489\n",
            "Batch 20 --- Loss: 0.1570\n",
            "Batch 21 --- Loss: 0.0761\n",
            "Batch 22 --- Loss: 0.0701\n",
            "Batch 23 --- Loss: 0.1219\n",
            "Batch 24 --- Loss: 0.1761\n",
            "Batch 25 --- Loss: 0.0963\n",
            "Batch 26 --- Loss: 0.1028\n",
            "Batch 27 --- Loss: 0.1457\n",
            "Batch 28 --- Loss: 0.0598\n",
            "Batch 29 --- Loss: 0.0710\n",
            "Batch 30 --- Loss: 0.1356\n",
            "Batch 31 --- Loss: 0.0596\n",
            "Batch 32 --- Loss: 0.0791\n",
            "Batch 33 --- Loss: 0.0819\n",
            "Batch 34 --- Loss: 0.1152\n",
            "Batch 35 --- Loss: 0.0189\n",
            "Batch 36 --- Loss: 0.0556\n",
            "Batch 37 --- Loss: 0.0964\n",
            "Batch 38 --- Loss: 0.0537\n",
            "Batch 39 --- Loss: 0.0729\n",
            "Batch 40 --- Loss: 0.0868\n",
            "Batch 41 --- Loss: 0.0283\n",
            "Batch 42 --- Loss: 0.0850\n",
            "Batch 43 --- Loss: 0.1473\n",
            "Batch 44 --- Loss: 0.1509\n",
            "Batch 45 --- Loss: 0.0542\n",
            "Batch 46 --- Loss: 0.0954\n",
            "Batch 47 --- Loss: 0.0714\n",
            "Batch 48 --- Loss: 0.1559\n",
            "Batch 49 --- Loss: 0.1159\n",
            "Batch 50 --- Loss: 0.0680\n",
            "Batch 51 --- Loss: 0.0256\n",
            "Batch 52 --- Loss: 0.0511\n",
            "Batch 53 --- Loss: 0.0698\n",
            "Batch 54 --- Loss: 0.0842\n",
            "Batch 55 --- Loss: 0.0503\n",
            "Batch 56 --- Loss: 0.0974\n",
            "Batch 57 --- Loss: 0.0800\n",
            "Batch 58 --- Loss: 0.0234\n",
            "Batch 59 --- Loss: 0.0605\n",
            "Batch 60 --- Loss: 0.0229\n",
            "Batch 61 --- Loss: 0.2139\n",
            "Batch 62 --- Loss: 0.0951\n",
            "Batch 63 --- Loss: 0.0620\n",
            "Batch 64 --- Loss: 0.0931\n",
            "Batch 65 --- Loss: 0.0579\n",
            "Batch 66 --- Loss: 0.0704\n",
            "Batch 67 --- Loss: 0.1020\n",
            "Batch 68 --- Loss: 0.0472\n",
            "Batch 69 --- Loss: 0.0639\n",
            "Batch 70 --- Loss: 0.1306\n",
            "Batch 71 --- Loss: 0.0396\n",
            "Batch 72 --- Loss: 0.2449\n",
            "Batch 73 --- Loss: 0.1841\n",
            "Batch 74 --- Loss: 0.0852\n",
            "Batch 75 --- Loss: 0.0910\n",
            "Batch 76 --- Loss: 0.0759\n",
            "Batch 77 --- Loss: 0.1003\n",
            "Batch 78 --- Loss: 0.1097\n",
            "Batch 79 --- Loss: 0.0470\n",
            "Batch 80 --- Loss: 0.0267\n",
            "Batch 81 --- Loss: 0.1249\n",
            "Batch 82 --- Loss: 0.0930\n",
            "Batch 83 --- Loss: 0.1308\n",
            "Batch 84 --- Loss: 0.0568\n",
            "Epoch 47 / 50 --- Average Loss: 0.0930\n",
            "Average Macro Precision: 0.2426 ---- Average Macro Recall: 0.2606 ---- Average F1 Score: 0.2456 ---- Average Loss: 0.2080\n",
            "Average No Damage Precision: 0.0455 ---- Average No Damage Recall: 0.0000 ---- Average No Damage F1: 0.0000\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8643 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9249\n",
            "Batch 0 --- Loss: 0.1048\n",
            "Batch 1 --- Loss: 0.0756\n",
            "Batch 2 --- Loss: 0.1086\n",
            "Batch 3 --- Loss: 0.0996\n",
            "Batch 4 --- Loss: 0.0988\n",
            "Batch 5 --- Loss: 0.0927\n",
            "Batch 6 --- Loss: 0.1149\n",
            "Batch 7 --- Loss: 0.1367\n",
            "Batch 8 --- Loss: 0.0689\n",
            "Batch 9 --- Loss: 0.1078\n",
            "Batch 10 --- Loss: 0.0638\n",
            "Batch 11 --- Loss: 0.0633\n",
            "Batch 12 --- Loss: 0.0911\n",
            "Batch 13 --- Loss: 0.0562\n",
            "Batch 14 --- Loss: 0.0683\n",
            "Batch 15 --- Loss: 0.1975\n",
            "Batch 16 --- Loss: 0.0672\n",
            "Batch 17 --- Loss: 0.0424\n",
            "Batch 18 --- Loss: 0.1202\n",
            "Batch 19 --- Loss: 0.0944\n",
            "Batch 20 --- Loss: 0.1117\n",
            "Batch 21 --- Loss: 0.1177\n",
            "Batch 22 --- Loss: 0.1057\n",
            "Batch 23 --- Loss: 0.0549\n",
            "Batch 24 --- Loss: 0.0539\n",
            "Batch 25 --- Loss: 0.1285\n",
            "Batch 26 --- Loss: 0.0610\n",
            "Batch 27 --- Loss: 0.0737\n",
            "Batch 28 --- Loss: 0.0408\n",
            "Batch 29 --- Loss: 0.1143\n",
            "Batch 30 --- Loss: 0.1457\n",
            "Batch 31 --- Loss: 0.0475\n",
            "Batch 32 --- Loss: 0.0679\n",
            "Batch 33 --- Loss: 0.0827\n",
            "Batch 34 --- Loss: 0.0932\n",
            "Batch 35 --- Loss: 0.0274\n",
            "Batch 36 --- Loss: 0.0489\n",
            "Batch 37 --- Loss: 0.0319\n",
            "Batch 38 --- Loss: 0.0529\n",
            "Batch 39 --- Loss: 0.0733\n",
            "Batch 40 --- Loss: 0.1175\n",
            "Batch 41 --- Loss: 0.0176\n",
            "Batch 42 --- Loss: 0.0664\n",
            "Batch 43 --- Loss: 0.0554\n",
            "Batch 44 --- Loss: 0.0930\n",
            "Batch 45 --- Loss: 0.0450\n",
            "Batch 46 --- Loss: 0.1519\n",
            "Batch 47 --- Loss: 0.1101\n",
            "Batch 48 --- Loss: 0.0968\n",
            "Batch 49 --- Loss: 0.0644\n",
            "Batch 50 --- Loss: 0.0560\n",
            "Batch 51 --- Loss: 0.0234\n",
            "Batch 52 --- Loss: 0.0530\n",
            "Batch 53 --- Loss: 0.0980\n",
            "Batch 54 --- Loss: 0.0421\n",
            "Batch 55 --- Loss: 0.0826\n",
            "Batch 56 --- Loss: 0.0376\n",
            "Batch 57 --- Loss: 0.0629\n",
            "Batch 58 --- Loss: 0.0290\n",
            "Batch 59 --- Loss: 0.0672\n",
            "Batch 60 --- Loss: 0.0263\n",
            "Batch 61 --- Loss: 0.0830\n",
            "Batch 62 --- Loss: 0.0550\n",
            "Batch 63 --- Loss: 0.0520\n",
            "Batch 64 --- Loss: 0.0921\n",
            "Batch 65 --- Loss: 0.1163\n",
            "Batch 66 --- Loss: 0.1063\n",
            "Batch 67 --- Loss: 0.1511\n",
            "Batch 68 --- Loss: 0.0382\n",
            "Batch 69 --- Loss: 0.0577\n",
            "Batch 70 --- Loss: 0.1778\n",
            "Batch 71 --- Loss: 0.0469\n",
            "Batch 72 --- Loss: 0.1678\n",
            "Batch 73 --- Loss: 0.2391\n",
            "Batch 74 --- Loss: 0.1369\n",
            "Batch 75 --- Loss: 0.2495\n",
            "Batch 76 --- Loss: 0.0646\n",
            "Batch 77 --- Loss: 0.0596\n",
            "Batch 78 --- Loss: 0.3267\n",
            "Batch 79 --- Loss: 0.0228\n",
            "Batch 80 --- Loss: 0.0264\n",
            "Batch 81 --- Loss: 0.1767\n",
            "Batch 82 --- Loss: 0.0693\n",
            "Batch 83 --- Loss: 0.2561\n",
            "Batch 84 --- Loss: 0.1351\n",
            "Epoch 48 / 50 --- Average Loss: 0.0907\n",
            "Average Macro Precision: 0.2667 ---- Average Macro Recall: 0.2257 ---- Average F1 Score: 0.2156 ---- Average Loss: 0.1826\n",
            "Average No Damage Precision: 0.3533 ---- Average No Damage Recall: 0.0277 ---- Average No Damage F1: 0.0443\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8664 ---- Average Background Recall: 0.9898 ---- Average Background F1: 0.9214\n",
            "Batch 0 --- Loss: 0.0996\n",
            "Batch 1 --- Loss: 0.1018\n",
            "Batch 2 --- Loss: 0.1040\n",
            "Batch 3 --- Loss: 0.1259\n",
            "Batch 4 --- Loss: 0.1712\n",
            "Batch 5 --- Loss: 0.0974\n",
            "Batch 6 --- Loss: 0.1145\n",
            "Batch 7 --- Loss: 0.1387\n",
            "Batch 8 --- Loss: 0.1401\n",
            "Batch 9 --- Loss: 0.1051\n",
            "Batch 10 --- Loss: 0.0672\n",
            "Batch 11 --- Loss: 0.0355\n",
            "Batch 12 --- Loss: 0.0914\n",
            "Batch 13 --- Loss: 0.0994\n",
            "Batch 14 --- Loss: 0.0376\n",
            "Batch 15 --- Loss: 0.1299\n",
            "Batch 16 --- Loss: 0.0734\n",
            "Batch 17 --- Loss: 0.1211\n",
            "Batch 18 --- Loss: 0.1519\n",
            "Batch 19 --- Loss: 0.0566\n",
            "Batch 20 --- Loss: 0.1522\n",
            "Batch 21 --- Loss: 0.1129\n",
            "Batch 22 --- Loss: 0.1185\n",
            "Batch 23 --- Loss: 0.0648\n",
            "Batch 24 --- Loss: 0.0602\n",
            "Batch 25 --- Loss: 0.0784\n",
            "Batch 26 --- Loss: 0.0665\n",
            "Batch 27 --- Loss: 0.0876\n",
            "Batch 28 --- Loss: 0.0527\n",
            "Batch 29 --- Loss: 0.0693\n",
            "Batch 30 --- Loss: 0.1001\n",
            "Batch 31 --- Loss: 0.1034\n",
            "Batch 32 --- Loss: 0.1222\n",
            "Batch 33 --- Loss: 0.1054\n",
            "Batch 34 --- Loss: 0.1323\n",
            "Batch 35 --- Loss: 0.0256\n",
            "Batch 36 --- Loss: 0.0564\n",
            "Batch 37 --- Loss: 0.0393\n",
            "Batch 38 --- Loss: 0.0375\n",
            "Batch 39 --- Loss: 0.0767\n",
            "Batch 40 --- Loss: 0.0761\n",
            "Batch 41 --- Loss: 0.0295\n",
            "Batch 42 --- Loss: 0.1240\n",
            "Batch 43 --- Loss: 0.0581\n",
            "Batch 44 --- Loss: 0.1017\n",
            "Batch 45 --- Loss: 0.0539\n",
            "Batch 46 --- Loss: 0.0873\n",
            "Batch 47 --- Loss: 0.1292\n",
            "Batch 48 --- Loss: 0.1149\n",
            "Batch 49 --- Loss: 0.1045\n",
            "Batch 50 --- Loss: 0.0604\n",
            "Batch 51 --- Loss: 0.0351\n",
            "Batch 52 --- Loss: 0.1888\n",
            "Batch 53 --- Loss: 0.1174\n",
            "Batch 54 --- Loss: 0.1539\n",
            "Batch 55 --- Loss: 0.0862\n",
            "Batch 56 --- Loss: 0.1325\n",
            "Batch 57 --- Loss: 0.1001\n",
            "Batch 58 --- Loss: 0.0323\n",
            "Batch 59 --- Loss: 0.0861\n",
            "Batch 60 --- Loss: 0.0819\n",
            "Batch 61 --- Loss: 0.1940\n",
            "Batch 62 --- Loss: 0.0602\n",
            "Batch 63 --- Loss: 0.0641\n",
            "Batch 64 --- Loss: 0.1413\n",
            "Batch 65 --- Loss: 0.0571\n",
            "Batch 66 --- Loss: 0.0749\n",
            "Batch 67 --- Loss: 0.1388\n",
            "Batch 68 --- Loss: 0.0383\n",
            "Batch 69 --- Loss: 0.0968\n",
            "Batch 70 --- Loss: 0.1431\n",
            "Batch 71 --- Loss: 0.1526\n",
            "Batch 72 --- Loss: 0.1857\n",
            "Batch 73 --- Loss: 0.1315\n",
            "Batch 74 --- Loss: 0.1438\n",
            "Batch 75 --- Loss: 0.1321\n",
            "Batch 76 --- Loss: 0.0903\n",
            "Batch 77 --- Loss: 0.1097\n",
            "Batch 78 --- Loss: 0.1648\n",
            "Batch 79 --- Loss: 0.0284\n",
            "Batch 80 --- Loss: 0.0209\n",
            "Batch 81 --- Loss: 0.1373\n",
            "Batch 82 --- Loss: 0.0948\n",
            "Batch 83 --- Loss: 0.1773\n",
            "Batch 84 --- Loss: 0.0650\n",
            "Epoch 49 / 50 --- Average Loss: 0.0979\n",
            "Average Macro Precision: 0.2335 ---- Average Macro Recall: 0.2606 ---- Average F1 Score: 0.2456 ---- Average Loss: 0.2968\n",
            "Average No Damage Precision: 0.0000 ---- Average No Damage Recall: 0.0000 ---- Average No Damage F1: 0.0000\n",
            "Average Minor Precision: 0.0000 ---- Average Minor Recall: 0.0000 ---- Average Minor F1: 0.0000\n",
            "Average Moderate Precision: 0.0000 ---- Average Moderate Recall: 0.0000 ---- Average Moderate F1: 0.0000\n",
            "Average Major Precision: 0.0000 ---- Average Major Recall: 0.0000 ---- Average Major F1: 0.0000\n",
            "Average Background Precision: 0.8643 ---- Average Background Recall: 1.0000 ---- Average Background F1: 0.9249\n",
            "Batch 0 --- Loss: 0.0853\n",
            "Batch 1 --- Loss: 0.0809\n",
            "Batch 2 --- Loss: 0.0808\n",
            "Batch 3 --- Loss: 0.1184\n",
            "Batch 4 --- Loss: 0.1335\n",
            "Batch 5 --- Loss: 0.0989\n",
            "Batch 6 --- Loss: 0.0868\n",
            "Batch 7 --- Loss: 0.1464\n",
            "Batch 8 --- Loss: 0.1367\n",
            "Batch 9 --- Loss: 0.1479\n",
            "Batch 10 --- Loss: 0.0662\n",
            "Batch 11 --- Loss: 0.0666\n",
            "Batch 12 --- Loss: 0.0599\n",
            "Batch 13 --- Loss: 0.0548\n",
            "Batch 14 --- Loss: 0.0650\n",
            "Batch 15 --- Loss: 0.0853\n",
            "Batch 16 --- Loss: 0.1151\n",
            "Batch 17 --- Loss: 0.0478\n",
            "Batch 18 --- Loss: 0.1181\n",
            "Batch 19 --- Loss: 0.0736\n",
            "Batch 20 --- Loss: 0.1210\n",
            "Batch 21 --- Loss: 0.0962\n",
            "Batch 22 --- Loss: 0.0612\n",
            "Batch 23 --- Loss: 0.0721\n",
            "Batch 24 --- Loss: 0.0962\n",
            "Batch 25 --- Loss: 0.0622\n",
            "Batch 26 --- Loss: 0.0623\n",
            "Batch 27 --- Loss: 0.0797\n",
            "Batch 28 --- Loss: 0.1235\n",
            "Batch 29 --- Loss: 0.0699\n",
            "Batch 30 --- Loss: 0.0902\n",
            "Batch 31 --- Loss: 0.1477\n",
            "Batch 32 --- Loss: 0.0728\n",
            "Batch 33 --- Loss: 0.0916\n",
            "Batch 34 --- Loss: 0.1081\n",
            "Batch 35 --- Loss: 0.0268\n",
            "Batch 36 --- Loss: 0.0582\n",
            "Batch 37 --- Loss: 0.1010\n",
            "Batch 38 --- Loss: 0.0333\n",
            "Batch 39 --- Loss: 0.0654\n",
            "Batch 40 --- Loss: 0.1206\n",
            "Batch 41 --- Loss: 0.0201\n",
            "Batch 42 --- Loss: 0.1217\n",
            "Batch 43 --- Loss: 0.0872\n",
            "Batch 44 --- Loss: 0.0932\n",
            "Batch 45 --- Loss: 0.0472\n",
            "Batch 46 --- Loss: 0.0765\n",
            "Batch 47 --- Loss: 0.0716\n",
            "Batch 48 --- Loss: 0.0934\n",
            "Batch 49 --- Loss: 0.1722\n",
            "Batch 50 --- Loss: 0.0637\n",
            "Batch 51 --- Loss: 0.0246\n",
            "Batch 52 --- Loss: 0.1082\n",
            "Batch 53 --- Loss: 0.1010\n",
            "Batch 54 --- Loss: 0.0443\n",
            "Batch 55 --- Loss: 0.0841\n",
            "Batch 56 --- Loss: 0.0843\n",
            "Batch 57 --- Loss: 0.1033\n",
            "Batch 58 --- Loss: 0.0260\n",
            "Batch 59 --- Loss: 0.0643\n",
            "Batch 60 --- Loss: 0.0209\n",
            "Batch 61 --- Loss: 0.1870\n",
            "Batch 62 --- Loss: 0.0510\n",
            "Batch 63 --- Loss: 0.0605\n",
            "Batch 64 --- Loss: 0.1809\n",
            "Batch 65 --- Loss: 0.1279\n",
            "Batch 66 --- Loss: 0.1035\n",
            "Batch 67 --- Loss: 0.1832\n",
            "Batch 68 --- Loss: 0.0384\n",
            "Batch 69 --- Loss: 0.0650\n",
            "Batch 70 --- Loss: 0.1188\n",
            "Batch 71 --- Loss: 0.0453\n",
            "Batch 72 --- Loss: 0.1640\n",
            "Batch 73 --- Loss: 0.1153\n",
            "Batch 74 --- Loss: 0.3728\n",
            "Batch 75 --- Loss: 0.1241\n",
            "Batch 76 --- Loss: 0.0797\n",
            "Batch 77 --- Loss: 0.1046\n",
            "Batch 78 --- Loss: 0.2036\n",
            "Batch 79 --- Loss: 0.0333\n",
            "Batch 80 --- Loss: 0.0853\n",
            "Batch 81 --- Loss: 0.1114\n",
            "Batch 82 --- Loss: 0.1096\n",
            "Batch 83 --- Loss: 0.1481\n",
            "Batch 84 --- Loss: 0.0592\n",
            "Epoch 50 / 50 --- Average Loss: 0.0942\n"
          ]
        }
      ],
      "source": [
        "def visualize_results(num_results, predictions, images=None, masks=None, randomize_images=False):\n",
        "    fig, axes = plt.subplots(num_results, 3, figsize=(32, 32))\n",
        "\n",
        "    predictions_flat = [item for sublist in predictions for item in sublist]\n",
        "    if (images != None):\n",
        "        images_flat = [item for sublist in images for item in sublist]\n",
        "    if (masks != None):\n",
        "        masks_flat = [item for sublist in masks for item in sublist]\n",
        "\n",
        "    if (randomize_images):\n",
        "        # Choose num_results number of images at random from the results.\n",
        "        image_idxs = random.sample(range(0, len(predictions_flat) - 1), num_results)\n",
        "    else:\n",
        "        image_idxs = [i for i in range(1, num_results + 2)]\n",
        "\n",
        "    for i in range(num_results):\n",
        "        # Plot the input image and ground truth mask\n",
        "        if (images == None or masks == None):\n",
        "            image, mask = test_dataset.get_item_resize_only(image_idxs[i], image_size)\n",
        "\n",
        "            #Reorder the channels for matplotlib.\n",
        "            image = torch.permute(image, (1, 2, 0))\n",
        "            mask = torch.permute(mask, (1, 2, 0))\n",
        "\n",
        "            axes[i, 0].imshow(image.numpy()[:, :, 0:3], aspect='equal')\n",
        "            axes[i, 0].imshow(image.numpy()[:, :, 3:6], alpha=0.5, aspect='equal')\n",
        "            #axes[i, 0].imshow(image.numpy()[:, :, 6:7], alpha=0.5, aspect='equal')\n",
        "            #axes[i, 0].imshow(image.numpy()[:, :, 7:8], alpha=0.5, aspect='equal')\n",
        "            axes[i, 2].imshow(mask.numpy(), cmap=\"viridis\", aspect='equal')\n",
        "        else:\n",
        "            image = images_flat[image_idxs[i]]\n",
        "            mask = masks_flat[image_idxs[i]]\n",
        "\n",
        "            #Reorder the channels for matplotlib.\n",
        "            image = np.transpose(image, (1, 2, 0))\n",
        "            #mask = np.transpose(mask, (1, 2, 0))\n",
        "\n",
        "            axes[i, 0].imshow(image[:, :, 0:3], aspect='equal')\n",
        "            axes[i, 0].imshow(image[:, :, 3:6], alpha=0.5, aspect='equal')\n",
        "            #axes[i, 0].imshow(image[:, :, 6:7], alpha=0.5, aspect='equal')\n",
        "            #axes[i, 0].imshow(image[:, :, 7:8], alpha=0.5, aspect='equal')\n",
        "            axes[i, 2].imshow(mask, cmap=\"viridis\", aspect='equal')\n",
        "\n",
        "        axes[i, 0].set_title(\"Combined Image\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 2].set_title(\"Ground Truth Mask\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "        # Plot the predicted image\n",
        "        axes[i, 1].imshow(predictions_flat[image_idxs[i]], cmap=\"viridis\", aspect='equal')\n",
        "        axes[i, 1].set_title(\"Predicted Image\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "batch_size = 3\n",
        "num_input_channels = 25\n",
        "num_classes = 5\n",
        "lr = 1e-4\n",
        "image_size = 750  #520x520 is the image size used by the pretrained weights in DeepLabv3\n",
        "\n",
        "train = True\n",
        "test = True\n",
        "\n",
        "# Whether the models parameters should be saved following the completion of a run.\n",
        "save = True\n",
        "#Whether an existing models parameters should be loaded before the run.\n",
        "load = False\n",
        "#Tracking the highest current F1 score and the epoch it occurred on for the model to know when to best save the model.\n",
        "highest_f1 = (0, 0)\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "if train:\n",
        "    print(\"Loading train images\")\n",
        "    train_dataset = HarveyData(os.path.join(cwd, 'drive/MyDrive/Flood Damage Extent Detection/dataset/training'), image_size=image_size, augment_data=True)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "if test:\n",
        "    print(\"Loading test images\")\n",
        "    test_dataset = HarveyData(os.path.join(cwd, 'drive/MyDrive/Flood Damage Extent Detection/dataset/testing'), image_size=image_size, augment_data=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(\"Finished loading images\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DeepLabV3(num_input_channels, num_classes)\n",
        "if (load):\n",
        "    if (os.path.exists('drive/MyDrive/Flood Damage Extent Detection/DeepLabv3.pt')):\n",
        "        print(\"Loading model.\")\n",
        "        model.load_state_dict(torch.load('drive/MyDrive/Flood Damage Extent Detection/DeepLabv3.pt'))\n",
        "    else:\n",
        "        print('Could not load model. File does not exist.')\n",
        "model.to(device)\n",
        "#model_preprocess = model.deeplabv3_weights.transforms()\n",
        "\n",
        "criterion = FocalLoss(reduction='mean')#torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005)\n",
        "\n",
        "#softmax = nn.Softmax(dim=1)\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "images = []\n",
        "masks = []\n",
        "predicted_images = []\n",
        "\n",
        "#Training\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    if train:\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for i, data in enumerate(train_dataloader):\n",
        "            image, mask = data\n",
        "\n",
        "            image = image.to(device)\n",
        "            mask = mask.squeeze().to(device)\n",
        "\n",
        "            outputs = model(image)['out']\n",
        "\n",
        "            loss = criterion(outputs, mask)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            print('Batch %d --- Loss: %.4f' % (i, loss.item() / batch_size))\n",
        "        print('Epoch %d / %d --- Average Loss: %.4f' % (epoch + 1, num_epochs, epoch_loss / train_dataset.__len__()))\n",
        "\n",
        "    if test:\n",
        "        total_loss = 0.0\n",
        "\n",
        "        total_macro_precision = 0.0\n",
        "        total_macro_recall = 0.0\n",
        "        total_macro_f1 = 0.0\n",
        "\n",
        "        total_class_precision = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "        total_class_recall = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "        total_class_f1 = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "    #Testing\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(test_dataloader):\n",
        "                image, mask = data\n",
        "\n",
        "                image = image.to(device)\n",
        "                mask = mask.squeeze().to(device)\n",
        "\n",
        "                outputs = model(image)['out']\n",
        "\n",
        "                loss = criterion(outputs, mask)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                predicted = torch.argmax(outputs, dim=1, keepdim=False)\n",
        "\n",
        "                image = image.cpu().numpy()\n",
        "                mask = mask.cpu().numpy()\n",
        "                predicted = predicted.cpu().numpy()\n",
        "\n",
        "                for i in range(len(mask)):\n",
        "                    # Calculate scores globally.\n",
        "                    precision, recall, f1, _ = precision_recall_fscore_support(mask[i].flatten(), predicted[i].flatten(), average='macro', zero_division=0.0)\n",
        "                    total_macro_precision += precision\n",
        "                    total_macro_recall += recall\n",
        "                    total_macro_f1 += f1\n",
        "\n",
        "                    # Calculate scores by class.\n",
        "                    precision, recall, f1, _ = precision_recall_fscore_support(mask[i].flatten(), predicted[i].flatten(), labels=[0, 1, 2, 3, 4], average=None, zero_division=0.0)\n",
        "                    total_class_precision += precision\n",
        "                    total_class_recall += recall\n",
        "                    total_class_f1 += f1\n",
        "\n",
        "                if (epoch + 1 == num_epochs):\n",
        "                    images.append(image)\n",
        "                    masks.append(mask)\n",
        "                    predicted_images.append(predicted)\n",
        "\n",
        "        average_loss = total_loss / len(test_dataset)\n",
        "\n",
        "        average_macro_precision = total_macro_precision / len(test_dataset)\n",
        "        average_macro_recall = total_macro_recall / len(test_dataset)\n",
        "        average_macro_f1 = total_macro_f1 / len(test_dataset)\n",
        "\n",
        "        average_class_precision = total_class_precision / len(test_dataset)\n",
        "        average_class_recall = total_class_recall / len(test_dataset)\n",
        "        average_class_f1 = total_class_f1 / len(test_dataset)\n",
        "\n",
        "        print('Average Macro Precision: %.4f ---- Average Macro Recall: %.4f ---- Average F1 Score: %.4f ---- Average Loss: %.4f' % (average_macro_precision, average_macro_recall, average_macro_f1, average_loss))\n",
        "        print('Average No Damage Precision: %.4f ---- Average No Damage Recall: %.4f ---- Average No Damage F1: %.4f' % (average_class_precision[0], average_class_recall[0], average_class_f1[0]))\n",
        "        print('Average Minor Precision: %.4f ---- Average Minor Recall: %.4f ---- Average Minor F1: %.4f' % (average_class_precision[1], average_class_recall[1], average_class_f1[1]))\n",
        "        print('Average Moderate Precision: %.4f ---- Average Moderate Recall: %.4f ---- Average Moderate F1: %.4f' % (average_class_precision[2], average_class_recall[2], average_class_f1[2]))\n",
        "        print('Average Major Precision: %.4f ---- Average Major Recall: %.4f ---- Average Major F1: %.4f' % (average_class_precision[3], average_class_recall[3], average_class_f1[3]))\n",
        "        print('Average Background Precision: %.4f ---- Average Background Recall: %.4f ---- Average Background F1: %.4f' % (average_class_precision[4], average_class_recall[4], average_class_f1[4]))\n",
        "\n",
        "        #Save model\n",
        "        if (save and average_macro_f1 > highest_f1[0]):\n",
        "            highest_f1 = (average_macro_f1, epoch + 1)\n",
        "            save_file = f'drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_{highest_f1[1]}_{highest_f1[0]}.pt'\n",
        "            torch.save(model.state_dict(), save_file)\n",
        "            print(f'Saved Model to {save_file}')\n",
        "\n",
        "        if (epoch + 1 == num_epochs):\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(f\"Elapsed Time at Epoch {epoch + 1} : {elapsed_time} seconds\")\n",
        "\n",
        "            if save:\n",
        "                save_file = f'drive/MyDrive/Flood Damage Extent Detection/DeepLabv3_epoch_{epoch + 1}_{average_macro_f1}.pt'\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                print(f'Saved Model to {save_file}')\n",
        "\n",
        "            visualize_results(6, predicted_images, images, masks)\n",
        "\n",
        "            images.clear()\n",
        "            masks.clear()\n",
        "            predicted_images.clear()\n",
        "\n",
        "            start_time = time.time()"
      ]
    }
  ]
}