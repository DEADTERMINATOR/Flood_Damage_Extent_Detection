{"cells":[{"cell_type":"markdown","metadata":{},"source":["Import Libraries:"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","from torchvision import models\n","import torchvision.transforms as transforms\n","from torchvision.transforms import Compose, Resize, v2\n","from torchvision.transforms.functional import to_tensor\n","\n","import os\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image"]},{"cell_type":"markdown","metadata":{},"source":["Import Dataset: "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class HarveyData(Dataset):\n","    def __init__(self, dataset_dir, transforms=None):\n","        super(HarveyData, self).__init__()\n","        self.dataset_dir = dataset_dir\n","        self.transforms = transforms\n","\n","        self.pre_image_paths = sorted(os.listdir(os.path.join(dataset_dir, 'pre_img')))\n","        self.post_image_paths = sorted(os.listdir(os.path.join(dataset_dir, 'post_img')))\n","        self.mask_paths = sorted(os.listdir(os.path.join(dataset_dir, 'post_msk')))\n","        self.num_images = len(self.pre_image_paths)\n","\n","    def __getitem__(self, idx):\n","        pre_image_path = os.path.join(self.dataset_dir, 'pre_img', self.pre_image_paths[idx])\n","        post_image_path = os.path.join(self.dataset_dir, 'post_img', self.post_image_paths[idx])\n","        mask_path = os.path.join(self.dataset_dir, 'post_msk', self.mask_paths[idx])\n","\n","        pre_image = Image.open(pre_image_path)\n","        post_image = Image.open(post_image_path)\n","        mask = Image.open(mask_path)\n","\n","        # Convert PIL Images to Tensors\n","        pre_image = to_tensor(pre_image)\n","        post_image = to_tensor(post_image)\n","        mask = to_tensor(mask)\n","\n","        # Apply additional transformations if any\n","        if self.transforms:\n","            pre_image = self.transforms(pre_image)\n","            post_image = self.transforms(post_image)\n","            mask = self.transforms(mask)\n","\n","        combined_image = torch.cat([pre_image, post_image], dim=0)\n","        return combined_image, mask\n","\n","    def __len__(self):\n","        return self.num_images"]},{"cell_type":"markdown","metadata":{},"source":["DeepLabV3 Model: "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["class DeepLabV3(nn.Module):\n","    def __init__(self, num_input_channels, num_classes):\n","        super(DeepLabV3, self).__init__()\n","        self.deeplabv3 = torchvision.models.segmentation.deeplabv3_resnet50(num_classes=num_classes, weights=None)\n","\n","        # Modify the first convolutional layer of the ResNet50 backbone\n","        self.deeplabv3.backbone.conv1 = nn.Conv2d(num_input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","        # Initialize the modified convolutional layer\n","        nn.init.kaiming_normal_(self.deeplabv3.backbone.conv1.weight, mode='fan_out', nonlinearity='relu')\n","\n","    def forward(self, x):\n","        return self.deeplabv3(x)['out']"]},{"cell_type":"markdown","metadata":{},"source":["Training and Testing: "]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"116airy7vw8bo75PG6P8_Fl8oNggQpfzh"},"executionInfo":{"elapsed":2458886,"status":"ok","timestamp":1700442290003,"user":{"displayName":"DEADTERMINATOR","userId":"00537253248716777682"},"user_tz":480},"id":"BDnqqojUpKLE","outputId":"6090b0f9-8ac7-470d-9795-4c840cf0b953"},"outputs":[{"ename":"RuntimeError","evalue":"Expected floating point type for target with class probabilities, got Long","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\weiba\\Desktop\\Assignments\\CMPT 742\\Project\\Flood_Damage_Extent_Detection\\DeepLabV3.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/weiba/Desktop/Assignments/CMPT%20742/Project/Flood_Damage_Extent_Detection/DeepLabV3.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Resize outputs to match mask size if necessary\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/weiba/Desktop/Assignments/CMPT%20742/Project/Flood_Damage_Extent_Detection/DeepLabV3.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m outputs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39minterpolate(outputs, size\u001b[39m=\u001b[39mmask\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:], mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/weiba/Desktop/Assignments/CMPT%20742/Project/Flood_Damage_Extent_Detection/DeepLabV3.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, mask)  \u001b[39m# mask is already LongTensor\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/weiba/Desktop/Assignments/CMPT%20742/Project/Flood_Damage_Extent_Detection/DeepLabV3.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/weiba/Desktop/Assignments/CMPT%20742/Project/Flood_Damage_Extent_Detection/DeepLabV3.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[1;32mc:\\Users\\weiba\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\weiba\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\weiba\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n","File \u001b[1;32mc:\\Users\\weiba\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n","\u001b[1;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"]}],"source":["batch_size = 16\n","num_input_channels = 6\n","num_classes = 4\n","lr = 1e-5\n","image_size = 224\n","\n","transforms = v2.Compose([\n","    v2.Resize((image_size, image_size), antialias=True),\n","    v2.RandomHorizontalFlip(p=0.5),\n","    v2.RandomVerticalFlip(p=0.5),\n","    v2.RandomRotation(degrees=(1, 359)),\n","    v2.RandomResizedCrop(size=image_size, antialias=True)\n","    ])\n","\n","cwd = os.getcwd()\n","\n","train_dataset = HarveyData(os.path.join(cwd, 'dataset//training'), transforms=transforms)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataset = HarveyData(os.path.join(cwd, 'dataset//testing'), transforms=transforms)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = DeepLabV3(num_input_channels, num_classes).to(device)\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","num_epochs = 20\n","\n","predicted_images = []\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for i, data in enumerate(train_dataloader):\n","        image, mask = data\n","        image = image.to(device)\n","        mask = mask.to(device).long()  # Ensure mask is LongTensor\n","\n","        optimizer.zero_grad()\n","        outputs = model(image)\n","\n","        # Resize outputs to match mask size if necessary\n","        outputs = nn.functional.interpolate(outputs, size=mask.shape[-2:], mode='bilinear', align_corners=True)\n","\n","        loss = criterion(outputs, mask)  # mask is already LongTensor\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        print('Batch %d --- Loss: %.4f' % (i, loss.item() / batch_size))\n","\n","    print('Epoch %d / %d --- Average Loss: %.4f' % (epoch + 1, num_epochs, epoch_loss / len(train_dataset)))\n","\n","    model.eval()\n","    total_loss = 0.0\n","    correct_predictions = 0\n","    total_pixels = 0\n","    dice_score = 0.0\n","\n","    with torch.no_grad():\n","        for i, data in enumerate(test_dataloader):\n","            image, mask = data\n","            image = image.to(device)\n","            mask = mask.to(device)\n","\n","            outputs = model(image)\n","            outputs = nn.functional.interpolate(outputs, size=mask.shape[-2:], mode='bilinear', align_corners=True)\n","\n","            loss = criterion(outputs, mask.long())  # Ensure the mask is in long format\n","            total_loss += loss.item()\n","\n","            predicted = torch.argmax(outputs, dim=1)\n","            correct_predictions += (predicted == mask).sum().item()\n","            total_pixels += mask.numel()\n","\n","            # Calculate Dice coefficient\n","            dice_score += (2 * (predicted & mask).sum()) / ((predicted + mask).sum() + 1e-8)\n","\n","    accuracy = 100.0 * correct_predictions / total_pixels\n","    average_loss = total_loss / len(test_dataset)\n","    dice_score = dice_score / len(test_dataset)\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs} --- Test Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%, Dice: {dice_score:.4f}')\n","\n","fig, axs = plt.subplots(8, 3, figsize=(32, 32))\n","\n","for i in range(8):\n","    # Plot the input image\n","    image, mask = test_dataset.__getitem__(i)\n","    axs[i, 0].imshow(image.numpy()[3:6, :, :].T, aspect='equal')\n","    axs[i, 0].set_title(\"Input Image\")\n","    axs[i, 0].axis('off')\n","\n","    # Plot the predicted image\n","    predicted_images_flat = [item for sublist in predicted_images for item in sublist]\n","    axs[i, 1].imshow(predicted_images_flat[i].T, cmap=\"viridis\", aspect='equal')  # Adjust the colormap as needed\n","    axs[i, 1].set_title(\"Predicted Image\")\n","    axs[i, 1].axis('off')\n","\n","    # Plot the ground truth mask\n","    axs[i, 2].imshow(mask.numpy()[0].T, cmap=\"viridis\", aspect='equal')  # Assuming the mask is a single-channel image\n","    axs[i, 2].set_title(\"Ground Truth Mask\")\n","    axs[i, 2].axis('off')\n","\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNO6v/H6tc3abrfxRCEsT6Y","gpuType":"T4","mount_file_id":"1KMWS2Oyo4vVFS2GYmUhqId2DopiI21d6","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
